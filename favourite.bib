@article{abadi2016tensorflow,
	title        = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	author       = {Mart{\'{\i}}n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Gregory S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian J. Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal J{\'{o}}zefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Man{\'{e}} and Rajat Monga and Sherry Moore and Derek Gordon Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul A. Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda B. Vi{\'{e}}gas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1603.04467},
	url          = {http://arxiv.org/abs/1603.04467},
	eprinttype   = {arXiv},
	eprint       = {1603.04467},
	timestamp    = {Mon, 13 Aug 2018 16:47:09 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/AbadiABBCCCDDDG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{abate20072d,
	title        = {2D and 3D face recognition: A survey},
	author       = {Andrea F. Abate and Michele Nappi and Daniel Riccio and Gabriele Sabatino},
	year         = 2007,
	journal      = {Pattern Recognition Letters},
	volume       = 28,
	number       = 14,
	pages        = {1885--1906},
	doi          = {https://doi.org/10.1016/j.patrec.2006.12.018},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865507000189},
	note         = {Image: Information and Control},
	keywords     = {2D/3D face recognition, Face databases},
	abstract     = {Government agencies are investing a considerable amount of resources into improving security systems as result of recent terrorist events that dangerously exposed flaws and weaknesses in today's safety mechanisms. Badge or password-based authentication procedures are too easy to hack. Biometrics represents a valid alternative but they suffer of drawbacks as well. Iris scanning, for example, is very reliable but too intrusive; fingerprints are socially accepted, but not applicable to non-consentient people. On the other hand, face recognition represents a good compromise between what's socially acceptable and what's reliable, even when operating under controlled conditions. In last decade, many algorithms based on linear/nonlinear methods, neural networks, wavelets, etc. have been proposed. Nevertheless, Face Recognition Vendor Test 2002 shown that most of these approaches encountered problems in outdoor conditions. This lowered their reliability compared to state of the art biometrics. This paper provides an “ex cursus” of recent face recognition research trends in 2D imagery and 3D model based algorithms. To simplify comparisons across different approaches, tables containing different collection of parameters (such as input size, recognition rate, number of addressed problems) are provided. This paper concludes by proposing possible future directions.}
}
@article{agresti1998approximate,
	title        = {Approximate is Better than “Exact” for Interval Estimation of Binomial Proportions},
	author       = {Alan   Agresti  and  Brent A.   Coull},
	year         = 1998,
	journal      = {The American Statistician},
	publisher    = {Taylor & Francis},
	volume       = 52,
	number       = 2,
	pages        = {119--126},
	doi          = {10.1080/00031305.1998.10480550},
	url          = {https://doi.org/10.1080/00031305.1998.10480550},
	eprint       = {https://doi.org/10.1080/00031305.1998.10480550}
}
@article{aiello2013sensing,
	title        = {Sensing Trending Topics in Twitter},
	author       = {Aiello, Luca Maria and Petkos, Georgios and Martin, Carlos and Corney, David and Papadopoulos, Symeon and Skraba, Ryan and Göker, Ayse and Kompatsiaris, Ioannis and Jaimes, Alejandro},
	year         = 2013,
	journal      = {IEEE Transactions on Multimedia},
	volume       = 15,
	number       = 6,
	pages        = {1268--1282},
	doi          = {10.1109/TMM.2013.2265080}
}
@article{alguliyev2019cosum,
	title        = {COSUM: Text summarization based on clustering and optimization},
	author       = {Alguliyev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R. and Abdi, Asad and Idris, Norisma},
	year         = 2019,
	journal      = {Expert Systems},
	volume       = 36,
	number       = 1,
	pages        = {e12340},
	doi          = {https://doi.org/10.1111/exsy.12340},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12340},
	note         = {e12340 EXSY-Jan-16-010.R2},
	keywords     = {adaptive differential evolution algorithm, content coverage, harmonic mean, information diversity, k-means, optimization model, sentence clustering, text summarization},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12340},
	abstract     = {Abstract Text summarization is a process of extracting salient information from a source text and presenting that information to the user in a condensed form while preserving its main content. In the text summarization, most of the difficult problems are providing wide topic coverage and diversity in a summary. Research based on clustering, optimization, and evolutionary algorithm for text summarization has recently shown good results, making this a promising area. In this paper, for a text summarization, a two-stage sentences selection model based on clustering and optimization techniques, called COSUM, is proposed. At the first stage, to discover all topics in a text, the sentences set is clustered by using k-means method. At the second stage, for selection of salient sentences from clusters, an optimization model is proposed. This model optimizes an objective function that expressed as a harmonic mean of the objective functions enforcing the coverage and diversity of the selected sentences in the summary. To provide readability of a summary, this model also controls length of sentences selected in the candidate summary. For solving the optimization problem, an adaptive differential evolution algorithm with novel mutation strategy is developed. The method COSUM was compared with the 14 state-of-the-art methods: DPSO-EDASum; LexRank; CollabSum; UnifiedRank; 0-1 non-linear; query, cluster, summarize; support vector machine; fuzzy evolutionary optimization model; conditional random fields; MA-SingleDocSum; NetSum; manifold ranking; ESDS-GHS-GLO; and differential evolution, using ROUGE tool kit on the DUC2001 and DUC2002 data sets. Experimental results demonstrated that COSUM outperforms the state-of-the-art methods in terms of ROUGE-1 and ROUGE-2 measures.}
}
@article{allerton1969sentence,
	title        = {The sentence as a linguistic unit},
	author       = {D.J. Allerton},
	year         = 1969,
	journal      = {Lingua},
	volume       = 22,
	pages        = {27--46},
	doi          = {https://doi.org/10.1016/0024-3841(69)90042-4},
	issn         = {0024-3841},
	url          = {https://www.sciencedirect.com/science/article/pii/0024384169900424}
}
@article{allison1986bit,
	title        = {A bit-string longest-common-subsequence algorithm},
	author       = {Lloyd Allison and Trevor I. Dix},
	year         = 1986,
	journal      = {Information Processing Letters},
	volume       = 23,
	number       = 5,
	pages        = {305--310},
	doi          = {https://doi.org/10.1016/0020-0190(86)90091-8},
	issn         = {0020-0190},
	url          = {https://www.sciencedirect.com/science/article/pii/0020019086900918},
	keywords     = {Longest common subsequence, edit distance, bit string},
	abstract     = {A longest-common-subsequence algorithm is described which operates in terms of bit or bit-string operations. It offers a speedup of the order of the word-length on a conventional computer.}
}
@article{alrajeh2020combining,
	title        = {Combining experts' causal judgments},
	author       = {Dalal Alrajeh and Hana Chockler and Joseph Y. Halpern},
	year         = 2020,
	journal      = {Artificial Intelligence},
	volume       = 288,
	pages        = 103355,
	doi          = {https://doi.org/10.1016/j.artint.2020.103355},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370220301065},
	keywords     = {Causality, Intervention, Combining causal judgments, Complexity},
	abstract     = {Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts' opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts' causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being compatible, and show how compatible causal models can be merged. We then use it as the basis for combining experts' causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples.}
}
@article{altinel2018semantic,
	title        = {Semantic text classification: A survey of past and recent advances},
	author       = {Berna Altınel and Murat Can Ganiz},
	year         = 2018,
	journal      = {Information Processing  \& Management},
	volume       = 54,
	number       = 6,
	pages        = {1129--1153},
	doi          = {https://doi.org/10.1016/j.ipm.2018.08.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457317305757},
	keywords     = {Text classification, Semantic text classification, Knowledge-based systems, Corpus-based systems, Neural language models, Deep learning},
	abstract     = {Automatic text classification is the task of organizing documents into pre-determined classes, generally using machine learning algorithms. Generally speaking, it is one of the most important methods to organize and make use of the gigantic amounts of information that exist in unstructured textual format. Text classification is a widely studied research area of language processing and text mining. In traditional text classification, a document is represented as a bag of words where the words in other words terms are cut from their finer context i.e. their location in a sentence or in a document. Only the broader context of document is used with some type of term frequency information in the vector space. Consequently, semantics of words that can be inferred from the finer context of its location in a sentence and its relations with neighboring words are usually ignored. However, meaning of words, semantic connections between words, documents and even classes are obviously important since methods that capture semantics generally reach better classification performances. Several surveys have been published to analyze diverse approaches for the traditional text classification methods. Most of these surveys cover application of different semantic term relatedness methods in text classification up to a certain degree. However, they do not specifically target semantic text classification algorithms and their advantages over the traditional text classification. In order to fill this gap, we undertake a comprehensive discussion of semantic text classification vs. traditional text classification. This survey explores the past and recent advancements in semantic text classification and attempts to organize existing approaches under five fundamental categories; domain knowledge-based approaches, corpus-based approaches, deep learning based approaches, word/character sequence enhanced approaches and linguistic enriched approaches. Furthermore, this survey highlights the advantages of semantic text classification algorithms over the traditional text classification algorithms.}
}
@unpublished{alvarez2017review,
	title        = {A review of word embedding and document similarity algorithms applied to academic text},
	author       = {Alvarez, Jon Ezeiza},
	year         = 2017,
	url          = {https://ad-publications.cs.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017}
}
@inproceedings{alzantot2019genattack,
	title        = {GenAttack: Practical Black-Box Attacks with Gradient-Free Optimization},
	author       = {Alzantot, Moustafa and Sharma, Yash and Chakraborty, Supriyo and Zhang, Huan and Hsieh, Cho-Jui and Srivastava, Mani B.},
	year         = 2019,
	booktitle    = {Proceedings of the Genetic and Evolutionary Computation Conference},
	location     = {Prague, Czech Republic},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {GECCO '19},
	pages        = {1111--1119},
	doi          = {10.1145/3321707.3321749},
	isbn         = 9781450361118,
	url          = {https://doi.org/10.1145/3321707.3321749},
	abstract     = {Deep neural networks are vulnerable to adversarial examples, even in the black-box setting, where the attacker is restricted solely to query access. Existing black-box approaches to generating adversarial examples typically require a significant number of queries, either for training a substitute network or performing gradient estimation. We introduce GenAttack, a gradient-free optimization technique that uses genetic algorithms for synthesizing adversarial examples in the black-box setting. Our experiments on different datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can successfully generate visually imperceptible adversarial examples against state-of-the-art image recognition models with orders of magnitude fewer queries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack required roughly 2,126 and 2,568 times fewer queries respectively, than ZOO, the prior state-of-the-art black-box attack. In order to scale up the attack to large-scale high-dimensional ImageNet models, we perform a series of optimizations that further improve the query efficiency of our attack leading to 237 times fewer queries against the Inception-v3 model than ZOO. Furthermore, we show that GenAttack can successfully attack some state-of-the-art ImageNet defenses, including ensemble adversarial training and non-differentiable or randomized input transformations. Our results suggest that evolutionary algorithms open up a promising area of research into effective black-box attacks.},
	numpages     = 9,
	keywords     = {deep learning, adversarial examples, computer vision, genetic algorithm}
}
@inproceedings{amati2021topic,
	title        = {Topic Modeling by Community Detection Algorithms},
	author       = {Amati, Giambattista and Angelini, Simone and Cruciani, Antonio and Fusco, Gianmarco and Gaudino, Giancarlo and Pasquini, Daniele and Vocca, Paola},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 Workshop on Open Challenges in Online Social Networks},
	location     = {Virtual Event, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {OASIS '21},
	pages        = {15--20},
	doi          = {10.1145/3472720.3483622},
	isbn         = 9781450386326,
	url          = {https://doi.org/10.1145/3472720.3483622},
	abstract     = {We first estimate the number of Italian users active on Twitter in the last year by filtering the Italian flow of Twitter. We show that our filter misses about the 6.86\% of the Italian flow, while 86.80\% of the selected tweets belongs to the Italian language. Given this accuracy of the Italian Twitter's Firehose filter, we are able to assess the actual number of the Italian active users (AUs) of this platform. We then introduce a massive text document clustering algorithm that is easily applicable and scalable to the Twitter social network. Instead of a topic modeling approach based on features selection and any conventional clustering algorithm, such as LDA, we apply community detection algorithms on the weighted hashtag graph . In order to scale with the graph size, we apply two linear community detection algorithms, CoDA and Louvain. Once the hashtags have been assigned to clusters, both the most numerous clusters and hashtags were associated with topics of general interest, such as sports, politics, health etc. In this way we are able to provide significant statistics of the topics covered on Twitter in the past year.},
	numpages     = 6,
	keywords     = {data analysis, topic modelling, lda, community detection, twitter, louvain}
}
@article{amer2020set,
	title        = {A set theory based similarity measure for text clustering and classification},
	author       = {Amer, Ali A. and Abdalla, Hassan I.},
	year         = 2020,
	month        = {Sep},
	day          = 14,
	journal      = {Journal of Big Data},
	volume       = 7,
	number       = 1,
	pages        = 74,
	doi          = {10.1186/s40537-020-00344-3},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-020-00344-3},
	abstract     = {Similarity measures have long been utilized in information retrieval and machine learning domains for multi-purposes including text retrieval, text clustering, text summarization, plagiarism detection, and several other text-processing applications. However, the problem with these measures is that, until recently, there has never been one single measure recorded to be highly effective and efficient at the same time. Thus, the quest for an efficient and effective similarity measure is still an open-ended challenge. This study, in consequence, introduces a new highly-effective and time-efficient similarity measure for text clustering and classification. Furthermore, the study aims to provide a comprehensive scrutinization for seven of the most widely used similarity measures, mainly concerning their effectiveness and efficiency. Using the K-nearest neighbor algorithm (KNN) for classification, the K-means algorithm for clustering, and the bag of word (BoW) model for feature selection, all similarity measures are carefully examined in detail. The experimental evaluation has been made on two of the most popular datasets, namely, Reuters-21 and Web-KB. The obtained results confirm that the proposed set theory-based similarity measure (STB-SM), as a pre-eminent measure, outweighs all state-of-art measures significantly with regards to both effectiveness and efficiency.}
}
@article{amir2017sentence,
	title        = {Sentence similarity based on semantic kernels for intelligent text retrieval},
	author       = {Amir, Samir and Tanasescu, Adrian and Zighed, Djamel A.},
	year         = 2017,
	month        = {Jun},
	day          = {01},
	journal      = {Journal of Intelligent Information Systems},
	volume       = 48,
	number       = 3,
	pages        = {675--689},
	doi          = {10.1007/s10844-016-0434-3},
	issn         = {1573-7675},
	url          = {https://doi.org/10.1007/s10844-016-0434-3},
	abstract     = {We propose a new approach to compute semantic similarity between sentences. It is based on the semantic kernel, composed of subject, verb, and object that, we suppose, summarize the general meaning of each sentence. Thanks to linguistics resources available such as Stanford Parser, many features are then extracted from the semantic kernels and aggregated by mean of weights. The weighting is produced by a supervised machine learning technique on a training data set provided by human experts as ground truth. The cross validation shows good performances. Thanks to this similarity measure between sentences, one can build an intelligent text retrieval engine more sensitive to the semantic content, specifically suited for short texts than the classical methods based on bag of words. An application is being developed for highlighting parts of speech in scientific articles.}
}
@inproceedings{amir2021smt,
	title        = {An SMT-based approach for verifying binarized neural networks},
	author       = {Amir, Guy and Wu, Haoze and Barrett, Clark and Katz, Guy},
	year         = 2021,
	booktitle    = {Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27--April 1, 2021, Proceedings, Part II 27},
	pages        = {203--222},
	organization = {Springer}
}
@book{amsler1980structure,
	title        = {The structure of the Merriam-Webster pocket dictionary},
	author       = {Amsler, Robert Alfred},
	year         = 1980,
	publisher    = {The University of Texas at Austin}
}
@article{ananthakrishnan2007some,
	title        = {Some issues in automatic evaluation of english-hindi mt: more blues for bleu},
	author       = {Ananthakrishnan, R and Bhattacharyya, Pushpak and Sasikumar, M and Shah, Ritesh M},
	year         = 2007,
	journal      = {ICON},
	volume       = 64
}
@inproceedings{anderson2016spice,
	title        = {SPICE: Semantic Propositional Image Caption Evaluation},
	author       = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	year         = 2016,
	booktitle    = {Computer Vision -- ECCV 2016},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {382--398},
	isbn         = {978-3-319-46454-1},
	editor       = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	abstract     = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?}
}
@article{Ando2005,
	title        = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	author       = {Ando, Rie Kubota and Zhang, Tong},
	year         = 2005,
	month        = dec,
	journal      = {Journal of Machine Learning Research},
	publisher    = {JMLR.org},
	volume       = 6,
	pages        = {1817--1853},
	issn         = {1532-4435},
	acmid        = 1194905,
	issue_date   = {12/1/2005},
	numpages     = 37
}
@inproceedings{andrew2007scalable,
	title        = {Scalable training of {L1}-regularized log-linear models},
	author       = {Andrew, Galen and Gao, Jianfeng},
	year         = 2007,
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	pages        = {33--40}
}
@inproceedings{andriushchenko2020square,
	title        = {Square Attack: A Query-Efficient Black-Box Adversarial Attack via Random Search},
	author       = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
	year         = 2020,
	booktitle    = {Computer Vision -- ECCV 2020},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {484--501},
	isbn         = {978-3-030-58592-1},
	editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	abstract     = {We propose the Square Attack, a score-based black-box {\$}{\$}l{\_}2{\$}{\$}l2- and {\$}{\$}l{\_}{\backslash}infty {\$}{\$}l∞-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-of-the-art {\$}{\$}l{\_}{\backslash}infty {\$}{\$}l∞-attack of Al-Dujaili {\&} O'Reilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.}
}
@article{andriushchenko2020understanding,
	title        = {Understanding and improving fast adversarial training},
	author       = {Andriushchenko, Maksym and Flammarion, Nicolas},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {16048--16059}
}
@inproceedings{antol2015vqa,
	title        = {VQA: Visual Question Answering},
	author       = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
	year         = 2015,
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@book{antoniou2004semantic,
	title        = {A semantic web primer},
	author       = {Antoniou, Grigoris and Van Harmelen, Frank},
	year         = 2004,
	publisher    = {MIT press}
}
@article{antos1999lower,
	title        = {Lower bounds for Bayes error estimation},
	author       = {Antos, A. and Devroye, L. and Gyorfi, L.},
	year         = 1999,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 21,
	number       = 7,
	pages        = {643--645},
	doi          = {10.1109/34.777375}
}
@inproceedings{arora2017simple,
	title        = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
	author       = {Sanjeev Arora and Yingyu Liang and Tengyu Ma},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SyK00v5xx},
	timestamp    = {Sun, 08 Aug 2021 16:40:51 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/AroraLM17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{asyrofi2021biasfinder,
	title        = {BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems},
	author       = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Yusuf, Imam Nur Bani and Kang, Hong Jin and Thung, Ferdian and Lo, David},
	year         = 2022,
	journal      = {IEEE Transactions on Software Engineering},
	volume       = 48,
	number       = 12,
	pages        = {5087--5101},
	doi          = {10.1109/TSE.2021.3136169}
}
@inproceedings{athalye2018obfuscated,
	title        = {Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples},
	author       = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	year         = 2018,
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {274--283},
	url          = {https://proceedings.mlr.press/v80/athalye18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf},
	abstract     = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.}
}
@inproceedings{athalye2018synthesizing,
	title        = {Synthesizing Robust Adversarial Examples},
	author       = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	year         = 2018,
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {284--293},
	url          = {https://proceedings.mlr.press/v80/athalye18b.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/athalye18b/athalye18b.pdf},
	abstract     = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.}
}
@misc{author2024irreduciblerobustness,
	title        = {Irreducible robustness error},
	author       = {Anonymous},
	year         = 2024,
	note         = {Accessed: 2024-01-19},
	howpublished = {\url{https://github.com/soumission-anonyme/irreducible-robustness-error}}
}
@inproceedings{baader2024expressivity,
	title        = {Expressivity of Re{LU}-Networks under Convex Relaxations},
	author       = {Maximilian Baader and Mark Niklas Mueller and Yuhao Mao and Martin Vechev},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=awHTL3Hpto}
}
@article{bagheri2020etm,
	title        = {ETM: Enrichment by topic modeling for automated clinical sentence classification to detect patients' disease history},
	author       = {Bagheri, Ayoub and Sammani, Arjan and van der Heijden, Peter G. M. and Asselbergs, Folkert W. and Oberski, Daniel L.},
	year         = 2020,
	month        = {Oct},
	day          = {01},
	journal      = {Journal of Intelligent Information Systems},
	volume       = 55,
	number       = 2,
	pages        = {329--349},
	doi          = {10.1007/s10844-020-00605-w},
	issn         = {1573-7675},
	url          = {https://doi.org/10.1007/s10844-020-00605-w},
	abstract     = {Given the rapid rate at which text data are being digitally gathered in the medical domain, there is growing need for automated tools that can analyze clinical notes and classify their sentences in electronic health records (EHRs). This study uses EHR texts to detect patients' disease history from clinical sentences. However, in EHRs, sentences are less topic-focused and shorter than that in general domain, which leads to the sparsity of co-occurrence patterns and the lack of semantic features. To tackle this challenge, current approaches for clinical sentence classification are dependent on external information to improve classification performance. However, this is implausible owing to a lack of universal medical dictionaries. This study proposes the ETM (enrichment by topic modeling) algorithm, based on latent Dirichlet allocation, to smoothen the semantic representations of short sentences. The ETM enriches text representation by incorporating probability distributions generated by an unsupervised algorithm into it. It considers the length of the original texts to enhance representation by using an internal knowledge acquisition procedure. When it comes to clinical predictive modeling, interpretability improves the acceptance of the model. Thus, for clinical sentence classification, the ETM approach employs an initial TFiDF (term frequency inverse document frequency) representation, where we use the support vector machine and neural network algorithms for the classification task. We conducted three sets of experiments on a data set consisting of clinical cardiovascular notes from the Netherlands to test the sentence classification performance of the proposed method in comparison with prevalent approaches. The results show that the proposed ETM approach outperformed state-of-the-art baselines.}
}
@inproceedings{bahdanau2014neural,
	title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
	author       = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1409.0473},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bai2021recent,
	title        = {Recent Advances in Adversarial Training for Adversarial Robustness},
	author       = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	year         = 2021,
	month        = 8,
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {4312--4321},
	doi          = {10.24963/ijcai.2021/591},
	url          = {https://doi.org/10.24963/ijcai.2021/591},
	note         = {Survey Track},
	editor       = {Zhi-Hua Zhou}
}
@inproceedings{bak2020improved,
	title        = {Improved Geometric Path Enumeration for Verifying ReLU Neural Networks},
	author       = {Bak, Stanley and Tran, Hoang-Dung and Hobbs, Kerianne and Johnson, Taylor T.},
	year         = 2020,
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {66--96},
	isbn         = {978-3-030-53288-8},
	editor       = {Lahiri, Shuvendu K. and Wang, Chao},
	abstract     = {Neural networks provide quick approximations to complex functions, and have been increasingly used in perception as well as control tasks. For use in mission-critical and safety-critical applications, however, it is important to be able to analyze what a neural network can and cannot do. For feed-forward neural networks with ReLU activation functions, although exact analysis is NP-complete, recently-proposed verification methods can sometimes succeed.}
}
@article{balagani2007relationship,
	title        = {On the Relationship Between Dependence Tree Classification Error and Bayes Error Rate},
	author       = {Balagani, Kiran S. and Phoha, Vir V.},
	year         = 2007,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 29,
	number       = 10,
	pages        = {1866--1868},
	doi          = {10.1109/TPAMI.2007.1184}
}
@inproceedings{balunovic2020adversarial,
	title        = {Adversarial Training and Provable Defenses: Bridging the Gap},
	author       = {Mislav Balunovic and Martin T. Vechev},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SJxSDxrKDr},
	timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/BalunovicV20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{banerjee2003extended,
	title        = {Extended Gloss Overlaps as a Measure of Semantic Relatedness},
	author       = {Banerjee, Satanjeev and Pedersen, Ted},
	year         = 2003,
	booktitle    = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
	location     = {Acapulco, Mexico},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {IJCAI'03},
	pages        = {805--810},
	abstract     = {This paper presents a new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses). This measure is unique in that it extends the glosses of the concepts under consideration to include the glosses of other concepts to which they are related according to a given concept hierarchy. We show that this new measure reasonably correlates to human judgments. We introduce a new method of word sense disambiguation based on extended gloss overlaps, and demonstrate that it fares well on the SENSEVAL-2 lexical sample data.},
	numpages     = 6
}
@techreport{bar2015composing,
	title        = {Composing Measures for Computing Text Similarity},
	author       = {Daniel B{\"a}r and Torsten Zesch and Iryna Gurevych},
	year         = 2015,
	month        = {January},
	address      = {Darmstadt, Germany},
	url          = {http://tuprints.ulb.tu-darmstadt.de/4342/},
	language     = {en},
	keywords     = {Text Similarity Plagiarism Paraphrase Recognition},
	abstract     = {We present a comprehensive study of computing similarity between texts. We start from the observation that while the concept of similarity is well grounded in psychology, text similarity is much less well-defined in the natural language processing community. We thus define the notion of text similarity and distinguish it from related tasks such as textual entailment and near-duplicate detection. We then identify multiple text dimensions, i.e. characteristics inherent to texts that can be used to judge text similarity, for which we provide empirical evidence. We discuss state-of-the-art text similarity measures previously proposed in the literature, before continuing with a thorough discussion of common evaluation metrics and datasets. Based on the analysis, we devise an architecture which combines text similarity measures in a unified classification framework. We apply our system in two evaluation settings, for which it consistently outperforms prior work and competing systems: (a) an intrinsic evaluation in the context of the Semantic Textual Similarity Task as part of the Semantic Evaluation (SemEval) exercises, and (b) an extrinsic evaluation for the detection of text reuse. As a basis for future work, we introduce DKPro Similarity, an open source software package which streamlines the development of text similarity measures and complete experimental setups.}
}
@article{barbon2017artificial,
	title        = {Artificial and Natural Topic Detection in Online Social Networks},
	author       = {Barbon Jr, Sylvio and Tavares, Gabriel Marques and Kido, Guilherme Sakaji},
	year         = 2017,
	month        = {Mar.},
	journal      = {iSys - Brazilian Journal of Information Systems},
	volume       = 10,
	number       = 1,
	pages        = {80--98},
	doi          = {10.5753/isys.2017.329},
	url          = {https://sol.sbc.org.br/journals/index.php/isys/article/view/329},
	abstractnote = {Online Social Networks (OSNs), such as Twitter, offer attractive means of social interactions and communications, but also raise privacy and security issues. The OSNs provide valuable information to marketing and competitiveness based on users posts and opinions stored inside a huge volume of data from several themes, topics, and subjects. In order to mining the topics discussed on an OSN we present a novel application of Louvain method for TopicModeling based on communities detection in graphs by modularity. The proposed approach succeeded in finding topics in five different datasets composed of textual content from Twitter and Youtube. Another important contribution achieved was about the presence of texts posted by spammers. In this case, a particular behavior observed by graph community architecture (density and degree) allows the indication of a topic strength and the classification of it as natural or artificial. The later created by the spammers on OSNs.}
}
@article{barkman1969phytosociology,
	title        = {Phytosociology and ecology of cryptogamic epiphytes (including a taxonomic survey and description of their vegetation units in Europe).},
	author       = {Barkman, Jan Johannes and others},
	year         = 1969,
	journal      = {Phytosociology and ecology of cryptogamic epiphytes (including a taxonomic survey and description of their vegetation units in Europe).},
	publisher    = {Van Gorcum  \& Comp. NV}
}
@article{baroni2009wacky,
	title        = {The WaCky wide web: a collection of very large linguistically processed web-crawled corpora},
	author       = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
	year         = 2009,
	month        = {Sep},
	day          = {01},
	journal      = {Language Resources and Evaluation},
	volume       = 43,
	number       = 3,
	pages        = {209--226},
	doi          = {10.1007/s10579-009-9081-4},
	issn         = {1574-0218},
	url          = {https://doi.org/10.1007/s10579-009-9081-4},
	abstract     = {This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.}
}
@book{barrett2018satisfiability,
	title        = {Satisfiability modulo theories},
	author       = {Barrett, Clark and Tinelli, Cesare},
	year         = 2018,
	publisher    = {Springer}
}
@article{barry2018alcohol,
	title        = {Alcohol Advertising on Twitter—A Topic Model},
	author       = {Adam E. Barry and Danny Valdez and Alisa A. Padon and Alex M. Russell},
	year         = 2018,
	journal      = {American Journal of Health Education},
	publisher    = {Routledge},
	volume       = 49,
	number       = 4,
	pages        = {256--263},
	doi          = {10.1080/19325037.2018.1473180},
	url          = {https://doi.org/10.1080/19325037.2018.1473180},
	eprint       = {https://doi.org/10.1080/19325037.2018.1473180}
}
@inproceedings{bastani2016measuring,
	title        = {Measuring Neural Net Robustness with Constraints},
	author       = {Bastani, Osbert and Ioannou, Yani and Lampropoulos, Leonidas and Vytiniotis, Dimitrios and Nori, Aditya and Criminisi, Antonio},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 29,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2016/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{behjati2019universal,
	title        = {Universal Adversarial Attacks on Text Classifiers},
	author       = {Behjati, Melika and Moosavi-Dezfooli, Seyed-Mohsen and Baghshah, Mahdieh Soleymani and Frossard, Pascal},
	year         = 2019,
	booktitle    = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	volume       = {},
	number       = {},
	pages        = {7345--7349},
	doi          = {10.1109/ICASSP.2019.8682430}
}
@article{benedetti2019computing,
	title        = {Computing inter-document similarity with Context Semantic Analysis},
	author       = {Fabio Benedetti and Domenico Beneventano and Sonia Bergamaschi and Giovanni Simonini},
	year         = 2019,
	journal      = {Information Systems},
	volume       = 80,
	pages        = {136--147},
	doi          = {https://doi.org/10.1016/j.is.2018.02.009},
	issn         = {0306-4379},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306437917301503},
	keywords     = {Knowledge base, Knowledge graph, Inter-document similarity, Similarity measures, Information Retrieval},
	abstract     = {We propose a novel knowledge-based technique for inter-document similarity computation, called Context Semantic Analysis (CSA). Several specialized approaches built on top of specific knowledge base (e.g. Wikipedia) exist in literature, but CSA differs from them because it is designed to be portable to any RDF knowledge base. In fact, our technique relies on a generic RDF knowledge base (e.g. DBpedia and Wikidata) to extract from it a Semantic Context Vector, a novel model for representing the context of a document, which is exploited by CSA to compute inter-document similarity effectively. Moreover, we show how CSA can be effectively applied in the Information Retrieval domain. Experimental results show that: (i) for the general task of inter-document similarity, CSA outperforms baselines built on top of traditional methods, and achieves a performance similar to the ones built on top of specific knowledge bases; (ii) for Information Retrieval tasks, enriching documents with context (i.e., employing the Semantic Context Vector model) improves the results quality of the state-of-the-art technique that employs such similar semantic enrichment.}
}
@article{bentley1975multidimensional,
	title        = {Multidimensional binary search trees used for associative searching},
	author       = {Bentley, Jon Louis},
	year         = 1975,
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 18,
	number       = 9,
	pages        = {509--517}
}
@article{berisha2015empirically,
	title        = {Empirically Estimable Classification Bounds Based on a Nonparametric Divergence Measure},
	author       = {Berisha, Visar and Wisler, Alan and Hero, Alfred O. and Spanias, Andreas},
	year         = 2016,
	journal      = {IEEE Transactions on Signal Processing},
	volume       = 64,
	number       = 3,
	pages        = {580--591},
	doi          = {10.1109/TSP.2015.2477805}
}
@article{bertossi2018datalog,
	title        = {Datalog: Bag Semantics via Set Semantics},
	author       = {Leopoldo E. Bertossi and Georg Gottlob and Reinhard Pichler},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.06445},
	url          = {http://arxiv.org/abs/1803.06445},
	eprinttype   = {arXiv},
	eprint       = {1803.06445},
	timestamp    = {Mon, 13 Aug 2018 16:46:24 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06445.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{beyer1999nearest,
	title        = {When Is "Nearest Neighbor" Meaningful?},
	author       = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
	year         = 1999,
	booktitle    = {Database Theory --- ICDT'99},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {217--235},
	isbn         = {978-3-540-49257-3},
	editor       = {Beeri, Catriel and Buneman, Peter},
	abstract     = {We explore the effect of dimensionality on the "nearest neighbor" problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.}
}
@article{bhat2020deep,
	title        = {Deep LDA : A new way to topic model},
	author       = {Muzafar Rasool Bhat and Majid A Kundroo and Tanveer A Tarray and Basant Agarwal},
	year         = 2020,
	journal      = {Journal of Information and Optimization Sciences},
	publisher    = {Taylor  \& Francis},
	volume       = 41,
	number       = 3,
	pages        = {823--834},
	doi          = {10.1080/02522667.2019.1616911},
	url          = {https://doi.org/10.1080/02522667.2019.1616911},
	eprint       = {https://doi.org/10.1080/02522667.2019.1616911}
}
@inproceedings{bhattacharya2019survey,
	title        = {A survey on: Facial emotion recognition invariant to pose, illumination and age},
	author       = {Bhattacharya, Saswati and Gupta, Mousumi},
	year         = 2019,
	booktitle    = {2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)},
	pages        = {1--6},
	organization = {IEEE}
}
@article{bird2009natural,
	title        = {Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit - O'Reilly Media, Beijing, 2009, {ISBN} 978-0-596-51649-9},
	author       = {Wiebke Wagner},
	year         = 2010,
	journal      = {Lang. Resour. Evaluation},
	volume       = 44,
	number       = 4,
	pages        = {421--424},
	doi          = {10.1007/s10579-010-9124-x},
	url          = {https://doi.org/10.1007/s10579-010-9124-x},
	timestamp    = {Thu, 05 Mar 2020 12:05:16 +0100},
	biburl       = {https://dblp.org/rec/journals/lre/Wagner10.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{bishop2006pattern,
	title        = {Pattern recognition and machine learning},
	author       = {Bishop, Christopher M and Nasrabadi, Nasser M},
	year         = 2006,
	publisher    = {Springer},
	volume       = 4,
	isbn         = 9780387310732,
	url          = {https://link.springer.com/book/9780387310732},
	timestamp    = {Fri, 17 Jul 2020 16:12:42 +0200},
	biburl       = {https://dblp.org/rec/books/lib/Bishop07.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{bizer2009dbpedia,
	title        = {DBpedia - A crystallization point for the Web of Data},
	author       = {Christian Bizer and Jens Lehmann and Georgi Kobilarov and Sören Auer and Christian Becker and Richard Cyganiak and Sebastian Hellmann},
	year         = 2009,
	journal      = {Journal of Web Semantics},
	volume       = 7,
	number       = 3,
	pages        = {154--165},
	doi          = {https://doi.org/10.1016/j.websem.2009.07.002},
	issn         = {1570-8268},
	url          = {https://www.sciencedirect.com/science/article/pii/S1570826809000225},
	note         = {The Web of Data},
	keywords     = {Web of Data, Linked Data, Knowledge extraction, Wikipedia, RDF},
	abstract     = {The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.}
}
@article{blank2020pymoo,
	title        = {Pymoo: Multi-Objective Optimization in Python},
	author       = {Blank, Julian and Deb, Kalyanmoy},
	year         = 2020,
	journal      = {IEEE Access},
	volume       = 8,
	number       = {},
	pages        = {89497--89509},
	doi          = {10.1109/ACCESS.2020.2990567}
}
@inproceedings{blei2001latent,
	title        = {Latent Dirichlet Allocation},
	author       = {Blei, David and Ng, Andrew and Jordan, Michael},
	year         = 2001,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = 14,
	url          = {https://proceedings.neurips.cc/paper/2001/file/296472c9542ad4d4788d543508116cbc-Paper.pdf},
	editor       = {T. Dietterich and S. Becker and Z. Ghahramani}
}
@article{blei2003latent,
	title        = {Latent Dirichlet Allocation},
	author       = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	year         = 2003,
	journal      = {J. Mach. Learn. Res.},
	volume       = 3,
	pages        = {993--1022},
	url          = {http://jmlr.org/papers/v3/blei03a.html},
	timestamp    = {Wed, 10 Jul 2019 15:28:30 +0200},
	biburl       = {https://dblp.org/rec/journals/jmlr/BleiNJ03.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{blevins_2010,
	title        = {Topic modeling Martha Ballard's diary: Cameron Blevins},
	author       = {Blevins, Cameron},
	year         = 2010,
	month        = {Apr},
	journal      = {Cameron Blevins |},
	url          = {http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/}
}
@book{blitzstein2015introduction,
	title        = {Introduction to probability},
	author       = {Blitzstein, Joseph K and Hwang, Jessica},
	year         = 2015,
	publisher    = {Crc Press Boca Raton, FL}
}
@book{blitzstein2019introduction,
	title        = {Introduction to probability},
	author       = {Blitzstein, Joseph K and Hwang, Jessica},
	year         = 2019,
	publisher    = {Crc Press}
}
@article{blondel2008fast,
	title        = {Fast unfolding of communities in large networks},
	author       = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
	year         = 2008,
	month        = {oct},
	journal      = {Journal of Statistical Mechanics: Theory and Experiment},
	publisher    = {{IOP} Publishing},
	volume       = 2008,
	number       = 10,
	pages        = {P10008},
	doi          = {10.1088/1742-5468/2008/10/p10008},
	url          = {https://doi.org/10.1088/1742-5468/2008/10/p10008},
	abstract     = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.}
}
@article{blough2001perception,
	title        = {The perception of similarity},
	author       = {Blough, Donald S},
	year         = 2001,
	journal      = {Avian visual cognition},
	volume       = 6,
	pages        = {23--25},
	url          = {https://pigeon.psy.tufts.edu/avc/dblough/default.htm}
}
@inproceedings{bollegala2007measuring,
	title        = {Measuring Semantic Similarity between Words Using Web Search Engines},
	author       = {Bollegala, Danushka and Matsuo, Yutaka and Ishizuka, Mitsuru},
	year         = 2007,
	booktitle    = {Proceedings of the 16th International Conference on World Wide Web},
	location     = {Banff, Alberta, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '07},
	pages        = {757--766},
	doi          = {10.1145/1242572.1242675},
	isbn         = 9781595936547,
	url          = {https://doi.org/10.1145/1242572.1242675},
	numpages     = 10
}
@article{borner2003visualizing,
	title        = {Visualizing knowledge domains},
	author       = {B{\"o}rner, Katy and Chen, Chaomei and Boyack, Kevin W},
	year         = 2003,
	journal      = {Annual review of information science and technology},
	volume       = 37,
	number       = 1,
	pages        = {179--255}
}
@book{bracewell1986fourier,
	title        = {The Fourier transform and its applications},
	author       = {Bracewell, Ronald Newbold and Bracewell, Ronald N},
	year         = 1986,
	publisher    = {McGraw-Hill New York},
	volume       = 31999
}
@inproceedings{brand2002incremental,
	title        = {Incremental Singular Value Decomposition of Uncertain Data with Missing Values},
	author       = {Brand, Matthew},
	year         = 2002,
	booktitle    = {Computer Vision --- ECCV 2002},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {707--720},
	isbn         = {978-3-540-47969-7},
	editor       = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
	abstract     = {We introduce an incremental singular value decomposition (svd) of incomplete data. The svd is developed as data arrives, and can handle arbitrary missing/untrusted values, correlated uncertainty across rows or columns of the measurement matrix, and user priors. Since incomplete data does not uniquely specify an svd, the procedure selects one having minimal rank. For a dense p {\texttimes} q matrix of low rank r, the incremental method has time complexity O(pqr) and space complexity O((p + q)r)---better than highly optimized batch algorithms such as matlab's svd(). In cases of missing data, it produces factorings of lower rank and residual than batch svd algorithms applied to standard missing-data imputations. We show applications in computer vision and audio feature extraction. In computer vision, we use the incremental svd to develop an efficient and unusually robust subspace-estimating flow-based tracker, and to handle occlusions/missing points in structure-from-motion factorizations.}
}
@article{breiman1996bagging,
	title        = {Bagging predictors},
	author       = {Breiman, Leo},
	year         = 1996,
	month        = {Aug},
	day          = {01},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = 24,
	number       = 2,
	pages        = {123--140},
	doi          = {10.1007/BF00058655},
	issn         = {1573-0565},
	url          = {https://doi.org/10.1007/BF00058655},
	abstract     = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.}
}
@article{brilhante2015planning,
	title        = {On planning sightseeing tours with TripBuilder},
	author       = {Igo Ramalho Brilhante and Jose Antonio Macedo and Franco Maria Nardini and Raffaele Perego and Chiara Renso},
	year         = 2015,
	journal      = {Information Processing  \& Management},
	volume       = 51,
	number       = 2,
	pages        = {1--15},
	doi          = {https://doi.org/10.1016/j.ipm.2014.10.003},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457314000922},
	keywords     = {Recommender systems, Trajectory mining, Sightseeing tours},
	abstract     = {We propose TripBuilder, an unsupervised framework for planning personalized sightseeing tours in cities. We collect categorized Points of Interests (PoIs) from Wikipedia and albums of geo-referenced photos from Flickr. By considering the photos as traces revealing the behaviors of tourists during their sightseeing tours, we extract from photo albums spatio-temporal information about the itineraries made by tourists, and we match these itineraries to the Points of Interest (PoIs) of the city. The task of recommending a personalized sightseeing tour is modeled as an instance of the Generalized Maximum Coverage (GMC) problem, where a measure of personal interest for the user given her preferences and visiting time-budget is maximized. The set of actual trajectories resulting from the GMC solution is scheduled on the tourist's agenda by exploiting a particular instance of the Traveling Salesman Problem (TSP). Experimental results on three different cities show that our approach is effective, efficient and outperforms competitive baselines.}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}
@inproceedings{bui2021generative,
	title        = {Generative Pre-training for Paraphrase Generation by Representing and Predicting Spans in Exemplars},
	author       = {Bui, Tien-Cuong and Le, Van-Duc and To, Hai-Thien and Cha, Sang Kyun},
	year         = 2021,
	booktitle    = {2021 IEEE International Conference on Big Data and Smart Computing (BigComp)},
	pages        = {83--90},
	organization = {IEEE}
}
@article{burrows2015eras,
	title        = {The Eras and Trends of Automatic Short Answer Grading},
	author       = {Steven Burrows and Iryna Gurevych and Benno Stein},
	year         = 2015,
	journal      = {Int. J. Artif. Intell. Educ.},
	volume       = 25,
	number       = 1,
	pages        = {60--117},
	doi          = {10.1007/s40593-014-0026-8},
	url          = {https://doi.org/10.1007/s40593-014-0026-8},
	timestamp    = {Tue, 29 Sep 2020 10:56:29 +0200},
	biburl       = {https://dblp.org/rec/journals/aiedu/BurrowsGS15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{camacho2016nasari,
	title        = {Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities},
	author       = {José Camacho-Collados and Mohammad Taher Pilehvar and Roberto Navigli},
	year         = 2016,
	journal      = {Artificial Intelligence},
	volume       = 240,
	pages        = {36--64},
	doi          = {https://doi.org/10.1016/j.artint.2016.07.005},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370216300820},
	keywords     = {Semantic representation, Lexical semantics, Word Sense Disambiguation, Semantic similarity, Sense clustering, Domain labeling},
	abstract     = {Owing to the need for a deep understanding of linguistic items, semantic representation is considered to be one of the fundamental components of several applications in Natural Language Processing and Artificial Intelligence. As a result, semantic representation has been one of the prominent research areas in lexical semantics over the past decades. However, due mainly to the lack of large sense-annotated corpora, most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses. In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively. Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/. As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.}
}
@article{camacho2018word,
	title        = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning},
	author       = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
	year         = 2018,
	month        = {sep},
	journal      = {J. Artif. Int. Res.},
	publisher    = {AI Access Foundation},
	address      = {El Segundo, CA, USA},
	volume       = 63,
	number       = 1,
	pages        = {743--788},
	doi          = {10.1613/jair.1.11259},
	issn         = {1076-9757},
	url          = {https://doi.org/10.1613/jair.1.11259},
	issue_date   = {September 2018},
	abstract     = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
	numpages     = 46
}
@article{cancedda2003word,
	title        = {Word Sequence Kernels},
	author       = {Cancedda, Nicola and Gaussier, Eric and Goutte, Cyril and Renders, Jean Michel},
	year         = 2003,
	month        = {mar},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 3,
	number       = {null},
	pages        = {1059--1082},
	issn         = {1532-4435},
	issue_date   = {3/1/2003},
	abstract     = {We address the problem of categorising documents using kernel-based methods such as Support Vector Machines. Since the work of Joachims (1998), there is ample experimental evidence that SVM using the standard word frequencies as features yield state-of-the-art performance on a number of benchmark problems. Recently, Lodhi et al. (2002) proposed the use of string kernels, a novel way of computing document similarity based of matching non-consecutive subsequences of characters. In this article, we propose the use of this technique with sequences of words rather than characters. This approach has several advantages, in particular it is more efficient computationally and it ties in closely with standard linguistic pre-processing techniques. We present some extensions to sequence kernels dealing with symbol-dependent and match-dependent decay factors, and present empirical evaluations of these extensions on the Reuters-21578 datasets.},
	numpages     = 24
}
@inproceedings{cao2015inferring,
	title        = {Inferring crowd-sourced venues for tweets},
	author       = {Cao, Bokai and Chen, Francine and Joshi, Dhiraj and Yu, Philip S.},
	year         = 2015,
	booktitle    = {2015 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {639--648},
	doi          = {10.1109/BigData.2015.7363808}
}
@inproceedings{cao2017joint,
	title        = {Joint copying and restricted generation for paraphrase},
	author       = {Cao, Ziqiang and Luo, Chuwei and Li, Wenjie and Li, Sujian},
	year         = 2017,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 31,
	number       = 1
}
@inproceedings{cao2017mitigating,
	title        = {Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification},
	author       = {Xiaoyu Cao and Neil Zhenqiang Gong},
	year         = 2017,
	booktitle    = {Proceedings of the 33rd Annual Computer Security Applications Conference, Orlando, FL, USA, December 4-8, 2017},
	publisher    = {{ACM}},
	pages        = {278--287},
	doi          = {10.1145/3134600.3134606},
	url          = {https://doi.org/10.1145/3134600.3134606},
	timestamp    = {Tue, 06 Nov 2018 16:59:23 +0100},
	biburl       = {https://dblp.org/rec/conf/acsac/CaoG17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{carlini2017adversarial,
	title        = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = 2017,
	booktitle    = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {3--14},
	isbn         = 9781450352024,
	url          = {https://doi.org/10.1145/3128572.3140444},
	abstract     = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	numpages     = 12
}
@inproceedings{carlini2017towards,
	title        = {Towards Evaluating the Robustness of Neural Networks},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = 2017,
	booktitle    = {2017 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {39--57},
	doi          = {10.1109/SP.2017.49}
}
@article{carlini2019evaluating,
	title        = {On Evaluating Adversarial Robustness},
	author       = {Nicholas Carlini and Anish Athalye and Nicolas Papernot and Wieland Brendel and Jonas Rauber and Dimitris Tsipras and Ian J. Goodfellow and Aleksander Madry and Alexey Kurakin},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1902.06705},
	url          = {http://arxiv.org/abs/1902.06705},
	eprinttype   = {arXiv},
	eprint       = {1902.06705},
	timestamp    = {Tue, 21 May 2019 18:03:36 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1902-06705.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{carlson2003building,
	title        = {Building a discourse-tagged corpus in the framework of rhetorical structure theory},
	author       = {Carlson, Lynn and Marcu, Daniel and Okurowski, Mary Ellen},
	year         = 2003,
	booktitle    = {Current and new directions in discourse and dialogue},
	publisher    = {Springer},
	pages        = {85--112}
}
@inproceedings{caron2021unsupervised,
	title        = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
	author       = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)}
}
@article{cer2018universal,
	title        = {Universal Sentence Encoder},
	author       = {Daniel Cer and Yinfei Yang and Sheng{-}yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St. John and Noah Constant and Mario Guajardo{-}Cespedes and Steve Yuan and Chris Tar and Yun{-}Hsuan Sung and Brian Strope and Ray Kurzweil},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.11175},
	url          = {http://arxiv.org/abs/1803.11175},
	eprinttype   = {arXiv},
	eprint       = {1803.11175},
	timestamp    = {Mon, 13 Aug 2018 16:46:40 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-11175.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{chakraborty2018adversarial,
	title        = {A survey on adversarial attacks and defences},
	author       = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year         = 2021,
	journal      = {CAAI Transactions on Intelligence Technology},
	volume       = 6,
	number       = 1,
	pages        = {25--45},
	doi          = {https://doi.org/10.1049/cit2.12028},
	url          = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12028},
	keywords     = {security of data, deep learning (artificial intelligence)},
	eprint       = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12028},
	abstract     = {Abstract Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.}
}
@article{chandrasekaran2021evolution,
	title        = {Evolution of Semantic Similarity—A Survey},
	author       = {Chandrasekaran, Dhivya and Mago, Vijay},
	year         = 2021,
	month        = {feb},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 54,
	number       = 2,
	doi          = {10.1145/3440755},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3440755},
	issue_date   = {March 2022},
	abstract     = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
	articleno    = 41,
	numpages     = 37,
	keywords     = {word embeddings, supervised and unsupervised methods, linguistics, knowledge-based methods, corpus-based methods, Semantic similarity}
}
@inproceedings{chen2013emerging,
	title        = {Emerging Topic Detection for Organizations from Microblogs},
	author       = {Chen, Yan and Amiri, Hadi and Li, Zhoujun and Chua, Tat-Seng},
	year         = 2013,
	booktitle    = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Dublin, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '13},
	pages        = {43--52},
	doi          = {10.1145/2484028.2484057},
	isbn         = 9781450320344,
	url          = {https://doi.org/10.1145/2484028.2484057},
	abstract     = {Microblog services have emerged as an essential way to strengthen the communications among individuals and organizations. These services promote timely and active discussions and comments towards products, markets as well as public events, and have attracted a lot of attentions from organizations. In particular, emerging topics are of immediate concerns to organizations since they signal current concerns of, and feedback by their users. Two challenges must be tackled for effective emerging topic detection. One is the problem of real-time relevant data collection and the other is the ability to model the emerging characteristics of detected topics and identify them before they become hot topics. To tackle these challenges, we first design a novel scheme to crawl the relevant messages related to the designated organization by monitoring multi-aspects of microblog content, including users, the evolving keywords and their temporal sequence. We then develop an incremental clustering framework to detect new topics, and employ a range of content and temporal features to help in promptly detecting hot emerging topics. Extensive evaluations on a representative real-world dataset based on Twitter data demonstrate that our scheme is able to characterize emerging topics well and detect them before they become hot topics.},
	numpages     = 10,
	keywords     = {emerging topic detection, organization monitoring, brand monitoring, microblog service}
}
@article{chen2020survey,
	title        = {A survey on adversarial examples in deep learning},
	author       = {Chen, Kai and Zhu, Haoqi and Yan, Leiming and Wang, Jinwei},
	year         = 2020,
	journal      = {Journal on Big Data},
	publisher    = {Tech Science Press},
	volume       = 2,
	number       = 2,
	pages        = 71
}
@article{chen2023evaluating,
	title        = {Evaluating Classification Model Against Bayes Error Rate},
	author       = {Chen, Qingqiang and Cao, Fuyuan and Xing, Ying and Liang, Jiye},
	year         = 2023,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 45,
	number       = 8,
	pages        = {9639--9653},
	doi          = {10.1109/TPAMI.2023.3240194}
}
@article{chernick2002saw,
	title        = {The Saw-Toothed Behavior of Power Versus Sample Size and Software Solutions},
	author       = {Michael R Chernick and Christine Y Liu},
	year         = 2002,
	journal      = {The American Statistician},
	publisher    = {Taylor & Francis},
	volume       = 56,
	number       = 2,
	pages        = {149--155},
	doi          = {10.1198/000313002317572835},
	url          = {https://doi.org/10.1198/000313002317572835},
	eprint       = {https://doi.org/10.1198/000313002317572835}
}
@article{chernoff1959sequential,
	title        = {Sequential Design of Experiments},
	author       = {Herman Chernoff},
	year         = 1959,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 30,
	number       = 3,
	pages        = {755--770},
	issn         = {00034851},
	url          = {http://www.jstor.org/stable/2237415},
	urldate      = {2023-05-10}
}
@inproceedings{chiang2020certified,
	title        = {Certified Defenses for Adversarial Patches},
	author       = {Ping{-}yeh Chiang and Renkun Ni and Ahmed Abdelkader and Chen Zhu and Christoph Studer and Tom Goldstein},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HyeaSkrYPH},
	timestamp    = {Mon, 02 May 2022 17:13:05 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/ChiangNAZSG20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{chicco2021siamese,
	title        = {Siamese Neural Networks: An Overview},
	author       = {Chicco, Davide},
	year         = 2021,
	booktitle    = {Artificial Neural Networks},
	publisher    = {Springer US},
	address      = {New York, NY},
	pages        = {73--94},
	doi          = {10.1007/978-1-0716-0826-5_3},
	isbn         = {978-1-0716-0826-5},
	url          = {https://doi.org/10.1007/978-1-0716-0826-5_3},
	abstract     = {Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman's rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.}
}
@inproceedings{chopra2005learning,
	title        = {Learning a similarity metric discriminatively, with application to face verification},
	author       = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	year         = 2005,
	booktitle    = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
	volume       = 1,
	number       = {},
	pages        = {539--546 vol. 1},
	doi          = {10.1109/CVPR.2005.202}
}
@article{chowdhery2022palm,
	title        = {PaLM: Scaling Language Modeling with Pathways},
	author       = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur{-}Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier{-}Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2204.02311},
	doi          = {10.48550/arXiv.2204.02311},
	url          = {https://doi.org/10.48550/arXiv.2204.02311},
	eprinttype   = {arXiv},
	eprint       = {2204.02311},
	timestamp    = {Thu, 07 Apr 2022 08:26:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2204-02311.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{chu2019meansum,
	title        = {{M}ean{S}um: A Neural Model for Unsupervised Multi-Document Abstractive Summarization},
	author       = {Chu, Eric and Liu, Peter},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {1223--1232},
	url          = {https://proceedings.mlr.press/v97/chu19b.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/chu19b/chu19b.pdf},
	abstract     = {Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outperforms a strong extractive baseline.}
}
@article{chudova2003analysis,
	title        = {Analysis of Pattern Discovery in Sequences Using a Bayes Error Framework},
	author       = {Chudova, Darya and Smyth, Padhraic},
	year         = 2003,
	month        = {Jul},
	day          = {01},
	journal      = {Data Mining and Knowledge Discovery},
	volume       = 7,
	number       = 3,
	pages        = {273--299},
	doi          = {10.1023/A:1024032204965},
	issn         = {1573-756X},
	url          = {https://doi.org/10.1023/A:1024032204965},
	abstract     = {In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. There are a number of fundamental aspects of this data mining problem that can make discovery ``easy'' or ``hard''---we characterize the difficulty of this problem using an analysis based on the Bayes error rate under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology.}
}
@article{cilibrasi2007google,
	title        = {The Google Similarity Distance},
	author       = {Cilibrasi, Rudi L. and Vitanyi, Paul M.B.},
	year         = 2007,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 19,
	number       = 3,
	pages        = {370--383},
	doi          = {10.1109/TKDE.2007.48}
}
@article{clarke2008global,
	title        = {Global Inference for Sentence Compression: An Integer Linear Programming Approach},
	author       = {James Clarke and Mirella Lapata},
	year         = 2008,
	journal      = {J. Artif. Intell. Res.},
	volume       = 31,
	pages        = {399--429},
	doi          = {10.1613/jair.2433},
	url          = {https://doi.org/10.1613/jair.2433},
	timestamp    = {Mon, 21 Jan 2019 15:01:18 +0100},
	biburl       = {https://dblp.org/rec/journals/jair/ClarkeL08.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cohan2016revisiting,
	title        = {Revisiting Summarization Evaluation for Scientific Articles},
	author       = {Arman Cohan and Nazli Goharian},
	year         = 2016,
	booktitle    = {Proceedings of the Tenth International Conference on Language Resources and Evaluation {LREC} 2016, Portoro{\v{z}}, Slovenia, May 23-28, 2016},
	publisher    = {European Language Resources Association {(ELRA)}},
	url          = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/1144.html},
	editor       = {Nicoletta Calzolari and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and H{\'{e}}l{\`{e}}ne Mazo and Asunci{\'{o}}n Moreno and Jan Odijk and Stelios Piperidis},
	timestamp    = {Mon, 19 Aug 2019 15:22:28 +0200},
	biburl       = {https://dblp.org/rec/conf/lrec/CohanG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cohen2019certified,
	title        = {Certified Adversarial Robustness via Randomized Smoothing},
	author       = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {1310--1320},
	url          = {https://proceedings.mlr.press/v97/cohen19c.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf},
	organization = {PMLR}
}
@inproceedings{collins2001convolution,
	title        = {Convolution Kernels for Natural Language},
	author       = {Collins, Michael and Duffy, Nigel},
	year         = 2001,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = 14,
	url          = {https://proceedings.neurips.cc/paper/2001/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf},
	editor       = {T. Dietterich and S. Becker and Z. Ghahramani}
}
@article{colombo2021infolm,
	title        = {InfoLM: {A} New Metric to Evaluate Summarization {\&} Data2Text Generation},
	author       = {Pierre Colombo and Chlo{\'{e}} Clavel and Pablo Piantanida},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2112.01589},
	url          = {https://arxiv.org/abs/2112.01589},
	eprinttype   = {arXiv},
	eprint       = {2112.01589},
	timestamp    = {Fri, 21 Jan 2022 20:43:46 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2112-01589.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{colwell1982spearman,
	title        = {Spearman versus Kendall},
	author       = {Colwell, D. J. and Gillett, J. R.},
	year         = 1982,
	journal      = {The Mathematical Gazette},
	publisher    = {Cambridge University Press},
	volume       = 66,
	number       = 438,
	pages        = {307--309},
	doi          = {10.2307/3615525}
}
@techreport{cooper1996using,
	title        = {Using the framework},
	author       = {Cooper, Robin and Crouch, Dick and Van Eijck, Jan and Fox, Chris and Van Genabith, Johan and Jaspars, Jan and Kamp, Hans and Milward, David and Pinkal, Manfred and Poesio, Massimo and others},
	year         = 1996,
	institution  = {Technical Report LRE 62-051 D-16, The FraCaS Consortium}
}
@book{cormen2022introduction,
	title        = {Introduction to Algorithms, 3rd Edition},
	author       = {Thomas H. Cormen and Charles E. Leiserson and Ronald L. Rivest and Clifford Stein},
	year         = 2009,
	publisher    = {{MIT} Press},
	isbn         = {978-0-262-03384-8},
	url          = {http://mitpress.mit.edu/books/introduction-algorithms},
	timestamp    = {Mon, 17 Aug 2020 11:36:12 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0023376.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{croce2020minimally,
	title        = {Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack},
	author       = {Croce, Francesco and Hein, Matthias},
	year         = 2020,
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 119,
	pages        = {2196--2205},
	url          = {https://proceedings.mlr.press/v119/croce20a.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/croce20a/croce20a.pdf},
	abstract     = {The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.}
}
@inproceedings{croce2020reliable,
	title        = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	author       = {Croce, Francesco and Hein, Matthias},
	year         = 2020,
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 119,
	pages        = {2206--2216},
	url          = {https://proceedings.mlr.press/v119/croce20b.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/croce20b/croce20b.pdf},
	abstract     = {The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	organization = {PMLR}
}
@article{croce2020scaling,
	title        = {Scaling up the Randomized Gradient-Free Adversarial Attack Reveals Overestimation of Robustness Using Established Attacks},
	author       = {Croce, Francesco and Rauber, Jonas and Hein, Matthias},
	year         = 2020,
	month        = {Apr},
	day          = {01},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer},
	volume       = 128,
	number       = 4,
	pages        = {1028--1046},
	doi          = {10.1007/s11263-019-01213-0},
	issn         = {1573-1405},
	url          = {https://doi.org/10.1007/s11263-019-01213-0},
	abstract     = {Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks (Croce and Hein in GCPR, 2018), in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.}
}
@inproceedings{cronen2002quantifying,
	title        = {Quantifying query ambiguity},
	author       = {Cronen-Townsend, Steve and Croft, W Bruce and others},
	year         = 2002,
	booktitle    = {Proceedings of HLT},
	volume       = 2,
	pages        = {94--98}
}
@misc{cs231n,
	title        = {CS231n Convolutional Neural Networks for Visual Recognition},
	journal      = {CS231N convolutional neural networks for visual recognition},
	url          = {https://cs231n.github.io/convolutional-networks/}
}
@misc{ctvnews2012danone,
	title        = {Danone to settle lawsuit over activia yogurt, Danactive Health claims},
	author       = {CTVNews},
	year         = 2012,
	month        = {Sep},
	journal      = {CTVNews},
	publisher    = {The Canadian Press},
	url          = {https://www.ctvnews.ca/health/health-headlines/danone-to-settle-lawsuit-over-activia-yogurt-danactive-health-claims-1.971371?cache=%3FclipId%3D375756}
}
@article{dagan_dolan_magnini_roth_2010,
	title        = {Recognizing textual entailment: Rational, evaluation and approaches - Erratum},
	author       = {DAGAN, IDO and DOLAN, BILL and MAGNINI, BERNARDO and ROTH, DAN},
	year         = 2010,
	journal      = {Natural Language Engineering},
	publisher    = {Cambridge University Press},
	volume       = 16,
	number       = 1,
	pages        = {105--105},
	doi          = {10.1017/S1351324909990234}
}
@article{dagan1999contextual,
	title        = {Contextual word similarity},
	author       = {Dagan, Ido},
	year         = 1999,
	journal      = {Handbook of Natural Language Processing},
	publisher    = {Marcel Dekker, inc.},
	pages        = {Chapter--19}
}
@inproceedings{dagan2005pascal,
	title        = {The PASCAL Recognising Textual Entailment Challenge},
	author       = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
	year         = 2006,
	booktitle    = {Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {177--190},
	isbn         = {978-3-540-33428-6},
	editor       = {Qui{\~{n}}onero-Candela, Joaquin and Dagan, Ido and Magnini, Bernardo and d'Alch{\'e}-Buc, Florence},
	abstract     = {This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.}
}
@article{dahal2019topic,
	title        = {Topic modeling and sentiment analysis of global climate change tweets},
	author       = {Dahal, Biraj and Kumar, Sathish A. P. and Li, Zhenlong},
	year         = 2019,
	month        = {Jun},
	day          = 10,
	journal      = {Social Network Analysis and Mining},
	volume       = 9,
	number       = 1,
	pages        = 24,
	doi          = {10.1007/s13278-019-0568-8},
	issn         = {1869-5469},
	url          = {https://doi.org/10.1007/s13278-019-0568-8},
	abstract     = {Social media websites can be used as a data source for mining public opinion on a variety of subjects including climate change. Twitter, in particular, allows for the evaluation of public opinion across both time and space because geotagged tweets include timestamps and geographic coordinates (latitude/longitude). In this study, a large dataset of geotagged tweets containing certain keywords relating to climate change is analyzed using volume analysis and text mining techniques such as topic modeling and sentiment analysis. Latent Dirichlet allocation was applied for topic modeling to infer the different topics of discussion, and Valence Aware Dictionary and sEntiment Reasoner was applied for sentiment analysis to determine the overall feelings and attitudes found in the dataset. These techniques are used to compare and contrast the nature of climate change discussion between different countries and over time. Sentiment analysis shows that the overall discussion is negative, especially when users are reacting to political or extreme weather events. Topic modeling shows that the different topics of discussion on climate change are diverse, but some topics are more prevalent than others. In particular, the discussion of climate change in the USA is less focused on policy-related topics than other countries.}
}
@article{dai2015document,
	title        = {Document Embedding with Paragraph Vectors},
	author       = {Andrew M. Dai and Christopher Olah and Quoc V. Le},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1507.07998},
	url          = {http://arxiv.org/abs/1507.07998},
	eprinttype   = {arXiv},
	eprint       = {1507.07998},
	timestamp    = {Mon, 13 Aug 2018 16:47:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/DaiOL15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dai2017social,
	title        = {From social media to public health surveillance: Word embedding based clustering method for twitter classification},
	author       = {Dai, Xiangfeng and Bikdash, Marwan and Meyer, Bradley},
	year         = 2017,
	booktitle    = {SoutheastCon 2017},
	pages        = {1--7},
	doi          = {10.1109/SECON.2017.7925400}
}
@article{das1999preference,
	title        = {A preference ordering among various Pareto optimal alternatives},
	author       = {Das, I.},
	year         = 1999,
	month        = {Aug},
	day          = {01},
	journal      = {Structural optimization},
	volume       = 18,
	number       = 1,
	pages        = {30--35},
	doi          = {10.1007/BF01210689},
	issn         = {1615-1488},
	url          = {https://doi.org/10.1007/BF01210689},
	abstract     = {It is often necessary to choose a Pareto optimal point from a set of many. This paper introduces the concept of order of efficiency, which provides a notion that is stronger than Pareto optimality and allows us to set up a preference ordering amongst various alternatives that are Pareto optimal. This approach does not resort to setting up a ranking on the basis of an arbitrary ``criterion of merit'' obtained by combining the multiple decision criteria into one scalar index. Examples are cited and it is argued that using the procedure described in this paper, it is possible to rule out Pareto alternatives with ``extreme components'' and retain alternatives ``in the middle'' of the Pareto set without the help of plots or other visualization aids. This makes the approach applicable for cases where the number of criteria is very high and visualization is intractable.}
}
@techreport{de2008stanford,
	title        = {Stanford typed dependencies manual},
	author       = {De Marneffe, Marie-Catherine and Manning, Christopher D},
	year         = 2008,
	institution  = {Technical report, Stanford University}
}
@inproceedings{de2009cross,
	title        = {Cross-Language Linking of News Stories on the Web Using Interlingual Topic Modelling},
	author       = {De Smet, Wim and Moens, Marie-Francine},
	year         = 2009,
	booktitle    = {Proceedings of the 2nd ACM Workshop on Social Web Search and Mining},
	location     = {Hong Kong, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SWSM '09},
	pages        = {57--64},
	doi          = {10.1145/1651437.1651447},
	isbn         = 9781605588063,
	url          = {https://doi.org/10.1145/1651437.1651447},
	abstract     = {We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News.},
	numpages     = 8,
	keywords     = {latent dirichlet allocation, event detection}
}
@article{deaton2019transformers,
	title        = {Transformers and pointer-generator networks for abstractive summarization},
	author       = {Deaton, Jon and Jacobs, Austin and Kenealy, Kathleen and See, Abigail}
}
@inproceedings{deb2007self,
	title        = {Self-adaptive simulated binary crossover for real-parameter optimization},
	author       = {Kalyanmoy Deb and Karthik Sindhya and Tatsuya Okabe},
	year         = 2007,
	booktitle    = {Genetic and Evolutionary Computation Conference, {GECCO} 2007, Proceedings, London, England, UK, July 7-11, 2007},
	publisher    = {{ACM}},
	pages        = {1187--1194},
	doi          = {10.1145/1276958.1277190},
	url          = {https://doi.org/10.1145/1276958.1277190},
	editor       = {Hod Lipson},
	timestamp    = {Tue, 06 Nov 2018 11:06:41 +0100},
	biburl       = {https://dblp.org/rec/conf/gecco/DebSO07.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{deerwester1990indexing,
	title        = {Indexing by latent semantic analysis},
	author       = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year         = 1990,
	journal      = {Journal of the American Society for Information Science},
	volume       = 41,
	number       = 6,
	pages        = {391--407},
	doi          = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	url          = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
	eprint       = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
	abstract     = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley  \& Sons, Inc.}
}
@book{degroot2012probability,
	title        = {Probability and statistics},
	author       = {DeGroot, Morris H and Schervish, Mark J},
	year         = 2012,
	publisher    = {Pearson Education}
}
@phdthesis{demartines1994analyse,
	title        = {Analyse de donn{\'e}es par r{\'e}seaux de neurones auto-organis{\'e}s},
	author       = {Demartines, Pierre},
	year         = 1994,
	school       = {Grenoble INPG}
}
@inproceedings{demontis2019adversarial,
	title        = {Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks},
	author       = {Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio},
	year         = 2019,
	booktitle    = {Proceedings of the 28th USENIX Conference on Security Symposium},
	location     = {Santa Clara, CA, USA},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {SEC'19},
	pages        = {321--338},
	isbn         = 9781939133069,
	abstract     = {Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transferability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferability: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack's transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.},
	numpages     = 18
}
@inproceedings{deng2019arcface,
	title        = {ArcFace: Additive Angular Margin Loss for Deep Face Recognition},
	author       = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	year         = 2019,
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {4685--4694},
	doi          = {10.1109/CVPR.2019.00482}
}
@inproceedings{deng2019retinaface,
	title        = {RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild},
	author       = {Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{dhillon2000technical,
	title        = {Technical Opinion: Information System Security Management in the New Millennium},
	author       = {Dhillon, Gurpreet and Backhouse, James},
	year         = 2000,
	month        = {jul},
	journal      = {Commun. ACM},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 43,
	number       = 7,
	pages        = {125–128},
	doi          = {10.1145/341852.341877},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/341852.341877},
	issue_date   = {July 2000},
	numpages     = 4
}
@inproceedings{di2020topfilter,
	title        = {TopFilter: An Approach to Recommend Relevant GitHub Topics},
	author       = {Di Rocco, Juri and Di Ruscio, Davide and Di Sipio, Claudio and Nguyen, Phuong and Rubei, Riccardo},
	year         = 2020,
	booktitle    = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
	location     = {Bari, Italy},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ESEM '20},
	doi          = {10.1145/3382494.3410690},
	isbn         = 9781450375801,
	url          = {https://doi.org/10.1145/3382494.3410690},
	abstract     = {Background: In the context of software development, GitHub has been at the forefront of platforms to store, analyze and maintain a large number of software repositories. Topics have been introduced by GitHub as an effective method to annotate stored repositories. However, labeling GitHub repositories should be carefully conducted to avoid adverse effects on project popularity and reachability. Aims: We present TopFilter, a novel approach to assist open source software developers in selecting suitable topics for GitHub repositories being created. Method: We built a project-topic matrix and applied a syntactic-based similarity function to recommend missing topics by representing repositories and related topics in a graph. The ten-fold cross-validation methodology has been used to assess the performance of TopFilter by considering different metrics, i.e., success rate, precision, recall, and catalog coverage. Result: The results show that TopFilter recommends good topics depending on different factors, i.e., collaborative filtering settings, considered datasets, and pre-processing activities. Moreover, TopFilter can be combined with a state-of-the-art topic recommender system (i.e., MNB network) to improve the overall prediction performance. Conclusion: Our results confirm that collaborative filtering techniques can successfully be used to provide relevant topics for GitHub repositories. Moreover, TopFilter can gain a significant boost in prediction performances by employing the outcomes obtained by the MNB network as its initial set of topics.},
	articleno    = 21,
	numpages     = 11,
	keywords     = {Collaborative filtering, Recommender systems, GitHub topics recommendation}
}
@inproceedings{ding2021repvgg,
	title        = {RepVGG: Making VGG-Style ConvNets Great Again},
	author       = {Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
	year         = 2021,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {13733--13742}
}
@inproceedings{dinu2009ordinal,
	title        = {Ordinal measures in authorship identification},
	author       = {Dinu, Liviu P and Popescu, Marius},
	year         = 2009,
	booktitle    = {Proc. SEPLN},
	volume       = 9,
	pages        = {62--66}
}
@inproceedings{doddington2002automatic,
	title        = {Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics},
	author       = {Doddington, George},
	year         = 2002,
	booktitle    = {Proceedings of the Second International Conference on Human Language Technology Research},
	location     = {San Diego, California},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {HLT '02},
	pages        = {138--145},
	abstract     = {Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&amp;D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an "evaluation understudy", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research.},
	numpages     = 8
}
@inproceedings{dolan2005automatically,
	title        = {Automatically constructing a corpus of sentential paraphrases},
	author       = {Dolan, William B and Brockett, Chris},
	year         = 2005,
	booktitle    = {Proceedings of the Third International Workshop on Paraphrasing (IWP2005)}
}
@article{dolan2005microsoft,
	title        = {Microsoft research paraphrase corpus},
	author       = {Dolan, Bill and Brockett, Chris and Quirk, Chris},
	year         = 2005,
	journal      = {Retrieved March},
	volume       = 29,
	number       = 2008,
	pages        = 63,
	url          = {http://research.microsoft.com/research/nlp/msr_paraphrase.htm}
}
@inproceedings{dong2018boosting,
	title        = {Boosting Adversarial Attacks With Momentum},
	author       = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	year         = 2018,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{dong2019evading,
	title        = {Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks},
	author       = {Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{dong2019unified,
	title        = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
	author       = {Li Dong and Nan Yang and Wenhui Wang and Furu Wei and Xiaodong Liu and Yu Wang and Jianfeng Gao and Ming Zhou and Hsiao{-}Wuen Hon},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
	pages        = {13042--13054},
	url          = {https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html},
	editor       = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	timestamp    = {Thu, 21 Jan 2021 15:15:20 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/00040WWLWGZH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dosovitskiy2020image,
	title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year         = 2021,
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=YicbFdNTTy},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{duvcinskas2011expected,
	title        = {Expected Bayes Error Rate in Supervised Classification of Spatial Gaussian Data},
	author       = {Du\v{c}inskas, Kundefinedstutis and Stabingienundefined, Lijana},
	year         = 2011,
	month        = {jul},
	journal      = {Informatica},
	publisher    = {IOS Press},
	address      = {NLD},
	volume       = 22,
	number       = 3,
	pages        = {371–381},
	issn         = {0868-4952},
	issue_date   = {July 2011},
	abstract     = {In the usual statistical approach of spatial classification, it is assumed that the feature observations are independent conditionally on class labels (conditional independence). Discarding this popular assumption, we consider the problem of statistical classification by using multivariate stationary Gaussian Random Field (GRF) for modeling the conditional distribution given class labels of feature observations. The classes are specified by multivariate regression model for means and by common factorized covariance function. In the two-class case and for the class labels modeled by Random Field (RF) based on 0–1 divergence, the formula of the Expected Bayes Error Rate (EBER) is derived. The effect of training sample size on the EBER and the influence of statistical parameters to the values of EBER are numerically evaluated in the case when the spatial framework of data is the subset of the 2-dimensional rectangular lattice with unit spacing.},
	numpages     = 11,
	keywords     = {Divergence, Gaussian Random Fields, Spatial Correlation, Bayes Discriminant Function}
}
@article{duvcinskas2015actual,
	title        = {Actual Error Rates in Classification of the T-Distributed Random Field Observation Based on Plug-in Linear Discriminant Function},
	author       = {Du\v{c}inskas, Kundefinedstutis and Zikarienundefined, Eglundefined},
	year         = 2015,
	month        = {jan},
	journal      = {Informatica},
	publisher    = {IOS Press},
	address      = {NLD},
	volume       = 26,
	number       = 4,
	pages        = {557–568},
	issn         = {0868-4952},
	issue_date   = 2015,
	abstract     = {In current paper a problem of classification of T-distributed random field observation into one of two populations specified by common scaling function is considered. The ML and LS estimators of the mean parameters are plugged into the linear discriminant function. The closed form expressions for the Bayes error rate and the actual error rate associated with the aforementioned discriminant functions are derived. This is the extension of one for the Gaussian case. The actual error rates are used to evaluate and compare the performance of the plug-in discriminant function by means of Monte Carlo study.},
	numpages     = 12,
	keywords     = {actual error rate, T-distributed random field, Bayes rule, spatial correlation, scaling function}
}
@article{duvsek2020evaluating,
	title        = {Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge},
	author       = {Ondřej Dušek and Jekaterina Novikova and Verena Rieser},
	year         = 2020,
	journal      = {Computer Speech  \& Language},
	volume       = 59,
	pages        = {123--156},
	doi          = {https://doi.org/10.1016/j.csl.2019.06.009},
	issn         = {0885-2308},
	url          = {https://www.sciencedirect.com/science/article/pii/S0885230819300919},
	abstract     = {This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures - with the majority implementing sequence-to-sequence models (seq2seq) - as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness - with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.}
}
@inproceedings{ehlers2017formal,
	title        = {Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks},
	author       = {Ehlers, R{\"u}diger},
	year         = 2017,
	booktitle    = {Automated Technology for Verification and Analysis},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {269--286},
	isbn         = {978-3-319-68167-2},
	editor       = {D'Souza, Deepak and Narayan Kumar, K.},
	abstract     = {We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.}
}
@article{engstrom2017rotation,
	title        = {A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations},
	author       = {Logan Engstrom and Dimitris Tsipras and Ludwig Schmidt and Aleksander Madry},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1712.02779},
	url          = {http://arxiv.org/abs/1712.02779},
	eprinttype   = {arXiv},
	eprint       = {1712.02779},
	timestamp    = {Mon, 13 Aug 2018 16:48:14 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1712-02779.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{enwiki:1091909435,
	title        = {Handedness --- {Wikipedia}{,} The Free Encyclopedia},
	author       = {{Wikipedia}},
	url          = {https://en.wikipedia.org/w/index.php?title=Handedness&oldid=1091909435},
	note         = {[Online; accessed 19-June-2022]}
}
@article{estes1955statistical,
	title        = {Statistical theory of spontaneous recovery and regression.},
	author       = {Estes, William K},
	year         = 1955,
	journal      = {Psychological review},
	volume       = 62,
	number       = 3,
	url          = {https://doi.org/10.1037/h0048509}
}
@misc{ets_2022,
	title        = {Sample essay responses and Rater Commentary for the argument task},
	author       = {ETS},
	year         = 2022,
	journal      = {ETS},
	url          = {https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/sample_responses}
}
@inproceedings{evtimov2018robust,
	title        = {Robust physical-world attacks on deep learning visual classification},
	author       = {Evtimov, Ivan and Eykholt, Kevin and Fernandes, Earlence and Kohno, Tadayoshi and Li, Bo and Prakash, Atul and Rahmati, Amir and Song, Dawn},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {1625--1634}
}
@article{faber2014sample,
	title        = {How sample size influences research outcomes},
	author       = {Faber, Jorge and Fonseca, Lilian Martins},
	year         = 2014,
	month        = jul,
	journal      = {Dental Press J Orthod},
	address      = {Brazil},
	volume       = 19,
	number       = 4,
	pages        = {27--29},
	abstract     = {Sample size calculation is part of the early stages of conducting an epidemiological, clinical or laboratory study. In preparing a scientific paper, there are ethical and methodological indications for its use. Two investigations conducted with the same methodology and achieving equivalent results, but different only in terms of sample size, may point the researcher in different directions when it comes to making clinical decisions. Therefore, ideally, samples should not be small and, contrary to what one might think, should not be excessive. The aim of this paper is to discuss in clinical language the main implications of the sample size when interpreting a study.},
	keywords     = {Clinical trial; Methodology; Sample calculation; Sample size; Scientific evidence},
	language     = {en}
}
@article{fact2019report,
	title        = {Report on the Facebook Third Party Fact Checking programme},
	author       = {Fact, Full},
	year         = 2019,
	month        = {Jul},
	journal      = {London, Full Fact},
	url          = {https://fullfact.org/media/uploads/tpfc-q1q2-2019.pdf}
}
@inproceedings{fader2014open,
	title        = {Open question answering over curated and extracted knowledge bases},
	author       = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
	year         = 2014,
	booktitle    = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {1156--1165}
}
@inproceedings{fang2012mining,
	title        = {Mining Contrastive Opinions on Political Texts Using Cross-Perspective Topic Model},
	author       = {Fang, Yi and Si, Luo and Somasundaram, Naveen and Yu, Zhengtao},
	year         = 2012,
	booktitle    = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
	location     = {Seattle, Washington, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WSDM '12},
	pages        = {63--72},
	doi          = {10.1145/2124295.2124306},
	isbn         = 9781450307475,
	url          = {https://doi.org/10.1145/2124295.2124306},
	abstract     = {This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model.},
	numpages     = 10,
	keywords     = {topic modeling, contrastive opinions, opinion mining, opinion retrieval}
}
@inbook{fellbaum2010wordnet,
	title        = {WordNet},
	author       = {Fellbaum, Christiane},
	year         = 2010,
	booktitle    = {Theory and Applications of Ontology: Computer Applications},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {231--243},
	doi          = {10.1007/978-90-481-8847-5_10},
	isbn         = {978-90-481-8847-5},
	url          = {https://doi.org/10.1007/978-90-481-8847-5_10},
	editor       = {Poli, Roberto and Healy, Michael and Kameas, Achilles},
	abstract     = {WordNet is a large electronic lexical database for English (Miller 1995, Fellbaum 1998a). It originated in 1986 at Princeton University where it continues to be developed and maintained. George A. Miller, a psycholinguist, was inspired by experiments in Artificial Intelligence that tried to understand human semantic memory (e.g., Collins and Quillian 1969). Given the fact that speakers possess knowledge about tens of thousands of words and the concepts expressed by these words, it seemed reasonable to assume efficient and economic storage and access mechanisms for words and concepts. The Collins and Quillian model proposed a hierarchical structure of concepts, where more specific concepts inherit information from their superordinate, more general concepts; only knowledge particular to more specific concepts needs to be stored with such concepts. Thus, it took subjects longer to confirm a statement like "canaries have feathers" than the statement "birds have feathers" since, presumably, the property "has-feathers" is stored with the concept bird and not redundantly with the concept for each kind of bird.}
}
@inproceedings{filighera2020fooling,
	title        = {Fooling Automatic Short Answer Grading Systems},
	author       = {Filighera, Anna and Steuer, Tim and Rensing, Christoph},
	year         = 2020,
	booktitle    = {Artificial Intelligence in Education},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {177--190},
	isbn         = {978-3-030-52237-7},
	editor       = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Mill{\'a}n, Eva},
	abstract     = {With the rising success of adversarial attacks on many NLP tasks, systems which actually operate in an adversarial scenario need to be reevaluated. For this purpose, we pose the following research question: How difficult is it to fool automatic short answer grading systems? In particular, we investigate the robustness of the state of the art automatic short answer grading system proposed by Sung et al. towards cheating in the form of universal adversarial trigger employment. These are short token sequences that can be prepended to students' answers in an exam to artificially improve their automatically assigned grade. Such triggers are especially critical as they can easily be used by anyone once they are found. In our experiments, we discovered triggers which allow students to pass exams with passing thresholds of {\$}{\$}50{\backslash}{\%}{\$}{\$}without answering a single question correctly. Furthermore, we show that such triggers generalize across models and datasets in this scenario, nullifying the defense strategy of keeping grading models or data secret.}
}
@article{filighera2022cheating,
	title        = {Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs},
	author       = {Anna Filighera and Sebastian Ochs and Tim Steuer and Thomas Tregel},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2201.08318},
	url          = {https://arxiv.org/abs/2201.08318},
	eprinttype   = {arXiv},
	eprint       = {2201.08318},
	timestamp    = {Tue, 01 Feb 2022 14:59:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2201-08318.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{finlayson2019adversarial,
	title        = {Adversarial attacks on medical machine learning},
	author       = {Samuel G. Finlayson  and John D. Bowers  and Joichi Ito  and Jonathan L. Zittrain  and Andrew L. Beam  and Isaac S. Kohane},
	year         = 2019,
	journal      = {Science},
	volume       = 363,
	number       = 6433,
	pages        = {1287--1289},
	doi          = {10.1126/science.aaw4399},
	url          = {https://www.science.org/doi/abs/10.1126/science.aaw4399},
	eprint       = {https://www.science.org/doi/pdf/10.1126/science.aaw4399},
	abstract     = {Emerging vulnerabilities demand new conversations With public and academic attention increasingly focused on the new role of machine learning in the health information economy, an unusual and no-longer-esoteric category of vulnerabilities in machine-learning systems could prove important. These vulnerabilities allow a small, carefully designed change in how inputs are presented to a system to completely alter its output, causing it to confidently arrive at manifestly wrong conclusions. These advanced techniques to subvert otherwise-reliable machine-learning systems—so-called adversarial attacks—have, to date, been of interest primarily to computer science researchers (1). However, the landscape of often-competing interests within health care, and billions of dollars at stake in systems' outputs, implies considerable problems. We outline motivations that various players in the health care system may have to use adversarial attacks and begin a discussion of what to do about them. Far from discouraging continued innovation with medical machine learning, we call for active engagement of medical, technical, legal, and ethical experts in pursuit of efficient, broadly available, and effective health care that machine learning will enable.}
}
@article{floridi2020gpt,
	title        = {GPT-3: Its Nature, Scope, Limits, and Consequences},
	author       = {Floridi, Luciano and Chiriatti, Massimo},
	year         = 2020,
	month        = {Dec},
	day          = {01},
	journal      = {Minds and Machines},
	volume       = 30,
	number       = 4,
	pages        = {681--694},
	doi          = {10.1007/s11023-020-09548-1},
	issn         = {1572-8641},
	url          = {https://doi.org/10.1007/s11023-020-09548-1},
	abstract     = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.}
}
@inproceedings{forgues2014bootstrapping,
	title        = {Bootstrapping dialog systems with word embeddings},
	author       = {Forgues, Gabriel and Pineau, Joelle and Larchev{\^e}que, Jean-Marie and Tremblay, R{\'e}al},
	year         = 2014,
	booktitle    = {Nips, modern machine learning and natural language processing workshop},
	volume       = 2,
	pages        = 168
}
@article{fortunato2010community,
	title        = {Community detection in graphs},
	author       = {Santo Fortunato},
	year         = 2010,
	journal      = {Physics Reports},
	volume       = 486,
	number       = 3,
	pages        = {75--174},
	doi          = {https://doi.org/10.1016/j.physrep.2009.11.002},
	issn         = {0370-1573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	keywords     = {Graphs, Clusters, Statistical physics},
	abstract     = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.}
}
@article{frasch2011bayes,
	title        = {A Bayes-true data generator for evaluation of supervised and unsupervised learning methods},
	author       = {Janick V. Frasch and Aleksander Lodwich and Faisal Shafait and Thomas M. Breuel},
	year         = 2011,
	journal      = {Pattern Recognition Letters},
	volume       = 32,
	number       = 11,
	pages        = {1523--1531},
	doi          = {https://doi.org/10.1016/j.patrec.2011.04.010},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865511001103},
	keywords     = {Synthetic data generation, Benchmarking, Experimental proofs},
	abstract     = {Benchmarking pattern recognition, machine learning and data mining methods commonly relies on real-world data sets. However, there are some disadvantages in using real-world data. On one hand collecting real-world data can become difficult or impossible for various reasons, on the other hand real-world variables are hard to control, even in the problem domain; in the feature domain, where most statistical learning methods operate, exercising control is even more difficult and hence rarely attempted. This is at odds with the scientific experimentation guidelines mandating the use of as directly controllable and as directly observable variables as possible. Because of this, synthetic data possesses certain advantages over real-world data sets. In this paper we propose a method that produces synthetic data with guaranteed global and class-specific statistical properties. This method is based on overlapping class densities placed on the corners of a regular k-simplex. This generator can be used for algorithm testing and fair performance evaluation of statistical learning methods. Because of the strong properties of this generator researchers can reproduce each others experiments by knowing the parameters used, instead of transmitting large data sets.}
}
@article{fraser2011bayes,
	title        = {{Is Bayes Posterior just Quick and Dirty Confidence?}},
	author       = {D. A. S. Fraser},
	year         = 2011,
	journal      = {Statistical Science},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 26,
	number       = 3,
	pages        = {299 -- 316},
	doi          = {10.1214/11-STS352},
	url          = {https://doi.org/10.1214/11-STS352},
	keywords     = {Bayes, Bayes error rate, Confidence, default prior, evaluating a prior, nonlinear parameter, Posterior, prior}
}
@inproceedings{fried2014analyzing,
	title        = {Analyzing the language of food on social media},
	author       = {Fried, Daniel and Surdeanu, Mihai and Kobourov, Stephen and Hingle, Melanie and Bell, Dane},
	year         = 2014,
	booktitle    = {2014 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {778--783},
	doi          = {10.1109/BigData.2014.7004305}
}
@misc{ftc2010dannon,
	title        = {Dannon agrees to drop exaggerated health claims for activia yogurt and danactive dairy drink},
	author       = {the Premerger Notification Office and DPIP and CTO},
	year         = 2010,
	month        = {Dec},
	journal      = {Federal Trade Commission},
	url          = {https://www.ftc.gov/news-events/news/press-releases/2010/12/dannon-agrees-drop-exaggerated-health-claims-activia-yogurt-danactive-dairy-drink}
}
@inproceedings{fu2016using,
	title        = {Using LSTM and GRU neural network methods for traffic flow prediction},
	author       = {Fu, Rui and Zhang, Zuo and Li, Li},
	year         = 2016,
	booktitle    = {2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
	volume       = {},
	number       = {},
	pages        = {324--328},
	doi          = {10.1109/YAC.2016.7804912}
}
@article{fukunaga1975k,
	title        = {k-nearest-neighbor Bayes-risk estimation},
	author       = {Fukunaga, K. and Hostetler, L.},
	year         = 1975,
	journal      = {IEEE Transactions on Information Theory},
	volume       = 21,
	number       = 3,
	pages        = {285--293},
	doi          = {10.1109/TIT.1975.1055373}
}
@book{fukunaga1990introduction,
	title        = {Introduction to Statistical Pattern Recognition (2nd Ed.)},
	author       = {Fukunaga, Keinosuke},
	year         = 1990,
	publisher    = {Academic Press Professional, Inc.},
	address      = {USA},
	isbn         = {0122698517}
}
@inproceedings{gabrilovich2007computing,
	title        = {Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic Analysis},
	author       = {Gabrilovich, Evgeniy and Markovitch, Shaul},
	year         = 2007,
	booktitle    = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
	location     = {Hyderabad, India},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {IJCAI'07},
	pages        = {1606--1611},
	abstract     = {Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.},
	numpages     = 6
}
@article{ganin2016domain,
	title        = {Domain-Adversarial Training of Neural Networks},
	author       = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran{\c{c}}ois Laviolette and Mario March and Victor Lempitsky},
	year         = 2016,
	journal      = {Journal of Machine Learning Research},
	volume       = 17,
	number       = 59,
	pages        = {1--35},
	url          = {http://jmlr.org/papers/v17/15-239.html}
}
@article{gao2015wordnet,
	title        = {A WordNet-based semantic similarity measurement combining edge-counting and information content theory},
	author       = {Jian-Bo Gao and Bao-Wen Zhang and Xiao-Hua Chen},
	year         = 2015,
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = 39,
	pages        = {80--88},
	doi          = {https://doi.org/10.1016/j.engappai.2014.11.009},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197614002814},
	keywords     = {Semantic similarity, WordNet, Edge-counting, Information content},
	abstract     = {Semantic similarity measuring between words can be applied to many applications, such as Artificial Intelligence, Information Processing, Medical Care and Linguistics. In this paper, we present a new approach for semantic similarity measuring which is based on edge-counting and information content theory. Specifically, the proposed measure nonlinearly transforms the weighted shortest path length between the compared concepts to achieve the semantic similarity results, and the relation between parameters and the correlation value is discussed in detail. Experimental results show that the proposed approach not only achieves high correlation value against human ratings but also has better distribution characteristics of the correlation coefficient compared with several related works in the literature. In addition, the proposed method is computationally efficient due to the simplified ways of weighting the shortest path length between the concept pairs.}
}
@article{gao2019incorporating,
	title        = {Incorporating word embeddings into topic modeling of short text},
	author       = {Gao, Wang and Peng, Min and Wang, Hua and Zhang, Yanchun and Xie, Qianqian and Tian, Gang},
	year         = 2019,
	month        = {Nov},
	day          = {01},
	journal      = {Knowledge and Information Systems},
	volume       = 61,
	number       = 2,
	pages        = {1123--1145},
	doi          = {10.1007/s10115-018-1314-7},
	issn         = {0219-3116},
	url          = {https://doi.org/10.1007/s10115-018-1314-7},
	abstract     = {Short texts have become the prevalent format of information on the Internet. Inferring the topics of this type of messages becomes a critical and challenging task for many applications. Due to the length of short texts, conventional topic models (e.g., latent Dirichlet allocation and its variants) suffer from the severe data sparsity problem which makes topic modeling of short texts difficult and unreliable. Recently, word embeddings have been proved effective to capture semantic and syntactic information about words, which can be used to induce similarity measures and semantic correlations among words. Enlightened by this, in this paper, we design a novel model for short text topic modeling, referred as Conditional Random Field regularized Topic Model (CRFTM). CRFTM not only develops a generalized solution to alleviate the sparsity problem by aggregating short texts into pseudo-documents, but also leverages a Conditional Random Field regularized model that encourages semantically related words to share the same topic assignment. Experimental results on two real-world datasets show that our method can extract more coherent topics, and significantly outperform state-of-the-art baselines on several evaluation metrics.}
}
@article{garber1988bounds,
	title        = {Bounds on the Bayes classification error based on pairwise risk functions},
	author       = {Garber, F.D. and Djouadi, A.},
	year         = 1988,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 10,
	number       = 2,
	pages        = {281--288},
	doi          = {10.1109/34.3891}
}
@book{gardenfors2004conceptual,
	title        = {Conceptual spaces - the geometry of thought},
	author       = {Peter G{\"{a}}rdenfors},
	year         = 2000,
	publisher    = {{MIT} Press},
	isbn         = {978-0-262-07199-4},
	timestamp    = {Mon, 18 Apr 2011 17:48:55 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0006106.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ge2018deep,
	title        = {Deep Metric Learning with Hierarchical Triplet Loss},
	author       = {Ge, Weifeng},
	year         = 2018,
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@inproceedings{gehr2018ai2,
	title        = {AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
	author       = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
	year         = 2018,
	booktitle    = {2018 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {3--18},
	doi          = {10.1109/SP.2018.00058}
}
@inproceedings{geirhos2018imagenet,
	title        = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	author       = {Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Bygh9j09KX},
	timestamp    = {Thu, 25 Jul 2019 13:03:15 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/GeirhosRMBWB19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{geirhos2018imagenettrained,
	title        = {ImageNet-trained {CNN}s are biased towards texture; increasing shape bias improves accuracy and robustness.},
	author       = {Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Bygh9j09KX}
}
@article{geirhos2020shortcut,
	title        = {Shortcut learning in deep neural networks},
	author       = {Robert Geirhos and J{\"{o}}rn{-}Henrik Jacobsen and Claudio Michaelis and Richard S. Zemel and Wieland Brendel and Matthias Bethge and Felix A. Wichmann},
	year         = 2020,
	journal      = {Nat. Mach. Intell.},
	volume       = 2,
	number       = 11,
	pages        = {665--673},
	doi          = {10.1038/s42256-020-00257-z},
	url          = {https://doi.org/10.1038/s42256-020-00257-z},
	timestamp    = {Wed, 15 Dec 2021 10:26:49 +0100},
	biburl       = {https://dblp.org/rec/journals/natmi/GeirhosJMZBBW20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{george2021real,
	title        = {Real-time spatio-temporal event detection on geotagged social media},
	author       = {George, Yasmeen and Karunasekera, Shanika and Harwood, Aaron and Lim, Kwan Hui},
	year         = 2021,
	month        = {Jun},
	day          = 24,
	journal      = {Journal of Big Data},
	volume       = 8,
	number       = 1,
	pages        = 91,
	doi          = {10.1186/s40537-021-00482-2},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-021-00482-2},
	abstract     = {A key challenge in mining social media data streams is to identify events which are actively discussed by a group of people in a specific local or global area. Such events are useful for early warning for accident, protest, election or breaking news. However, neither the list of events nor the resolution of both event time and space is fixed or known beforehand. In this work, we propose an online spatio-temporal event detection system using social media that is able to detect events at different time and space resolutions. First, to address the challenge related to the unknown spatial resolution of events, a quad-tree method is exploited in order to split the geographical space into multiscale regions based on the density of social media data. Then, a statistical unsupervised approach is performed that involves Poisson distribution and a smoothing method for highlighting regions with unexpected density of social posts. Further, event duration is precisely estimated by merging events happening in the same region at consecutive time intervals. A post processing stage is introduced to filter out events that are spam, fake or wrong. Finally, we incorporate simple semantics by using social media entities to assess the integrity, and accuracy of detected events. The proposed method is evaluated using different social media datasets: Twitter and Flickr for different cities: Melbourne, London, Paris and New York. To verify the effectiveness of the proposed method, we compare our results with two baseline algorithms based on fixed split of geographical space and clustering method. For performance evaluation, we manually compute recall and precision. We also propose a new quality measure named strength index, which automatically measures how accurate the reported event is.}
}
@article{gesi2022code,
	title        = {Code Smells in Machine Learning Systems},
	author       = {Jiri Gesi and Siqi Liu and Jiawei Li and Iftekhar Ahmed and Nachiappan Nagappan and David Lo and Eduardo Santana de Almeida and Pavneet Singh Kochhar and Lingfeng Bao},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2203.00803},
	doi          = {10.48550/arXiv.2203.00803},
	url          = {https://doi.org/10.48550/arXiv.2203.00803},
	eprinttype   = {arXiv},
	eprint       = {2203.00803},
	timestamp    = {Wed, 16 Mar 2022 16:39:52 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2203-00803.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gilmer2018adversarial,
	title        = {Adversarial Spheres},
	author       = {Justin Gilmer and Luke Metz and Fartash Faghri and Samuel S. Schoenholz and Maithra Raghu and Martin Wattenberg and Ian J. Goodfellow},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SkthlLkPf},
	timestamp    = {Thu, 04 Apr 2019 13:20:09 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/GilmerMFSRWG18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gilmer2018motivating,
	title        = {Motivating the Rules of the Game for Adversarial Example Research},
	author       = {Justin Gilmer and Ryan P. Adams and Ian J. Goodfellow and David G. Andersen and George E. Dahl},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1807.06732},
	url          = {http://arxiv.org/abs/1807.06732},
	eprinttype   = {arXiv},
	eprint       = {1807.06732},
	timestamp    = {Wed, 07 Oct 2020 15:24:17 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1807-06732.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gilpin2018explaining,
	title        = {Explaining explanations: An overview of interpretability of machine learning},
	author       = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	year         = 2018,
	booktitle    = {2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
	pages        = {80--89},
	organization = {IEEE}
}
@inproceedings{goodfellow2014explaining,
	title        = {Explaining and Harnessing Adversarial Examples},
	author       = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6572},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:38 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{goodfellow2016deep,
	title        = {Deep learning},
	author       = {GOODFELLOW, Ian and BENGIO, Yoshua and COURVILLE, Aaron},
	year         = 2016,
	publisher    = {MIT Press},
	place        = {Cambridge; Massachusetts; London}
}
@article{goodman2020advbox,
	title        = {Advbox: a toolbox to generate adversarial examples that fool neural networks},
	author       = {Dou Goodman and Xin Hao and Yang Wang and Yuesheng Wu and Junfeng Xiong and Huan Zhang},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2001.05574},
	url          = {https://arxiv.org/abs/2001.05574},
	eprinttype   = {arXiv},
	eprint       = {2001.05574},
	timestamp    = {Fri, 17 Jan 2020 14:07:30 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2001-05574.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{goswami2018unravelling,
	title        = {Unravelling Robustness of Deep Learning Based Face Recognition Against Adversarial Attacks},
	author       = {Goswami, Gaurav and Ratha, Nalini and Agarwal, Akshay and Singh, Richa and Vatsa, Mayank},
	year         = 2018,
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32,
	number       = 1,
	doi          = {10.1609/aaai.v32i1.12341},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/12341},
	abstractnote = {&lt;p&gt; Deep neural network (DNN) architecture based models have high expressive power and learning capacity. However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities. In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: (i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; (ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and (iii) making corrections to the processing pipeline to alleviate the problem. Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition. &lt;/p&gt;}
}
@inproceedings{gowal2019scalable,
	title        = {Scalable verified training for provably robust image classification},
	author       = {Gowal, Sven and Dvijotham, Krishnamurthy Dj and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {4842--4851}
}
@article{graham2017can,
	title        = {Can machine translation systems be evaluated by the crowd alone},
	author       = {GRAHAM, YVETTE and BALDWIN, TIMOTHY and MOFFAT, ALISTAIR and ZOBEL, JUSTIN},
	year         = 2017,
	journal      = {Natural Language Engineering},
	publisher    = {Cambridge University Press},
	volume       = 23,
	number       = 1,
	pages        = {3--30},
	doi          = {10.1017/S1351324915000339}
}
@inproceedings{graves2014towards,
	title        = {Towards End-To-End Speech Recognition with Recurrent Neural Networks},
	author       = {Graves, Alex and Jaitly, Navdeep},
	year         = 2014,
	month        = {22--24 Jun},
	booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Bejing, China},
	series       = {Proceedings of Machine Learning Research},
	volume       = 32,
	number       = 2,
	pages        = {1764--1772},
	url          = {https://proceedings.mlr.press/v32/graves14.html},
	editor       = {Xing, Eric P. and Jebara, Tony},
	pdf          = {http://proceedings.mlr.press/v32/graves14.pdf}
}
@article{grimmer2010bayesian,
	title        = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases},
	author       = {Grimmer, Justin},
	year         = 2010,
	journal      = {Political Analysis},
	publisher    = {Cambridge University Press},
	volume       = 18,
	number       = 1,
	pages        = {1--35},
	doi          = {10.1093/pan/mpp034}
}
@inproceedings{grosse2017adversarial,
	title        = {Adversarial Examples for Malware Detection},
	author       = {Grosse, Kathrin and Papernot, Nicolas and Manoharan, Praveen and Backes, Michael and McDaniel, Patrick},
	year         = 2017,
	booktitle    = {Computer Security -- ESORICS 2017},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {62--79},
	isbn         = {978-3-319-66399-9},
	editor       = {Foley, Simon N. and Gollmann, Dieter and Snekkenes, Einar},
	abstract     = {Machine learning models are known to lack robustness against inputs crafted by an adversary. Such adversarial examples can, for instance, be derived from regular inputs by introducing minor---yet carefully selected---perturbations.},
	organization = {Springer}
}
@article{guarino1995ontologies,
	title        = {Ontologies and knowledge bases},
	author       = {Guarino, Nicola and Giaretta, Pierdaniele},
	year         = 1995,
	journal      = {Towards very large knowledge bases},
	publisher    = {IOS press Clifton, VA, USA},
	pages        = {1--2}
}
@misc{guide51,
	title        = {Iso/iec guide 51: Safety aspects-guidelines for their inclusion in standards. Geneva, Switzerland},
	author       = {ISO},
	year         = 2014
}
@inproceedings{guo2017calibration,
	title        = {On calibration of modern neural networks},
	author       = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {1321--1330},
	organization = {PMLR}
}
@article{gusfield1997algorithms,
	title        = {Algorithms on Stings, Trees, and Sequences: Computer Science and Computational Biology},
	author       = {Gusfield, Dan},
	year         = 1997,
	month        = {dec},
	journal      = {SIGACT News},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 28,
	number       = 4,
	pages        = {41--60},
	doi          = {10.1145/270563.571472},
	issn         = {0163-5700},
	url          = {https://doi.org/10.1145/270563.571472},
	issue_date   = {Dec. 1997},
	numpages     = 20
}
@inproceedings{haim2006second,
	title        = {The second pascal recognising textual entailment challenge},
	author       = {Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
	year         = 2006,
	booktitle    = {Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
	volume       = 7
}
@inproceedings{halder2021transformer,
	title        = {Transformer-Based Multi-task Learning for Queuing Time Aware Next POI Recommendation},
	author       = {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and Zhang, Xiuzhen},
	year         = 2021,
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {510--523},
	isbn         = {978-3-030-75765-6},
	editor       = {Karlapalem, Kamal and Cheng, Hong and Ramakrishnan, Naren and Agrawal, R. K. and Reddy, P. Krishna and Srivastava, Jaideep and Chakraborty, Tanmoy},
	abstract     = {Next point-of-interest (POI) recommendation is an important and challenging problem due to different contextual information and wide variety in human mobility patterns. Most of the prior studies incorporated user travel spatiotemporal andsequential patterns to recommend next POIs. However, few of these previous approaches considered the queuing time at POIs and its influence on user's mobility. The queuing time plays a significant role in affecting user mobility behaviour, e.g., having to queue a long time to enter a POI might reduce visitor's enjoyment. Recently, attention based recurrent neural networks-based approaches show promising performance in next POI recommendation but they are limited to single head attention which can have difficulty finding the appropriate complex connections between users, previous travel history and POI information. In this research, we present a problem of queuing time aware next POI recommendation and demonstrate how it is non-trivial to both recommend a next POI and simultaneously predict its queuing time. To solve this problem, we propose a multi-task, multi head attention transformer model called TLR-M. The model recommends next POIs to the target users and predicts queuing time to access the POIs simultaneously. By utilizing multi-head attention, the TLR-M model can integrate long range dependencies between any two POI visit efficiently and evaluate their contribution to select next POIs and to predict queuing time. Extensive experiments on eight real datasets show that the proposed model outperforms than the state-of-the-art baseline approaches in terms of precision, recall and F1 score evaluation metrics. The model also predicts and minimizes the queuing time effectively.}
}
@article{han2014unsupervised,
	title        = {Unsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation},
	author       = {Han, Aaron L.-F. and Wong, Derek F. and Chao, Lidia S. and He, Liangye and Lu, Yi},
	year         = 2014,
	month        = {Apr},
	day          = 28,
	journal      = {The Scientific World Journal},
	publisher    = {Hindawi Publishing Corporation},
	volume       = 2014,
	pages        = 760301,
	doi          = {10.1155/2014/760301},
	issn         = {2356-6140},
	url          = {https://doi.org/10.1155/2014/760301},
	abstract     = {With the rapid development of machine translation (MT), the MT evaluation becomes very important to timely tell us whether the MT system makes any progress. The conventional MT evaluation methods tend to calculate the similarity between hypothesis translations offered by automatic translation systems and reference translations offered by professional translators. There are several weaknesses in existing evaluation metrics. Firstly, the designed incomprehensive factors result in language-bias problem, which means they perform well on some special language pairs but weak on other language pairs. Secondly, they tend to use no linguistic features or too many linguistic features, of which no usage of linguistic feature draws a lot of criticism from the linguists and too many linguistic features make the model weak in repeatability. Thirdly, the employed reference translations are very expensive and sometimes not available in the practice. In this paper, the authors propose an unsupervised MT evaluation metric using universal part-of-speech tagset without relying on reference translations. The authors also explore the performances of the designed metric on traditional supervised evaluation tasks. Both the supervised and unsupervised experiments show that the designed methods yield higher correlation scores with human judgments.}
}
@inproceedings{han2015learning,
	title        = {Learning both Weights and Connections for Efficient Neural Network},
	author       = {Song Han and Jeff Pool and John Tran and William J. Dally},
	year         = 2015,
	journal      = {Advances in neural information processing systems},
	booktitle    = {Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
	volume       = 28,
	pages        = {1135--1143},
	url          = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
	editor       = {Corinna Cortes and Neil D. Lawrence and Daniel D. Lee and Masashi Sugiyama and Roman Garnett},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/HanPTD15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{han2016machine,
	title        = {Machine translation evaluation resources and methods: A survey},
	author       = {Han, Lifeng},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1605.04515}
}
@inproceedings{han2020ghostnet,
	title        = {GhostNet: More Features From Cheap Operations},
	author       = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{han2021rethinking,
	title        = {Rethinking Channel Dimensions for Efficient Model Design},
	author       = {Han, Dongyoon and Yun, Sangdoo and Heo, Byeongho and Yoo, YoungJoon},
	year         = 2021,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {732--741}
}
@book{hastie2009elements,
	title        = {The Elements of Statistical Learning},
	author       = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year         = 2009,
	publisher    = {Springer},
	url          = {https://hastie.su.domains/Papers/ESLII.pdf},
	keywords     = {book.ml,ebook},
	edition      = 2
}
@inproceedings{he2016deep,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{heckner2008tagging,
	title        = {Tagging tagging. Analysing user keywords in scientific bibliography management systems},
	author       = {Heckner, Markus and M{\"u}hlbacher, Susanne and Wolff, Christian},
	year         = 2008
}
@article{hegde2020unsupervised,
	title        = {Unsupervised Paraphrase Generation using Pre-trained Language Models},
	author       = {Chaitra V. Hegde and Shrikumar Patil},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2006.05477},
	url          = {https://arxiv.org/abs/2006.05477},
	eprinttype   = {arXiv},
	eprint       = {2006.05477},
	timestamp    = {Sat, 13 Jun 2020 18:28:13 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2006-05477.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hein2017formal,
	title        = {Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation},
	author       = {Hein, Matthias and Andriushchenko, Maksym},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{hendrycks2019benchmarking,
	title        = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
	author       = {Dan Hendrycks and Thomas G. Dietterich},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1903.12261},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HJz6tiCqYm},
	timestamp    = {Thu, 25 Jul 2019 14:25:46 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/HendrycksD19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hendrycks2021natural,
	title        = {Natural adversarial examples},
	author       = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {15262--15271}
}
@inproceedings{hermann2015teaching,
	title        = {Teaching Machines to Read and Comprehend},
	author       = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 28,
	url          = {https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
	editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@article{hevner2004design,
	title        = {Design Science in Information Systems Research},
	author       = {Alan R. Hevner and Salvatore T. March and Jinsoo Park and Sudha Ram},
	year         = 2004,
	journal      = {MIS Quarterly},
	publisher    = {Management Information Systems Research Center, University of Minnesota},
	volume       = 28,
	number       = 1,
	pages        = {75--105},
	issn         = {02767783},
	url          = {http://www.jstor.org/stable/25148625},
	urldate      = {2022-08-28},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.}
}
@article{hickey1993stylistics,
	title        = {Stylistics, Pragmatics and Pragmastylistics},
	author       = {Leo Hickey},
	year         = 1993,
	journal      = {Revue belge de philologie et d'histoire},
	publisher    = {Pers{\'e}e-Portail des revues scientifiques en SHS},
	volume       = 71,
	number       = 3,
	pages        = {573--586},
	doi          = {10.3406/rbph.1993.3890},
	url          = {https://www.persee.fr/doc/rbph_0035-0818_1993_num_71_3_3890}
}
@inproceedings{hirst2003paraphrasing,
	title        = {Paraphrasing paraphrased},
	author       = {Hirst, Graeme},
	year         = 2003,
	booktitle    = {Keynote address for The Second International Workshop on Paraphrasing: Paraphrase acquisition and Applications}
}
@article{hjorland2015classical,
	title        = {Classical databases and knowledge organization: A case for boolean retrieval and human decision-making during searches},
	author       = {Hj{\o}rland, Birger},
	year         = 2015,
	journal      = {Journal of the Association for Information Science and Technology},
	publisher    = {Wiley Online Library},
	volume       = 66,
	number       = 8,
	pages        = {1559--1575}
}
@article{hodosh2013framing,
	title        = {Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics},
	author       = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
	year         = 2013,
	month        = {may},
	journal      = {J. Artif. Int. Res.},
	publisher    = {AI Access Foundation},
	address      = {El Segundo, CA, USA},
	volume       = 47,
	number       = 1,
	pages        = {853--899},
	issn         = {1076-9757},
	issue_date   = {May 2013},
	abstract     = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
	numpages     = 47
}
@article{hoffart2013yago2,
	title        = {YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia},
	author       = {Johannes Hoffart and Fabian M. Suchanek and Klaus Berberich and Gerhard Weikum},
	year         = 2013,
	journal      = {Artificial Intelligence},
	volume       = 194,
	pages        = {28--61},
	doi          = {https://doi.org/10.1016/j.artint.2012.06.001},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370212000719},
	note         = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
	keywords     = {Ontologies, Knowledge bases, Spatio-temporal facts, Information extraction},
	abstract     = {We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an accuracy of 95\% of the facts in YAGO2. In this paper, we present the extraction methodology, the integration of the spatio-temporal dimension, and our knowledge representation SPOTL, an extension of the original SPO-triple model to time and space.}
}
@inproceedings{hoffer2015deep,
	title        = {Deep Metric Learning Using Triplet Network},
	author       = {Hoffer, Elad and Ailon, Nir},
	year         = 2015,
	booktitle    = {Similarity-Based Pattern Recognition},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {84--92},
	isbn         = {978-3-319-24261-3},
	editor       = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	abstract     = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.}
}
@inproceedings{hofmann1999probabilistic,
	title        = {Probabilistic Latent Semantic Analysis},
	author       = {Thomas Hofmann},
	year         = 1999,
	booktitle    = {{UAI} '99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, Stockholm, Sweden, July 30 - August 1, 1999},
	publisher    = {Morgan Kaufmann},
	pages        = {289--296},
	url          = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=179\&proceeding\_id=15},
	editor       = {Kathryn B. Laskey and Henri Prade},
	timestamp    = {Wed, 03 Feb 2021 11:09:31 +0100},
	biburl       = {https://dblp.org/rec/conf/uai/Hofmann99.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{holland1992genetic,
	title        = {Genetic algorithms},
	author       = {John H. Holland},
	year         = 2012,
	journal      = {Scholarpedia},
	volume       = 7,
	number       = 12,
	pages        = 1482,
	doi          = {10.4249/scholarpedia.1482},
	url          = {https://doi.org/10.4249/scholarpedia.1482},
	timestamp    = {Thu, 23 May 2019 15:09:50 +0200},
	biburl       = {https://dblp.org/rec/journals/scholarpedia/Holland12.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hong2010empirical,
	title        = {Empirical Study of Topic Modeling in Twitter},
	author       = {Hong, Liangjie and Davison, Brian D.},
	year         = 2010,
	booktitle    = {Proceedings of the First Workshop on Social Media Analytics},
	location     = {Washington D.C., District of Columbia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOMA '10},
	pages        = {80--88},
	doi          = {10.1145/1964858.1964870},
	isbn         = 9781450302173,
	url          = {https://doi.org/10.1145/1964858.1964870},
	abstract     = {Social networks such as Facebook, LinkedIn, and Twitter have been a crucial source of information for a wide spectrum of users. In Twitter, popular information that is deemed important by the community propagates through the network. Studying the characteristics of content in the messages becomes important for a number of tasks, such as breaking news detection, personalized message recommendation, friends recommendation, sentiment analysis and others. While many researchers wish to use standard text mining tools to understand messages on Twitter, the restricted length of those messages prevents them from being employed to their full potential.We address the problem of using standard topic models in micro-blogging environments by studying how the models can be trained on the dataset. We propose several schemes to train a standard topic model and compare their quality and effectiveness through a set of carefully designed experiments from both qualitative and quantitative perspectives. We show that by training a topic model on aggregated messages we can obtain a higher quality of learned model which results in significantly better performance in two real-world classification problems. We also discuss how the state-of-the-art Author-Topic model fails to model hierarchical relationships between entities in Social Media.},
	numpages     = 9,
	keywords     = {topic models, Twitter, social media}
}
@inproceedings{hu2004mining,
	title        = {Mining and Summarizing Customer Reviews},
	author       = {Hu, Minqing and Liu, Bing},
	year         = 2004,
	booktitle    = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Seattle, WA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '04},
	pages        = {168–177},
	doi          = {10.1145/1014052.1014073},
	isbn         = 1581138881,
	url          = {https://doi.org/10.1145/1014052.1014073},
	abstract     = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
	numpages     = 10,
	keywords     = {summarization, text mining, sentiment classification, reviews}
}
@article{hu2012manipulation,
	title        = {Manipulation of online reviews: An analysis of ratings, readability, and sentiments},
	author       = {Nan Hu and Indranil Bose and Noi Sian Koh and Ling Liu},
	year         = 2012,
	journal      = {Decision Support Systems},
	volume       = 52,
	number       = 3,
	pages        = {674--684},
	doi          = {https://doi.org/10.1016/j.dss.2011.11.002},
	issn         = {0167-9236},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167923611002065},
	keywords     = {Manipulation, Online reviews, Ratings, Readability, Runs test, Sentiments, Text mining},
	abstract     = {As consumers become increasingly reliant on online reviews to make purchase decisions, the sales of the product becomes dependent on the word of mouth (WOM) that it generates. As a result, there can be attempts by firms to manipulate online reviews of products to increase their sales. Despite the suspicion on the existence of such manipulation, the amount of such manipulation is unknown, and deciding which reviews to believe in is largely based on the reader's discretion and intuition. Therefore, the success of the manipulation of reviews by firms in generating sales of products is unknown. In this paper, we propose a simple statistical method to detect online reviews manipulation, and assess how consumers respond to products with manipulated reviews. In particular, the writing style of reviewers is examined, and the effectiveness of manipulation through ratings, sentiments, and readability is investigated. Our analysis examines textual information available in online reviews by combining sentiment mining techniques with readability assessments. We discover that around 10.3\% of the products are subject to online reviews manipulation. In spite of the deliberate use of sentiments and ratings in manipulated products, consumers are only able to detect manipulation taking place through ratings, but not through sentiments. The findings from this research ensue a note of caution for all consumers that rely on online reviews of books for making purchases, and encourage them to delve deep into the book reviews without being deceived by fraudulent manipulation.}
}
@inproceedings{hu2013spatial,
	title        = {Spatial Topic Modeling in Online Social Media for Location Recommendation},
	author       = {Hu, Bo and Ester, Martin},
	year         = 2013,
	booktitle    = {Proceedings of the 7th ACM Conference on Recommender Systems},
	location     = {Hong Kong, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {RecSys '13},
	pages        = {25--32},
	doi          = {10.1145/2507157.2507174},
	isbn         = 9781450324090,
	url          = {https://doi.org/10.1145/2507157.2507174},
	abstract     = {Mobile networks enable users to post on social media services (e.g., Twitter) from anywhere. The activities of mobile users involve three major entities: user, post, and location. The interaction of these entities is the key to answer questions such as who will post a message where and on what topic? In this paper, we address the problem of profiling mobile users by modeling their activities, i.e., we explore topic modeling considering the spatial and textual aspects of user posts, and predict future user locations. We propose the first ST (Spatial Topic) model to capture the correlation between users' movements and between user interests and the function of locations. We employ the sparse coding technique which greatly speeds up the learning process. We perform experiments on two real life data sets from Twitter and Yelp. Through comprehensive experiments, we demonstrate that our proposed model consistently improves the average precision@1,5,10,15,20 for location recommendation by at least 50% (Twitter) and 300% (Yelp) against existing state-of-the-art recommendation algorithms and geographical topic models.},
	numpages     = 8,
	keywords     = {mobile users, spatial topic model, location recommendation}
}
@inproceedings{hu2020tf,
	title        = {TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search},
	author       = {Hu, Yibo and Wu, Xiang and He, Ran},
	year         = 2020,
	booktitle    = {Computer Vision -- ECCV 2020},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {123--139},
	isbn         = {978-3-030-58555-6},
	editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	abstract     = {With the flourish of differentiable neural architecture search (NAS), automatically searching latency-constrained architectures gives a new perspective to reduce human labor and expertise. However, the searched architectures are usually suboptimal in accuracy and may have large jitters around the target latency. In this paper, we rethink three freedoms of differentiable NAS, i.e. operation-level, depth-level and width-level, and propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good classification accuracy and precise latency constraint. For the operation-level, we present a bi-sampling search algorithm to moderate the operation collapse. For the depth-level, we introduce a sink-connecting search space to ensure the mutual exclusion between skip and other candidate operations, as well as eliminate the architecture redundancy. For the width-level, we propose an elasticity-scaling strategy that achieves precise latency constraint in a progressively fine-grained manner. Experiments on ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched TF-NAS-A obtains 76.9{\%} top-1 accuracy, achieving state-of-the-art results with less latency. Code is available at https://github.com/AberHu/TF-NAS.}
}
@inproceedings{huang2008labeled,
	title        = {{Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments}},
	author       = {Huang, Gary B. and Mattar, Marwan and Berg, Tamara and Learned-Miller, Eric},
	year         = 2008,
	month        = Oct,
	booktitle    = {{Workshop on Faces in 'Real-Life' Images: Detection, Alignment, and Recognition}},
	address      = {Marseille, France},
	url          = {https://hal.inria.fr/inria-00321923},
	organization = {{Erik Learned-Miller and Andras Ferencz and Fr{\'e}d{\'e}ric Jurie}},
	pdf          = {https://hal.inria.fr/inria-00321923/file/Huang_long_eccv2008-lfw.pdf},
	hal_id       = {inria-00321923},
	hal_version  = {v1}
}
@inproceedings{huang2017safety,
	title        = {Safety Verification of Deep Neural Networks},
	author       = {Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
	year         = 2017,
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {3--29},
	isbn         = {978-3-319-63387-9},
	editor       = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	abstract     = {Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.}
}
@inproceedings{huang2017snapshot,
	title        = {Snapshot Ensembles: Train 1, Get {M} for Free},
	author       = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJYwwY9ll},
	timestamp    = {Fri, 18 Nov 2022 15:40:45 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/HuangLP0HW17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{huang2020bridging,
	title        = {Bridging the Performance Gap between {FGSM} and {PGD} Adversarial Training},
	author       = {Tianjin Huang and Vlado Menkovski and Yulong Pei and Mykola Pechenizkiy},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2011.05157},
	url          = {https://arxiv.org/abs/2011.05157},
	eprinttype   = {arXiv},
	eprint       = {2011.05157},
	timestamp    = {Thu, 12 Nov 2020 15:14:56 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2011-05157.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{huang2020curricularface,
	title        = {CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition},
	author       = {Huang, Yuge and Wang, Yuhan and Tai, Ying and Liu, Xiaoming and Shen, Pengcheng and Li, Shaoxin and Li, Jilin and Huang, Feiyue},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@phdthesis{hummels1987nonparametric,
	title        = {Nonparametric estimation of the Bayes error},
	author       = {Hummels, Donald Michael},
	year         = 1987,
	url          = {https://docs.lib.purdue.edu/dissertations/AAI8814491/},
	school       = {Purdue University}
}
@inproceedings{huster2019limitations,
	title        = {Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples},
	author       = {Huster, Todd and Chiang, Cho-Yu Jason and Chadha, Ritu},
	year         = 2019,
	booktitle    = {ECML PKDD 2018 Workshops},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {16--29},
	isbn         = {978-3-030-13453-2},
	editor       = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and Garc{\'i}a-Mart{\'i}n, Eva and Gavald{\`a}, Ricard and Koprinska, Irena and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
	abstract     = {Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses.}
}
@misc{IEC61508,
	title        = {Functional safety of electrical/electronic/programmable electronic safety-related systems -- Part 7: Overview of techniques and measures},
	author       = {International Electrotechnical Commission},
	year         = 2010,
	note         = {IEC 61508-7:2010},
	howpublished = {Annex D, page 126}
}
@inproceedings{ishida2022performance,
	title        = {Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification},
	author       = {Takashi Ishida and Ikko Yamane and Nontawat Charoenphakdee and Gang Niu and Masashi Sugiyama},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=FZdJQgy05rz}
}
@article{islam2008semantic,
	title        = {Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity},
	author       = {Islam, Aminul and Inkpen, Diana},
	year         = 2008,
	month        = {jul},
	journal      = {ACM Trans. Knowl. Discov. Data},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/1376815.1376819},
	issn         = {1556-4681},
	url          = {https://doi.org/10.1145/1376815.1376819},
	issue_date   = {July 2008},
	abstract     = {We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.},
	articleno    = 10,
	numpages     = 25,
	keywords     = {Semantic similarity of words, corpus-based measures, similarity of short texts}
}
@book{iverson1962programming,
	title        = {A Programming Language},
	author       = {Iverson, Kenneth E.},
	year         = 1962,
	publisher    = {John Wiley \&amp; Sons, Inc.},
	address      = {USA},
	isbn         = {0471430145},
	abstract     = {From the PrefaceApplied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or programs. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a programming language.Much of applied mathematics, particularly the more recent computer-related areas which cut across the older disciplines, suffers from the lack of an adequate programming language. It is the central thesis of this book that the descriptive and analytic power of an adequate programming language amply repays the considerable effort required for its mastery. This thesis is developed by first presenting the entire language and then applying it in later chapters to several major topics.The areas of application are chosen primarily for their intrinsic interest and lack of previous treatment, but they are also designed to illustrate the universality and other facets of the language. For example, the microprogramming of Chapter 2 illustrates the divisibility of the language, i.e., the ability to treat a restricted area using only a small portion of the complete language. Chapter 6 (Sorting) shows its capacity to compass a relatively complex and detailed topic in a short space. Chapter 7 (The Logical Calculus) emphasizes the formal manipulability of the language and its utility in theoretical work.The material was developed largely in a graduate course given for several years at Harvard and in a later course presented repeatedly at the IBM Systems Research Institute in New York. It should prove suitable for a two-semester course at the senior or graduate level. Although for certain audiences an initial presentation of the entire language may be appropriate, I have found it helpful to motivate the development by presenting the minimum notation required for a given topic, proceeding to its treatment (e.g., microprogramming), and then returning to further notation. The 130-odd problems not only provide the necessary finger exercises but also develop results of general interest.Chapter 1 or some part of it is prerequisite to each of the remaining "applications" chapters, but the applications chapters are virtually independent of one another. A complete appreciation of search techniques (Chapter 4) does, however, require a knowledge of methods of representation (Chapter 3). The cross references which do occur in the applications chapters are either nonessential or are specific to a given figure, table, or program. The entire language presented in Chapter 1 is summarized for reference at the end of the book.}
}
@misc{iyer2017qqp,
	title        = {First Quora Dataset Release: Question Pairs},
	author       = {Shankar Iyer and Nikhil Dandekar and Kornél Csernai},
	year         = 2017,
	url          = {https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}
}
@article{jacobi2016quantitative,
	title        = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	author       = {Carina Jacobi and Wouter van Atteveldt and Kasper Welbers},
	year         = 2016,
	journal      = {Digital Journalism},
	publisher    = {Routledge},
	volume       = 4,
	number       = 1,
	pages        = {89--106},
	doi          = {10.1080/21670811.2015.1093271},
	url          = {https://doi.org/10.1080/21670811.2015.1093271},
	eprint       = {https://doi.org/10.1080/21670811.2015.1093271}
}
@book{jain2011handbook,
	title        = {Handbook of Face Recognition, 2nd Edition},
	year         = 2011,
	publisher    = {Springer},
	doi          = {10.1007/978-0-85729-932-1},
	isbn         = {978-0-85729-931-4},
	url          = {https://doi.org/10.1007/978-0-85729-932-1},
	editor       = {Stan Z. Li and Anil K. Jain},
	timestamp    = {Fri, 27 Oct 2017 15:34:03 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0027896.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{jaro1989advances,
	title        = {Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida},
	author       = {Matthew A.   Jaro},
	year         = 1989,
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = 84,
	number       = 406,
	pages        = {414--420},
	doi          = {10.1080/01621459.1989.10478785},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478785},
	eprint       = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478785}
}
@article{jensen2011measuring,
	title        = {Measuring spatial dispersion: exact results on the variance of random spatial distributions},
	author       = {Jensen, Pablo and Michel, Julien},
	year         = 2011,
	month        = {Aug},
	day          = {01},
	journal      = {The Annals of Regional Science},
	volume       = 47,
	number       = 1,
	pages        = {81--110},
	doi          = {10.1007/s00168-009-0342-3},
	issn         = {1432-0592},
	url          = {https://doi.org/10.1007/s00168-009-0342-3},
	abstract     = {Measuring the spatial distribution of locations of many entities (trees, atoms, economic activities, etc.), and, more precisely, the deviations from purely random configurations, is a powerful method to unravel their underlying interactions. Several coefficients have been developed in the past to quantify the possible deviations. It is important to quantify the variances of the coefficients for random distributions, to ascertain the statistical significance of an empirical deviation. By lack of a proper analytical expression, the significance is usually obtained by simulating many random configurations by Monte Carlo simulations. In the present paper, we present an exact analytical expression for the variance of several spatial coefficients for random distributions, and we rigorously show that these distributions asymptotically follow a Normal law. These two results eliminate the need for cumbersome Monte Carlo simulations. They also allow to understand qualitatively the main factors that may change the variance: number of sites, spatial inhomogeneity, etc.}
}
@article{jeong2019social,
	title        = {Social media mining for product planning: A product opportunity mining approach based on topic modeling and sentiment analysis},
	author       = {Byeongki Jeong and Janghyeok Yoon and Jae-Min Lee},
	year         = 2019,
	journal      = {International Journal of Information Management},
	volume       = 48,
	pages        = {280--290},
	doi          = {https://doi.org/10.1016/j.ijinfomgt.2017.09.009},
	issn         = {0268-4012},
	url          = {https://www.sciencedirect.com/science/article/pii/S0268401217302955},
	keywords     = {Product opportunity, New product development, Social media mining, Opportunity algorithm, Topic modeling, Sentiment analysis},
	abstract     = {Social media data have recently attracted considerable attention as an emerging voice of the customer as it has rapidly become a channel for exchanging and storing customer-generated, large-scale, and unregulated voices about products. Although product planning studies using social media data have used systematic methods for product planning, their methods have limitations, such as the difficulty of identifying latent product features due to the use of only term-level analysis and insufficient consideration of opportunity potential analysis of the identified features. Therefore, an opportunity mining approach is proposed in this study to identify product opportunities based on topic modeling and sentiment analysis of social media data. For a multifunctional product, this approach can identify latent product topics discussed by product customers in social media using topic modeling, thereby quantifying the importance of each product topic. Next, the satisfaction level of each product topic is evaluated using sentiment analysis. Finally, the opportunity value and improvement direction of each product topic from a customer-centered view are identified by an opportunity algorithm based on product topics' importance and satisfaction. We expect that our approach for product planning will contribute to the systematic identification of product opportunities from large-scale customer-generated social media data and will be used as a real-time monitoring tool for changing customer needs analysis in rapidly evolving product environments.}
}
@article{jiang2015feature,
	title        = {Feature-based approaches to semantic similarity assessment of concepts using Wikipedia},
	author       = {Yuncheng Jiang and Xiaopei Zhang and Yong Tang and Ruihua Nie},
	year         = 2015,
	journal      = {Information Processing  \& Management},
	volume       = 51,
	number       = 3,
	pages        = {215--234},
	doi          = {https://doi.org/10.1016/j.ipm.2015.01.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457315000023},
	keywords     = {Concept similarity, Semantic similarity, Semantic relatedness, Feature-based measures, Wikipedia},
	abstract     = {Semantic similarity assessment between concepts is an important task in many language related applications. In the past, several approaches to assess similarity by evaluating the knowledge modeled in an (or multiple) ontology (or ontologies) have been proposed. However, there are some limitations such as the facts of relying on predefined ontologies and fitting non-dynamic domains in the existing measures. Wikipedia provides a very large domain-independent encyclopedic repository and semantic network for computing semantic similarity of concepts with more coverage than usual ontologies. In this paper, we propose some novel feature based similarity assessment methods that are fully dependent on Wikipedia and can avoid most of the limitations and drawbacks introduced above. To implement similarity assessment based on feature by making use of Wikipedia, firstly a formal representation of Wikipedia concepts is presented. We then give a framework for feature based similarity based on the formal representation of Wikipedia concepts. Lastly, we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework. The evaluation, based on several widely used benchmarks and a benchmark developed in ourselves, sustains the intuitions with respect to human judgements. Overall, several methods proposed in this paper have good human correlation and constitute some effective ways of determining similarity between Wikipedia concepts.}
}
@inproceedings{jiang2015travel,
	title        = {Travel Recommendation via Author Topic Model Based Collaborative Filtering},
	author       = {Jiang, Shuhui and Qian, Xueming and Shen, Jialie and Mei, Tao},
	year         = 2015,
	booktitle    = {MultiMedia Modeling},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {392--402},
	isbn         = {978-3-319-14442-9},
	editor       = {He, Xiangjian and Luo, Suhuai and Tao, Dacheng and Xu, Changsheng and Yang, Jie and Hasan, Muhammad Abul},
	abstract     = {While automatic travel recommendation has attracted a lot of attentions, the existing approaches generally suffer from different kinds of weaknesses. For example, sparsity problem can significantly degrade the performance of traditional collaborative filtering (CF). If a user only visits very few locations, accurate similar user identification becomes very challenging due to lack of sufficient information. Motivated by this concern, we propose an Author Topic Collaborative Filtering (ATCF) method to facilitate comprehensive Points of Interest (POIs) recommendation for social media users. In our approach, the topics about user preference (e.g., cultural, cityscape, or landmark) are extracted from the textual description of photos by author topic model instead of from GPS (geo-tag). Consequently, unlike CF based approaches, even without GPS records, similar users could still be identified accurately according to the similarity of users' topic preferences. In addition, ATCF doesn't pre-define the category of travel topics. The category and user topic preference could be elicited simultaneously. Experiment results with a large test collection demonstrate various kinds of advantages of our approach.}
}
@article{jiang2017wikipedia,
	title        = {Wikipedia-based information content and semantic similarity computation},
	author       = {Yuncheng Jiang and Wen Bai and Xiaopei Zhang and Jiaojiao Hu},
	year         = 2017,
	journal      = {Information Processing  \& Management},
	volume       = 53,
	number       = 1,
	pages        = {248--265},
	doi          = {https://doi.org/10.1016/j.ipm.2016.09.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457316303934},
	keywords     = {Information content, Semantic similarity, Concept similarity, Wikipedia, Category structure},
	abstract     = {The Information Content (IC) of a concept is a fundamental dimension in computational linguistics. It enables a better understanding of concept's semantics. In the past, several approaches to compute IC of a concept have been proposed. However, there are some limitations such as the facts of relying on corpora availability, manual tagging, or predefined ontologies and fitting non-dynamic domains in the existing methods. Wikipedia provides a very large domain-independent encyclopedic repository and semantic network for computing IC of concepts with more coverage than usual ontologies. In this paper, we propose some novel methods to IC computation of a concept to solve the shortcomings of existing approaches. The presented methods focus on the IC computation of a concept (i.e., Wikipedia category) drawn from the Wikipedia category structure. We propose several new IC-based measures to compute the semantic similarity between concepts. The evaluation, based on several widely used benchmarks and a benchmark developed in ourselves, sustains the intuitions with respect to human judgments. Overall, some methods proposed in this paper have a good human correlation and constitute some effective ways of determining IC values for concepts and semantic similarity between concepts.}
}
@book{jing1994association,
	title        = {An association thesaurus for information retrieval},
	author       = {Jing, Yufeng and Croft, W Bruce},
	year         = 1994,
	publisher    = {University of Massachusetts, Department of Computer Science}
}
@inproceedings{joachims2002optimizing,
	title        = {Optimizing search engines using clickthrough data},
	author       = {Joachims, Thorsten},
	year         = 2002,
	booktitle    = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {133--142}
}
@inbook{jones1972statistical,
	title        = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
	author       = {Sparck Jones, Karen},
	year         = 1988,
	booktitle    = {Document Retrieval Systems},
	publisher    = {Taylor Graham Publishing},
	address      = {GBR},
	pages        = {132--142},
	isbn         = {0947568212},
	numpages     = 11
}
@inproceedings{jurman2009canberra,
	title        = {Canberra distance on ranked lists},
	author       = {Jurman, Giuseppe and Riccadonna, Samantha and Visintainer, Roberto and Furlanello, Cesare},
	year         = 2009,
	booktitle    = {Proceedings of advances in ranking NIPS 09 workshop},
	pages        = {22--27},
	organization = {Citeseer}
}
@inproceedings{kamigaito2020syntactically,
	title        = {Syntactically Look-Ahead Attention Network for Sentence Compression},
	author       = {Hidetaka Kamigaito and Manabu Okumura},
	year         = 2020,
	booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
	publisher    = {{AAAI} Press},
	pages        = {8050--8057},
	url          = {https://aaai.org/ojs/index.php/AAAI/article/view/6315},
	timestamp    = {Tue, 02 Feb 2021 08:00:36 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/KamigaitoO20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{kamp1978semantics,
	title        = {Semantics Versus Pragmatics},
	author       = {Kamp, Hans},
	year         = 1978,
	booktitle    = {Formal Semantics and Pragmatics for Natural Languages},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {255--287},
	doi          = {10.1007/978-94-009-9775-2_9},
	isbn         = {978-94-009-9775-2},
	url          = {https://doi.org/10.1007/978-94-009-9775-2_9},
	editor       = {Guenthner, F. and Schmidt, S. J.},
	abstract     = {Consider the sentences(1)You may take an apple,(2)You may take a pear, and(3)You may take an apple or take a pear.}
}
@inproceedings{kang2004product,
	title        = {Product approximation by minimizing the upper bound of Bayes error rate for Bayesian combination of classifiers},
	author       = {Hee-Joong Kang and Doermann, D.},
	year         = 2004,
	booktitle    = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
	volume       = 1,
	number       = {},
	pages        = {252--255 Vol.1},
	doi          = {10.1109/ICPR.2004.1334071}
}
@article{kannan2018adversarial,
	title        = {Adversarial Logit Pairing},
	author       = {Harini Kannan and Alexey Kurakin and Ian J. Goodfellow},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.06373},
	url          = {http://arxiv.org/abs/1803.06373},
	eprinttype   = {arXiv},
	eprint       = {1803.06373},
	timestamp    = {Mon, 13 Aug 2018 16:47:15 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06373.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{katz2017reluplex,
	title        = {Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
	author       = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
	year         = 2017,
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {97--117},
	isbn         = {978-3-319-63387-9},
	editor       = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	abstract     = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.}
}
@inproceedings{katz2017towards,
	title        = {Towards Proving the Adversarial Robustness of Deep Neural Networks},
	author       = {Guy Katz and Clark Barrett and David L. Dill and Kyle Julian and Mykel J. Kochenderfer},
	year         = 2017,
	month        = sep,
	booktitle    = {Proceedings of the First Workshop on Formal Verification of Autonomous Vehicles (FVAV '17)},
	series       = {Electronic Proceedings in Theoretical Computer Science},
	volume       = 257,
	pages        = {19--26},
	url          = {http://eptcs.web.cse.unsw.edu.au/paper.cgi?FVAV2017.3},
	note         = {Turin, Italy},
	editor       = {Lukas Bulwahn and Maryam Kamali and Sven Linker}
}
@inproceedings{katz2019marabou,
	title        = {The marabou framework for verification and analysis of deep neural networks},
	author       = {Katz, Guy and Huang, Derek A and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'c}, Aleksandar and others},
	year         = 2019,
	booktitle    = {Computer Aided Verification: 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I 31},
	pages        = {443--452},
	organization = {Springer}
}
@inproceedings{ke2019automated,
	title        = {Automated Essay Scoring: A Survey of the State of the Art},
	author       = {Ke, Zixuan and Ng, Vincent},
	year         = 2019,
	month        = 7,
	booktitle    = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {6300--6308},
	doi          = {10.24963/ijcai.2019/879},
	url          = {https://doi.org/10.24963/ijcai.2019/879}
}
@article{kelly2021measuring,
	title        = {Measuring Technological Innovation over the Long Run},
	author       = {Kelly, Bryan and Papanikolaou, Dimitris and Seru, Amit and Taddy, Matt},
	year         = 2021,
	month        = {September},
	journal      = {American Economic Review: Insights},
	volume       = 3,
	number       = 3,
	pages        = {303--20},
	doi          = {10.1257/aeri.20190499},
	url          = {https://www.aeaweb.org/articles?id=10.1257/aeri.20190499}
}
@inproceedings{kevselj2003n,
	title        = {N-Gram Feature Selection for Authorship Identification},
	author       = {Houvardas, John and Stamatatos, Efstathios},
	year         = 2006,
	booktitle    = {Artificial Intelligence: Methodology, Systems, and Applications},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {77--86},
	isbn         = {978-3-540-40931-1},
	editor       = {Euzenat, J{\'e}r{\^o}me and Domingue, John},
	abstract     = {Automatic authorship identification offers a valuable tool for supporting crime investigation and security. It can be seen as a multi-class, single-label text categorization task. Character n-grams are a very successful approach to represent text for stylistic purposes since they are able to capture nuances in lexical, syntactical, and structural level. So far, character n-grams of fixed length have been used for authorship identification. In this paper, we propose a variable-length n-gram approach inspired by previous work for selecting variable-length word sequences. Using a subset of the new Reuters corpus, consisting of texts on the same topic by 50 different authors, we show that the proposed approach is at least as effective as information gain for selecting the most significant n-grams although the feature sets produced by the two methods have few common members. Moreover, we explore the significance of digits for distinguishing between authors showing that an increase in performance can be achieved using simple text pre-processing.}
}
@article{kim2020torchattacks,
	title        = {Torchattacks: A pytorch repository for adversarial attacks},
	author       = {Kim, Hoki},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.01950}
}
@inproceedings{kingma2014adam,
	title        = {Adam: {A} Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6980},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kingma2018glow,
	title        = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	author       = {Kingma, Durk P and Dhariwal, Prafulla},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@inproceedings{knight2000statistics,
	title        = {Statistics-Based Summarization - Step One: Sentence Compression},
	author       = {Kevin Knight and Daniel Marcu},
	year         = 2000,
	booktitle    = {Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on on Innovative Applications of Artificial Intelligence, July 30 - August 3, 2000, Austin, Texas, {USA}},
	publisher    = {{AAAI} Press / The {MIT} Press},
	pages        = {703--710},
	url          = {http://www.aaai.org/Library/AAAI/2000/aaai00-108.php},
	editor       = {Henry A. Kautz and Bruce W. Porter},
	timestamp    = {Wed, 10 Feb 2021 08:43:54 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/KnightM00.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kohavi1995study,
	title        = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	author       = {Ron Kohavi},
	year         = 1995,
	booktitle    = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, {IJCAI} 95, Montr{\'{e}}al Qu{\'{e}}bec, Canada, August 20-25 1995, 2 Volumes},
	publisher    = {Morgan Kaufmann},
	pages        = {1137--1145},
	url          = {http://ijcai.org/Proceedings/95-2/Papers/016.pdf},
	timestamp    = {Tue, 20 Aug 2019 16:17:30 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/Kohavi95.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{komkov2021advhat,
	title        = {AdvHat: Real-World Adversarial Attack on ArcFace Face ID System},
	author       = {Komkov, Stepan and Petiushko, Aleksandr},
	year         = 2021,
	booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
	volume       = {},
	number       = {},
	pages        = {819--826},
	doi          = {10.1109/ICPR48806.2021.9412236}
}
@article{koupaee2018wikihow,
	title        = {WikiHow: {A} Large Scale Text Summarization Dataset},
	author       = {Mahnaz Koupaee and William Yang Wang},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1810.09305},
	url          = {http://arxiv.org/abs/1810.09305},
	eprinttype   = {arXiv},
	eprint       = {1810.09305},
	timestamp    = {Wed, 31 Oct 2018 14:24:29 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1810-09305.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kozarzewski2011similarity,
	title        = {Similarity of symbolic sequences},
	author       = {Kozarzewski, B},
	year         = 2011,
	journal      = {arXiv preprint arXiv:1108.1979}
}
@article{krizhevsky2009learning,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Krizhevsky, Alex and Hinton, Geoffrey and others},
	year         = 2009,
	publisher    = {Toronto, ON, Canada}
}
@article{krokhmal2002portfolio,
	title        = {Portfolio optimization with conditional value-at-risk objective and constraints},
	author       = {Krokhmal, Pavlo and Palmquist, Jonas and Uryasev, Stanislav},
	year         = 2002,
	journal      = {Journal of risk},
	publisher    = {Citeseer},
	volume       = 4,
	pages        = {43--68}
}
@book{kumar2014twitter,
	title        = {Twitter Data Analytics},
	author       = {Shamanth Kumar and Fred Morstatter and Huan Liu},
	year         = 2014,
	publisher    = {Springer},
	series       = {Springer Briefs in Computer Science},
	doi          = {10.1007/978-1-4614-9372-3},
	isbn         = {978-1-4614-9372-3},
	url          = {https://doi.org/10.1007/978-1-4614-9372-3},
	timestamp    = {Tue, 18 Jun 2019 16:48:39 +0200},
	biburl       = {https://dblp.org/rec/books/sp/KumarML14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kurakin2016adversarial,
	title        = {Adversarial Machine Learning at Scale},
	author       = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJm4T4Kgx},
	timestamp    = {Thu, 25 Jul 2019 14:25:40 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/KurakinGB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kurakin2018adversarial,
	title        = {Adversarial examples in the physical world},
	author       = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HJGU3Rodl},
	timestamp    = {Thu, 04 Apr 2019 13:20:08 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/KurakinGB17a.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kusner2015word,
	title        = {From Word Embeddings To Document Distances},
	author       = {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
	year         = 2015,
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	volume       = 37,
	pages        = {957--966},
	url          = {https://proceedings.mlr.press/v37/kusnerb15.html},
	editor       = {Bach, Francis and Blei, David},
	pdf          = {http://proceedings.mlr.press/v37/kusnerb15.pdf},
	abstract     = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.}
}
@inproceedings{kwan2020understanding,
	title        = {Understanding Public Sentiments, Opinions and Topics about COVID-19 using Twitter},
	author       = {Shaynn-Ly Kwan, Jolin and Hui Lim, Kwan},
	year         = 2020,
	booktitle    = {2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
	volume       = {},
	number       = {},
	pages        = {623--626},
	doi          = {10.1109/ASONAM49781.2020.9381384}
}
@inproceedings{lamontagne2006combining,
	title        = {Combining Multiple Similarity Metrics Using a Multicriteria Approach},
	author       = {Lamontagne, Luc and Abi-Zeid, Ir{\`e}ne},
	year         = 2006,
	booktitle    = {Advances in Case-Based Reasoning},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {415--428},
	isbn         = {978-3-540-36846-5},
	editor       = {Roth-Berghofer, Thomas R. and G{\"o}ker, Mehmet H. and G{\"u}venir, H. Altay},
	abstract     = {The design of a CBR system involves the use of similarity metrics. For many applications, various functions can be adopted to compare case features and to aggregate them into a global similarity measure. Given the availability of multiple similarity metrics, the designer is hence left with two options in order to come up with a working system: Either select one similarity metric or try to combine multiple metrics in a super-metric. In this paper, we study how techniques borrowed from multicriteria decision aid can be applied to CBR for combining the results of multiple similarity metrics. The problem of multi-metrics retrieval is presented as an instance of the problem of ranking alternatives based on multiple attributes. Discrete methods such as ELECTRE II have been proposed by the multicriteria decision aid community to address such situations. We conducted our experiments for ranking cases with ELECTRE II, a procedure based on pairwise comparisons. We used textual cases and multiple metrics. Our results indicate that the use of a combination of metrics with a multicriteria decision aid method can increase retrieval precision and provide an advantage over weighted sum combinations especially when similarity is measured on scales that are different in nature.}
}
@inproceedings{lan2019albert,
	title        = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language Representations},
	author       = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=H1eA7AEtvS},
	timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{landauer1997solution,
	title        = {A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
	author       = {Landauer, Thomas K. and Dumais, Susan T.},
	year         = 1997,
	journal      = {Psychological Review},
	publisher    = {American Psychological Association},
	address      = {US},
	volume       = 104,
	number       = 2,
	pages        = {211--240},
	doi          = {10.1037/0033-295X.104.2.211},
	url          = {https://doi.org/10.1037/0033-295X.104.2.211},
	keywords     = {*Knowledge Level; *Learning; *Semantics; *Theories; Psycholinguistics},
	abstract     = {How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena and problems are sketched. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}
@book{lanham1992revising,
	title        = {Revising prose},
	author       = {Lanham, Richard A and Stodel, James},
	year         = 1992,
	publisher    = {Macmillan Publishing Company}
}
@article{lansley2016geography,
	title        = {The geography of Twitter topics in London},
	author       = {Guy Lansley and Paul A. Longley},
	year         = 2016,
	journal      = {Computers, Environment and Urban Systems},
	volume       = 58,
	pages        = {85--96},
	doi          = {https://doi.org/10.1016/j.compenvurbsys.2016.04.002},
	issn         = {0198-9715},
	url          = {https://www.sciencedirect.com/science/article/pii/S0198971516300394},
	keywords     = {Social media, Twitter, Topic modelling, Latent Dirichlet Allocation, Geotemporal},
	abstract     = {Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time-stamped and (sometimes) precisely located. Such data can be mined to provide planners, marketers and researchers with useful information about activities and opinions across time and space. However, in their raw form, textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters. This paper explores the use of an unsupervised learning algorithm to classify geo-tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups, following extensive text cleaning techniques. Our classification identifies 20 distinctive and interpretive topic groupings, which represent key types of Tweets, from describing activities or informal conversations between users, to the use of check-in applets. Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users. Topics and attitudes expressed through Tweets are found to vary substantially across Inner London, and by time of day. Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio-economic characteristics of users, but place and local activities can also exert a considerable influence. Overall, the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London.}
}
@article{lastra2017hesml,
	title        = {HESML: A scalable ontology-based semantic similarity measures library with a set of reproducible experiments and a replication dataset},
	author       = {Juan J. Lastra-Díaz and Ana García-Serrano and Montserrat Batet and Miriam Fernández and Fernando Chirigati},
	year         = 2017,
	journal      = {Information Systems},
	volume       = 66,
	pages        = {97--118},
	doi          = {https://doi.org/10.1016/j.is.2017.02.002},
	issn         = {0306-4379},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306437916303246},
	keywords     = {HESML, PosetHERep, Semantic measures library, Ontology-based semantic similarity measures, Intrinsic and corpus-based Information Content models, Reproducible experiments on word similarity, WNSimRep v1 dataset, ReproZip, WordNet-based semantic similarity measures},
	abstract     = {This work is a detailed companion reproducibility paper of the methods and experiments proposed by Lastra-Díaz and García-Serrano in (2015, 2016) [56-58], which introduces the following contributions: (1) a new and efficient representation model for taxonomies, called PosetHERep, which is an adaptation of the half-edge data structure commonly used to represent discrete manifolds and planar graphs; (2) a new Java software library called the Half-Edge Semantic Measures Library (HESML) based on PosetHERep, which implements most ontology-based semantic similarity measures and Information Content (IC) models reported in the literature; (3) a set of reproducible experiments on word similarity based on HESML and ReproZip with the aim of exactly reproducing the experimental surveys in the three aforementioned works; (4) a replication framework and dataset, called WNSimRep v1, whose aim is to assist the exact replication of most methods reported in the literature; and finally, (5) a set of scalability and performance benchmarks for semantic measures libraries. PosetHERep and HESML are motivated by several drawbacks in the current semantic measures libraries, especially the performance and scalability, as well as the evaluation of new methods and the replication of most previous methods. The reproducible experiments introduced herein are encouraged by the lack of a set of large, self-contained and easily reproducible experiments with the aim of replicating and confirming previously reported results. Likewise, the WNSimRep v1 dataset is motivated by the discovery of several contradictory results and difficulties in reproducing previously reported methods and experiments. PosetHERep proposes a memory-efficient representation for taxonomies which linearly scales with the size of the taxonomy and provides an efficient implementation of most taxonomy-based algorithms used by the semantic measures and IC models, whilst HESML provides an open framework to aid research into the area by providing a simpler and more efficient software architecture than the current software libraries. Finally, we prove the outperformance of HESML on the state-of-the-art libraries, as well as the possibility of significantly improving their performance and scalability without caching using PosetHERep.}
}
@article{lastra2019reproducible,
	title        = {A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art},
	author       = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre},
	year         = 2019,
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = 85,
	pages        = {645--665},
	doi          = {https://doi.org/10.1016/j.engappai.2019.07.010},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197619301745},
	keywords     = {Ontology-based semantic similarity measures, Word embedding models, Information Content models, WordNet, Experimental survey, HESML},
	abstract     = {Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.}
}
@inproceedings{le2014distributed,
	title        = {Distributed Representations of Sentences and Documents},
	author       = {Le, Quoc and Mikolov, Tomas},
	year         = 2014,
	month        = {22--24 Jun},
	booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Bejing, China},
	series       = {Proceedings of Machine Learning Research},
	volume       = 32,
	number       = 2,
	pages        = {1188--1196},
	url          = {https://proceedings.mlr.press/v32/le14.html},
	editor       = {Xing, Eric P. and Jebara, Tony},
	pdf          = {http://proceedings.mlr.press/v32/le14.pdf},
	abstract     = {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, "powerful," "strong"  and "Paris" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.}
}
@misc{lecunmnist,
	author       = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
	journal      = {MNIST handwritten digit database},
	url          = {http://yann.lecun.com/exdb/mnist/}
}
@inproceedings{lecuyer2019certified,
	title        = {Certified robustness to adversarial examples with differential privacy},
	author       = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	year         = 2019,
	booktitle    = {2019 IEEE Symposium on Security and Privacy (SP)},
	pages        = {656--672},
	organization = {IEEE}
}
@article{lee1988thirteen,
	title        = {Thirteen ways to look at the correlation coefficient},
	author       = {Lee Rodgers, Joseph and Nicewander, W Alan},
	year         = 1988,
	journal      = {The American Statistician},
	publisher    = {Taylor  \& Francis},
	volume       = 42,
	number       = 1,
	pages        = {59--66}
}
@article{lee2011novel,
	title        = {A novel sentence similarity measure for semantic-based expert systems},
	author       = {Ming Che Lee},
	year         = 2011,
	journal      = {Expert Systems with Applications},
	volume       = 38,
	number       = 5,
	pages        = {6392--6399},
	doi          = {https://doi.org/10.1016/j.eswa.2010.10.043},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417410011875},
	keywords     = {Sentence, Similarity, Semantic web, Ontology},
	abstract     = {A novel sentence similarity measure for semantic based expert systems is presented. The well-known problem in the fields of semantic processing, such as QA systems, is to evaluate the semantic similarity between irregular sentences. This paper takes advantage of corpus-based ontology to overcome this problem. A transformed vector space model is introduced in this article. The proposed two-phase algorithm evaluates the semantic similarity for two or more sentences via a semantic vector space. The first phase built part-of-speech (POS) based subspaces by the raw data, and the latter carried out a cosine evaluation and adopted the WordNet ontology to construct the semantic vectors. Unlike other related researches that focused only on short sentences, our algorithm is applicable to short (4-5 words), medium (8-12 words), and even long sentences (over 12 words). The experiment demonstrates that the proposed algorithm has outstanding performance in handling long sentences with complex syntax. The significance of this research lies in the semantic similarity extraction of sentences, with arbitrary structures.}
}
@article{lee2020lipschitz,
	title        = {Lipschitz-certifiable training with a tight outer bound},
	author       = {Lee, Sungyoon and Lee, Jaewook and Park, Saerom},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {16891--16902}
}
@inproceedings{lehmann2012dynamical,
	title        = {Dynamical Classes of Collective Attention in Twitter},
	author       = {Lehmann, Janette and Gon\c{c}alves, Bruno and Ramasco, Jos\'{e} J. and Cattuto, Ciro},
	year         = 2012,
	booktitle    = {Proceedings of the 21st International Conference on World Wide Web},
	location     = {Lyon, France},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '12},
	pages        = {251--260},
	doi          = {10.1145/2187836.2187871},
	isbn         = 9781450312295,
	url          = {https://doi.org/10.1145/2187836.2187871},
	abstract     = {Micro-blogging systems such as Twitter expose digital traces of social discourse with an unprecedented degree of resolution of individual behaviors. They offer an opportunity to investigate how a large-scale social system responds to exogenous or endogenous stimuli, and to disentangle the temporal, spatial and topical aspects of users' activity. Here we focus on spikes of collective attention in Twitter, and specifically on peaks in the popularity of hashtags. Users employ hashtags as a form of social annotation, to define a shared context for a specific event, topic, or meme. We analyze a large-scale record of Twitter activity and find that the evolution of hashtag popularity over time defines discrete classes of hashtags. We link these dynamical classes to the events the hashtags represent and use text mining techniques to provide a semantic characterization of the hashtag classes. Moreover, we track the propagation of hashtags in the Twitter social network and find that epidemic spreading plays a minor role in hashtag popularity, which is mostly driven by exogenous factors.},
	numpages     = 10,
	keywords     = {content analysis, online social networks, micro-blogging}
}
@inproceedings{lesk1986automatic,
	title        = {Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone},
	author       = {Michael Lesk},
	year         = 1986,
	booktitle    = {Proceedings of the 5th Annual International Conference on Systems Documentation, {SIGDOC} 1986, Toronto, Ontario, Canada, 1986},
	publisher    = {{ACM}},
	pages        = {24--26},
	doi          = {10.1145/318723.318728},
	url          = {https://doi.org/10.1145/318723.318728},
	editor       = {Virginia DeBuys},
	timestamp    = {Tue, 06 Nov 2018 11:07:46 +0100},
	biburl       = {https://dblp.org/rec/conf/sigdoc/Lesk86.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{levenshtein1966binary,
	title        = {Binary codes capable of correcting deletions, insertions, and reversals},
	author       = {Levenshtein, Vladimir I and others},
	year         = 1966,
	booktitle    = {Soviet physics doklady},
	volume       = 10,
	number       = 8,
	pages        = {707--710},
	organization = {Soviet Union}
}
@article{leveson1993investigation,
	title        = {An investigation of the Therac-25 accidents},
	author       = {Leveson, N.G. and Turner, C.S.},
	year         = 1993,
	journal      = {Computer},
	publisher    = {IEEE},
	volume       = 26,
	number       = 7,
	pages        = {18--41},
	doi          = {10.1109/MC.1993.274940}
}
@inproceedings{levy2014neural,
	title        = {Neural Word Embedding as Implicit Matrix Factorization},
	author       = {Levy, Omer and Goldberg, Yoav},
	year         = 2014,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 27,
	url          = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
	editor       = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger}
}
@inproceedings{ley2002dblp,
	title        = {The DBLP computer science bibliography: Evolution, research issues, perspectives},
	author       = {Ley, Michael},
	year         = 2002,
	booktitle    = {International symposium on string processing and information retrieval},
	pages        = {1--10},
	organization = {Springer}
}
@inproceedings{li2019analyzing,
	title        = {Analyzing deep neural networks with symbolic propagation: Towards higher precision and faster verification},
	author       = {Li, Jianlin and Liu, Jiangchao and Yang, Pengfei and Chen, Liqian and Huang, Xiaowei and Zhang, Lijun},
	year         = 2019,
	booktitle    = {Static Analysis: 26th International Symposium, SAS 2019, Porto, Portugal, October 8--11, 2019, Proceedings 26},
	pages        = {296--319},
	organization = {Springer}
}
@inproceedings{li2019preventing,
	title        = {Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks},
	author       = {Li, Qiyang and Haque, Saminul and Anil, Cem and Lucas, James and Grosse, Roger B and Jacobsen, Joern-Henrik},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1ce3e6e3f452828e23a0c94572bef9d9-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{li2019universal,
	title        = {Universal Perturbation Attack Against Image Retrieval},
	author       = {Li, Jie and Ji, Rongrong and Liu, Hong and Hong, Xiaopeng and Gao, Yue and Tian, Qi},
	year         = 2019,
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages        = {4899--4908}
}
@article{li2020light,
	title        = {Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems},
	author       = {Haoliang Li and Yufei Wang and Xiaofei Xie and Yang Liu and Shiqi Wang and Renjie Wan and Lap{-}Pui Chau and Alex C. Kot},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2009.06996},
	url          = {https://arxiv.org/abs/2009.06996},
	eprinttype   = {arXiv},
	eprint       = {2009.06996},
	timestamp    = {Thu, 14 Oct 2021 09:14:00 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2009-06996.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2020tilted,
	title        = {Tilted Empirical Risk Minimization},
	author       = {Tian Li and Ahmad Beirami and Maziar Sanjabi and Virginia Smith},
	year         = 2021,
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=K5YasWXZT3O},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/0005BSS21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2023sok,
	title        = {SoK: Certified Robustness for Deep Neural Networks},
	author       = {Linyi Li and Tao Xie and Bo Li},
	year         = 2023,
	booktitle    = {44th {IEEE} Symposium on Security and Privacy, {SP} 2023, San Francisco, CA, USA, 22-26 May 2023},
	publisher    = {IEEE},
	url          = {https://arxiv.org/abs/2009.04131}
}
@article{liao2012mining,
	title        = {Data mining of social networks represented as graphs},
	author       = {David F. Nettleton},
	year         = 2013,
	journal      = {Computer Science Review},
	volume       = 7,
	pages        = {1--34},
	doi          = {https://doi.org/10.1016/j.cosrev.2012.12.001},
	issn         = {1574-0137},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574013712000445},
	keywords     = {Graphs, Online social networks, Graph mining, Data mining, Statistical analysis, Data modelling},
	abstract     = {In this survey we review the literature and concepts of the data mining of social networks, with special emphasis on their representation as a graph structure. The survey is divided into two principal parts: first we conduct a survey of the literature which forms the ‘basis' and background for the field; second we define a set of ‘hot topics' which are currently in vogue in congresses and the literature. The ‘basis' or background part is divided into four major themes: graph theory, social networks, online social networks and graph mining. The graph mining theme is organized into ten subthemes. The second, ‘hot topic' part, is divided into five major themes: communities, influence and recommendation, models metrics and dynamics, behaviour and relationships, and information diffusion.}
}
@book{liddell1894greek,
	title        = {A greek-english lexicon},
	author       = {Liddell, Henry George and W., Glare P G and McKenzie, Roderick and Mackenzie, Roderick and Scott, Robert and Jones, Henry Stuart},
	year         = 1996,
	publisher    = {Clarendon Press},
	place        = {Oxford}
}
@inproceedings{lim2012topological,
	title        = {A Topological Approach for Detecting Twitter Communities with Common Interests},
	author       = {Lim, Kwan Hui and Datta, Amitava},
	year         = 2013,
	booktitle    = {Ubiquitous Social Media Analysis},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {23--43},
	isbn         = {978-3-642-45392-2},
	editor       = {Atzmueller, Martin and Chin, Alvin and Helic, Denis and Hotho, Andreas},
	abstract     = {The efficient identification of communities with common interests is a key consideration in applying targeted advertising and viral marketing to online social networking sites. Existing methods involve large scale community detection on the entire social network before determining the interests of individuals within these communities. This approach is both computationally intensive and may result in communities without a common interest. We propose an efficient topological-based approach for detecting communities that share common interests on Twitter. Our approach involves first identifying celebrities that are representative of an interest category before detecting communities based on linkages among followers of these celebrities. We also study the network characteristics and tweeting behaviour of these communities, and the effects of deepening or specialization of interest on their community structures. In particular, our evaluation on Twitter shows that these detected communities comprise members who are well-connected, cohesive and tweet about their common interest.}
}
@article{lim2016interaction,
	title        = {An interaction-based approach to detecting highly interactive Twitter communities using tweeting links},
	author       = {Lim, Kwan Hui and Datta, Amitava},
	year         = 2016,
	journal      = {Web Intelligence},
	publisher    = {IOS Press},
	volume       = 14,
	pages        = {1--15},
	doi          = {10.3233/WEB-160328},
	issn         = {2405-6464},
	url          = {https://doi.org/10.3233/WEB-160328},
	note         = 1,
	keywords     = {Twitter; tweets; social network analysis; community detection; like-minded communities; interaction links; common interests},
	abstract     = {The immense popularity and rapid growth of Online Social Networks (OSN) have attracted the interest of researchers and companies, particularly in how users group together to form communities online. While many community detection algorithms have been developed to detect communities on such OSNs, most of these algorithms are based only on topological links and researchers have observed that many topological links do not translate to actual user interaction. As such, many members of the detected communities do not communicate frequently to each other. This inactivity creates a problem in targeted advertising and viral marketing, which require the community to be highly active so as to facilitate the diffusion of product/service information. We propose an approach to detect highly interactive Twitter communities that share common interests, based on the frequency and patterns of direct tweeting among users, rather than the topological information implicit in follower/following links. Our experimental results show that communities detected by our proposed approach are more cohesive and connected within different interest groups, based on topological measures. We also show that the detected communities actively interact about the specific interests, based on the high frequency of {\#}hashtags and @mentions related to this interest. In addition, we study the trends in their tweeting patterns such as how they follow and unfollow other users, and observe that our approach detects communities comprising users whose links are more persistent compared to those in other groups of users.}
}
@inproceedings{lim2017clustop,
	title        = {ClusTop: A clustering-based topic modelling algorithm for twitter using word networks},
	author       = {Lim, Kwan Hui and Karunasekera, Shanika and Harwood, Aaron},
	year         = 2017,
	booktitle    = {2017 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {2009--2018},
	doi          = {10.1109/BigData.2017.8258147}
}
@inproceedings{lin1998information,
	title        = {An Information-Theoretic Definition of Similarity},
	author       = {Dekang Lin},
	year         = 1998,
	booktitle    = {Proceedings of the Fifteenth International Conference on Machine Learning {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
	publisher    = {Morgan Kaufmann},
	pages        = {296--304},
	editor       = {Jude W. Shavlik},
	timestamp    = {Thu, 30 Jun 2011 10:34:12 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/Lin98.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lin2014microsoft,
	title        = {Microsoft coco: Common objects in context},
	author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	year         = 2014,
	booktitle    = {European conference on computer vision},
	pages        = {740--755},
	organization = {Springer}
}
@inproceedings{lin2017structured,
	title        = {A Structured Self-Attentive Sentence Embedding},
	author       = {Zhouhan Lin and Minwei Feng and C{\'{\i}}cero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJC\_jUqxe},
	timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LinFSYXZB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lin2019nesterov,
	title        = {Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks},
	author       = {Jiadong Lin and Chuanbiao Song and Kun He and Liwei Wang and John E. Hopcroft},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SJlHwkBYDH}
}
@inproceedings{lin2019robustness,
	title        = {Robustness Verification of Classification Deep Neural Networks via Linear Programming},
	author       = {Lin, Wang and Yang, Zhengfeng and Chen, Xin and Zhao, Qingye and Li, Xiangkun and Liu, Zhiming and He, Jifeng},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {11418--11427}
}
@inproceedings{linstead2007mining,
	title        = {Mining Concepts from Code with Probabilistic Topic Models},
	author       = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
	year         = 2007,
	booktitle    = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
	location     = {Atlanta, Georgia, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASE '07},
	pages        = {461--464},
	doi          = {10.1145/1321631.1321709},
	isbn         = 9781595938824,
	url          = {https://doi.org/10.1145/1321631.1321709},
	abstract     = {We develop and apply statistical topic models to software as a means of extracting concepts from source code. The effectiveness of the technique is demonstrated on 1,555 projects from SourceForge and Apache consisting of 113,000 files and 19 million lines of code. In addition to providing an automated, unsupervised, solution to the problem of summarizing program functionality, the approach provides a probabilistic framework with which to analyze and visualize source file similarity. Finally, we introduce an information-theoretic approach for computing tangling and scattering of extracted concepts, and present preliminary results},
	numpages     = 4,
	keywords     = {program understanding, topic models, mining software}
}
@inproceedings{liu2014chi,
	title        = {CHI 1994-2013: Mapping Two Decades of Intellectual Progress through Co-Word Analysis},
	author       = {Liu, Yong and Goncalves, Jorge and Ferreira, Denzil and Xiao, Bei and Hosio, Simo and Kostakos, Vassilis},
	year         = 2014,
	booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	location     = {Toronto, Ontario, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '14},
	pages        = {3553--3562},
	doi          = {10.1145/2556288.2556969},
	isbn         = 9781450324731,
	url          = {https://doi.org/10.1145/2556288.2556969},
	abstract     = {This study employs hierarchical cluster analysis, strategic diagrams and network analysis to map and visualize the intellectual landscape of the CHI conference on Human Computer Interaction through the use of co-word analysis. The study quantifies and describes the thematic evolution of the field based on a total of 3152 CHI articles and their associated 16035 keywords published between 1994 and 2013. The analysis is conducted for two time periods (1994-2003, 2004-2013) and a comparison between them highlights the underlying trends in our community. More significantly, this study identifies the evolution of major themes in the discipline, and highlights individual topics as popular, core, or backbone research topics within HCI.},
	numpages     = 10,
	keywords     = {cohesion, bibliometric study, hci, conceptual evolution, coherence, co-word analysis}
}
@inproceedings{liu2017improved,
	title        = {Improved Image Captioning via Policy Gradient Optimization of SPIDEr},
	author       = {Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin},
	year         = 2017,
	month        = {Oct},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{liu2017sphereface,
	title        = {SphereFace: Deep Hypersphere Embedding for Face Recognition},
	author       = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
	year         = 2017,
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {212--220}
}
@inproceedings{liu2018adv,
	title        = {Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network},
	author       = {Xuanqing Liu and Yao Li and Chongruo Wu and Cho{-}Jui Hsieh},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rk4Qso0cKm},
	timestamp    = {Sun, 02 Oct 2022 16:05:32 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LiuLWH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2018generative,
	title        = {Generative Adversarial Network for Abstractive Text Summarization},
	author       = {Linqing Liu and Yao Lu and Min Yang and Qiang Qu and Jia Zhu and Hongyan Li},
	year         = 2018,
	booktitle    = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018},
	publisher    = {{AAAI} Press},
	pages        = {8109--8110},
	url          = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16238},
	editor       = {Sheila A. McIlraith and Kilian Q. Weinberger},
	timestamp    = {Tue, 08 Mar 2022 21:46:37 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/LiuLYQZL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2018towards,
	title        = {Towards Robust Neural Networks via Random Self-ensemble},
	author       = {Liu, Xuanqing and Cheng, Minhao and Zhang, Huan and Hsieh, Cho-Jui},
	year         = 2018,
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@inproceedings{liu2019adaptiveface,
	title        = {AdaptiveFace: Adaptive Margin and Sampling for Face Recognition},
	author       = {Liu, Hao and Zhu, Xiangyu and Lei, Zhen and Li, Stan Z.},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{liu2019roberta,
	title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1907.11692},
	url          = {http://arxiv.org/abs/1907.11692},
	archiveprefix = {arXiv},
	eprint       = {1907.11692},
	timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	eprinttype   = {arXiv}
}
@inproceedings{liu2021swin,
	title        = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
	author       = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year         = 2021,
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages        = {10012--10022}
}
@inproceedings{logeswaran2018efficient,
	title        = {An efficient framework for learning sentence representations},
	author       = {Lajanugen Logeswaran and Honglak Lee},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJvJXZb0W},
	timestamp    = {Thu, 25 Jul 2019 14:25:49 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LogeswaranL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{long_2013,
	title        = {Hemingway Editor},
	author       = {Adam and Long, Ben},
	year         = 2013,
	journal      = {Hemingway Editor},
	url          = {https://hemingwayapp.com/}
}
@article{lopez2019word,
	title        = {Word n-gram attention models for sentence similarity and inference},
	author       = {I. Lopez-Gazpio and M. Maritxalar and M. Lapata and E. Agirre},
	year         = 2019,
	journal      = {Expert Systems with Applications},
	volume       = 132,
	pages        = {1--11},
	doi          = {https://doi.org/10.1016/j.eswa.2019.04.054},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417419302842},
	keywords     = {Attention models, Deep learning, Natural language understanding, Natural Language Inference, Semantic textual similarity},
	abstract     = {Semantic Textual Similarity and Natural Language Inference are two popular natural language understanding tasks used to benchmark sentence representation models where two sentences are paired. In such tasks sentences are represented as bag of words, sequences, trees or convolutions, but the attention model is based on word pairs. In this article we introduce the use of word n-grams in the attention model. Our results on five datasets show an error reduction of up to 41\% with respect to the word-based attention model. The improvements are especially relevant with low data regimes and, in the case of natural language inference, on the recently released hard subset of Natural Language Inference datasets.}
}
@article{luhn1957statistical,
	title        = {A Statistical Approach to Mechanized Encoding and Searching of Literary Information},
	author       = {Luhn, H. P.},
	year         = 1957,
	journal      = {IBM Journal of Research and Development},
	volume       = 1,
	number       = 4,
	pages        = {309--317},
	doi          = {10.1147/rd.14.0309}
}
@article{lund1996producing,
	title        = {Producing high-dimensional semantic spaces from lexical co-occurrence},
	author       = {Lund, Kevin and Burgess, Curt},
	year         = 1996,
	month        = {Jun},
	day          = {01},
	journal      = {Behavior Research Methods, Instruments, {\&} Computers},
	volume       = 28,
	number       = 2,
	pages        = {203--208},
	doi          = {10.3758/BF03204766},
	issn         = {1532-5970},
	url          = {https://doi.org/10.3758/BF03204766},
	abstract     = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).}
}
@article{lyon2004theoretical,
	title        = {A theoretical basis to the automated detection of copying between texts, and its practical implementation in the Ferret plagiarism and collusion detector},
	author       = {Lyon, Caroline and Barrett, Ruth and Malcolm, James},
	year         = 2004,
	journal      = {Plagiarism: Prevention, Practice and Policies}
}
@inproceedings{lyu2021towards,
	title        = {Towards Evaluating and Training Verifiably Robust Neural Networks},
	author       = {Lyu, Zhaoyang and Guo, Minghao and Wu, Tong and Xu, Guodong and Zhang, Kehuan and Lin, Dahua},
	year         = 2021,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {4308--4317}
}
@inproceedings{ma2012will,
	title        = {Will This \#hashtag Be Popular Tomorrow?},
	author       = {Ma, Zongyang and Sun, Aixin and Cong, Gao},
	year         = 2012,
	booktitle    = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Portland, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '12},
	pages        = {1173--1174},
	doi          = {10.1145/2348283.2348525},
	isbn         = 9781450314725,
	url          = {https://doi.org/10.1145/2348283.2348525},
	abstract     = {Hashtags are widely used in Twitter to define a shared context for events or topics. In this paper, we aim to predict hashtag popularity in near future (i.e., next day). Given a hashtag that has the potential to be popular in the next day, we construct a hashtag profile using the tweets containing the hashtag, and extract both content and context features for hashtag popularity prediction. We model this prediction problem as a classification problem and evaluate the effectiveness of the extracted features and classification models.},
	numpages     = 2,
	keywords     = {hashtag, hashtag clarity, popularity prediction, twitter}
}
@inproceedings{ma2018characterizing,
	title        = {Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},
	author       = {Xingjun Ma and Bo Li and Yisen Wang and Sarah M. Erfani and Sudanthi N. R. Wijewickrema and Grant Schoenebeck and Dawn Song and Michael E. Houle and James Bailey},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=B1gJ1L2aW},
	timestamp    = {Sun, 27 Oct 2019 17:57:12 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/Ma0WEWSSHB18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{madry2017towards,
	title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	author       = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJzIBfZAb},
	timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{maheshwary2021generating,
	title        = {Generating Natural Language Attacks in a Hard Label Black Box Setting},
	author       = {Rishabh Maheshwary and Saket Maheshwary and Vikram Pudi},
	year         = 2021,
	booktitle    = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021},
	publisher    = {{AAAI} Press},
	pages        = {13525--13533},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17595},
	timestamp    = {Mon, 07 Jun 2021 11:46:04 +0200},
	biburl       = {https://dblp.org/rec/conf/aaai/MaheshwaryMP21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{maier2018applying,
	title        = {Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology},
	author       = {Daniel Maier and A. Waldherr and P. Miltner and G. Wiedemann and A. Niekler and A. Keinert and B. Pfetsch and G. Heyer and U. Reber and T. Häussler and H. Schmid-Petri and S. Adam},
	year         = 2018,
	journal      = {Communication Methods and Measures},
	publisher    = {Routledge},
	volume       = 12,
	number       = {2-3},
	pages        = {93--118},
	doi          = {10.1080/19312458.2018.1430754},
	url          = {https://doi.org/10.1080/19312458.2018.1430754},
	eprint       = {https://doi.org/10.1080/19312458.2018.1430754}
}
@article{manning2006local,
	title        = {Local textual inference: it's hard to circumscribe, but you know it when you see it-and nlp needs it.},
	author       = {Manning, Christopher D},
	year         = 2006,
	publisher    = {Citeseer}
}
@unpublished{marteau2018sequence,
	title        = {{Sequence Covering Similarity for Symbolic Sequence Comparison}},
	author       = {Marteau, Pierre-Fran{\c c}ois},
	year         = 2018,
	month        = Mar,
	url          = {https://hal.archives-ouvertes.fr/hal-01689286},
	note         = {working paper or preprint},
	keywords     = {Symbolic Sequence Match- ing ; Sequence Covering ; Sequence Covering Similarity ; String Matching ; Index terms- Sequence Covering Similarity ; Sequence Mining ; Similarity ; Symbolic Sequence Matching},
	pdf          = {https://hal.archives-ouvertes.fr/hal-01689286v3/file/CoveringSimilarity-v2.pdf},
	hal_id       = {hal-01689286},
	hal_version  = {v3}
}
@article{martinez2013semantic,
	title        = {Semantic similarity measurement using historical google search patterns},
	author       = {Martinez-Gil, Jorge and Aldana-Montes, Jos{\'e} F.},
	year         = 2013,
	month        = {Jul},
	day          = {01},
	journal      = {Information Systems Frontiers},
	volume       = 15,
	number       = 3,
	pages        = {399--410},
	doi          = {10.1007/s10796-012-9404-7},
	issn         = {1572-9419},
	url          = {https://doi.org/10.1007/s10796-012-9404-7},
	abstract     = {Computing the semantic similarity between terms (or short text expressions) that have the same meaning but which are not lexicographically similar is an important challenge in the information integration field. The problem is that techniques for textual semantic similarity measurement often fail to deal with words not covered by synonym dictionaries. In this paper, we try to solve this problem by determining the semantic similarity for terms using the knowledge inherent in the search history logs from the Google search engine. To do this, we have designed and evaluated four algorithmic methods for measuring the semantic similarity between terms using their associated history search patterns. These algorithmic methods are: a) frequent co-occurrence of terms in search patterns, b) computation of the relationship between search patterns, c) outlier coincidence on search patterns, and d) forecasting comparisons. We have shown experimentally that some of these methods correlate well with respect to human judgment when evaluating general purpose benchmark datasets, and significantly outperform existing methods when evaluating datasets containing terms that do not usually appear in dictionaries.}
}
@article{martinez2014overview,
	title        = {An overview of textual semantic similarity measures based on web intelligence},
	author       = {Martinez-Gil, Jorge},
	year         = 2014,
	month        = {Dec},
	day          = {01},
	journal      = {Artificial Intelligence Review},
	volume       = 42,
	number       = 4,
	pages        = {935--943},
	doi          = {10.1007/s10462-012-9349-8},
	issn         = {1573-7462},
	url          = {https://doi.org/10.1007/s10462-012-9349-8},
	abstract     = {Computing the semantic similarity between terms (or short text expressions) that have the same meaning but which are not lexicographically similar is a key challenge in many computer related fields. The problem is that traditional approaches to semantic similarity measurement are not suitable for all situations, for example, many of them often fail to deal with terms not covered by synonym dictionaries or are not able to cope with acronyms, abbreviations, buzzwords, brand names, proper nouns, and so on. In this paper, we present and evaluate a collection of emerging techniques developed to avoid this problem. These techniques use some kinds of web intelligence to determine the degree of similarity between text expressions. These techniques implement a variety of paradigms including the study of co-occurrence, text snippet comparison, frequent pattern finding, or search log analysis. The goal is to substitute the traditional techniques where necessary.}
}
@article{matloff1984asymptotic,
	title        = {The asymptotic distribution of an estimator of the Bayes error rate},
	author       = {Norman Matloff and Ronald Pruitt},
	year         = 1984,
	journal      = {Pattern Recognition Letters},
	volume       = 2,
	number       = 5,
	pages        = {271--274},
	doi          = {https://doi.org/10.1016/0167-8655(84)90013-8},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/0167865584900138},
	keywords     = {Asymptotic distribution, Bayes error rate, risk averaging, logistic model},
	abstract     = {The asymptotic distribution of a class of estimators of the Bayes error rate in pattern recognition problems is derived. The class consists of estimators of the risk-averaging type, applied to parametric models such as the logistic function.}
}
@inproceedings{mccann2017learned,
	title        = {Learned in Translation: Contextualized Word Vectors},
	author       = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	url          = {https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{mccann2018natural,
	title        = {The Natural Language Decathlon: Multitask Learning as Question Answering},
	author       = {Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1806.08730},
	url          = {http://arxiv.org/abs/1806.08730},
	eprinttype   = {arXiv},
	eprint       = {1806.08730},
	timestamp    = {Mon, 13 Aug 2018 16:49:05 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1806-08730.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{mcgowan_2021,
	title        = {Topic model based recommendation systems},
	author       = {McGowan, Jamie},
	year         = 2021,
	month        = {Sep},
	journal      = {Medium},
	publisher    = {Towards Data Science},
	url          = {https://towardsdatascience.com/topic-model-based-recommendation-systems-a02d198408b7}
}
@book{mead1990design,
	title        = {The design of experiments: statistical principles for practical applications},
	author       = {Mead, Roger},
	year         = 1990,
	publisher    = {Cambridge university press}
}
@inproceedings{mehrotra2013improving,
	title        = {Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling},
	author       = {Mehrotra, Rishabh and Sanner, Scott and Buntine, Wray and Xie, Lexing},
	year         = 2013,
	booktitle    = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Dublin, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '13},
	pages        = {889--892},
	doi          = {10.1145/2484028.2484166},
	isbn         = 9781450320344,
	url          = {https://doi.org/10.1145/2484028.2484166},
	abstract     = {Twitter, or the world of 140 characters poses serious challenges to the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodified LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA topic models on Twitter content.},
	numpages     = 4,
	keywords     = {lda, microblogs, topic modeling}
}
@inproceedings{merity2016pointer,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Byj72udxe},
	timestamp    = {Thu, 25 Jul 2019 14:25:57 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{merriam_webster_concise,
	title        = {Concise definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/concise}
}
@inproceedings{metzen2017detecting,
	title        = {On Detecting Adversarial Perturbations},
	author       = {Jan Hendrik Metzen and Tim Genewein and Volker Fischer and Bastian Bischoff},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SJzCSf9xg},
	timestamp    = {Thu, 05 Mar 2020 12:59:38 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/MetzenGFB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{metzger2022realizing,
	title        = {Realizing self-adaptive systems via online reinforcement learning and feature-model-guided exploration},
	author       = {Metzger, Andreas and Quinton, Cl{\'e}ment and Mann, Zolt{\'a}n {\'A}d{\'a}m and Baresi, Luciano and Pohl, Klaus},
	year         = 2022,
	month        = {Mar},
	day          = {01},
	journal      = {Computing},
	doi          = {10.1007/s00607-022-01052-x},
	issn         = {1436-5057},
	url          = {https://doi.org/10.1007/s00607-022-01052-x},
	abstract     = {A self-adaptive system can automatically maintain its quality requirements in the presence of dynamic environment changes. Developing a self-adaptive system may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. To realize self-adaptive systems in the presence of design time uncertainty, online machine learning, i.e., machine learning at runtime, is increasingly used. In particular, online reinforcement learning is proposed, which learns suitable adaptation actions through interactions with the environment at runtime. To learn about its environment, online reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens impacts the performance of the learning process. We focus on two problems related to how adaptation actions are explored. First, existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions. Second, they are unaware of system evolution, and thus may explore new adaptation actions introduced during evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and system evolution. Experimental results for two realistic self-adaptive systems indicate an average speed-up of the learning process of 33.7{\%} in the presence of many adaptation actions, and of 50.6{\%} in the presence of evolution.}
}
@misc{michaelis2019benchmarking,
	title        = {Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming},
	author       = {Claudio Michaelis and Benjamin Mitzkus and Robert Geirhos and Evgenia Rusak and Oliver Bringmann and Alexander S. Ecker and Matthias Bethge and Wieland Brendel},
	year         = 2020,
	url          = {https://openreview.net/forum?id=ryljMpNtwr}
}
@inproceedings{mihalcea2007wikify,
	title        = {Wikify! Linking Documents to Encyclopedic Knowledge},
	author       = {Mihalcea, Rada and Csomai, Andras},
	year         = 2007,
	booktitle    = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
	location     = {Lisbon, Portugal},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CIKM '07},
	pages        = {233--242},
	doi          = {10.1145/1321440.1321475},
	isbn         = 9781595938039,
	url          = {https://doi.org/10.1145/1321440.1321475},
	abstract     = {This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.},
	numpages     = 10,
	keywords     = {keyword extraction, wikipedia, word sense disambiguation, semantic annotation}
}
@inproceedings{mikolov2013distributed,
	title        = {Distributed Representations of Words and Phrases and their Compositionality},
	author       = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year         = 2013,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 26,
	url          = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	editor       = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger}
}
@inproceedings{mikolov2013efficient,
	title        = {Efficient Estimation of Word Representations in Vector Space},
	author       = {Tom{\'{a}}s Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year         = 2013,
	booktitle    = {1st International Conference on Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
	url          = {http://arxiv.org/abs/1301.3781},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Mon, 28 Dec 2020 11:31:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{miller1995wordnet,
	title        = {WordNet: A Lexical Database for English},
	author       = {Miller, George A.},
	year         = 1995,
	month        = {nov},
	journal      = {Commun. ACM},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 38,
	number       = 11,
	pages        = {39--41},
	doi          = {10.1145/219717.219748},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/219717.219748},
	issue_date   = {Nov. 1995},
	abstract     = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	numpages     = 3
}
@book{miller1998wordnet,
	title        = {WordNet: An electronic lexical database},
	author       = {Miller, George A},
	year         = 1998,
	publisher    = {The MIT Press},
	editor       = {Fellbaum, Christiane},
	place        = {Cambridge, MA}
}
@inbook{mirjalili2019genetic,
	title        = {Genetic Algorithm},
	author       = {Mirjalili, Seyedali},
	year         = 2019,
	booktitle    = {Evolutionary Algorithms and Neural Networks: Theory and Applications},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {43--55},
	doi          = {10.1007/978-3-319-93025-1_4},
	isbn         = {978-3-319-93025-1},
	url          = {https://doi.org/10.1007/978-3-319-93025-1_4},
	abstract     = {Genetic Algorithm (GA) is one of the first population-based stochastic algorithm proposed in the history. Similar to other EAs, the main operators of GA are selection, crossover, and mutation. This chapter briefly presents this algorithm and applies it to several case studies to observe its performance.}
}
@inproceedings{mirman2018differentiable,
	title        = {Differentiable Abstract Interpretation for Provably Robust Neural Networks},
	author       = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
	year         = 2018,
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {3578--3586},
	url          = {https://proceedings.mlr.press/v80/mirman18b.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf},
	abstract     = {We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.}
}
@article{mirman2021fundamental,
	title        = {The Fundamental Limits of Neural Networks for Interval Certified Robustness},
	author       = {Matthew B Mirman and Maximilian Baader and Martin Vechev},
	year         = 2022,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=fsacLLU35V},
	note         = {}
}
@inproceedings{missier2016tracking,
	title        = {Tracking Dengue Epidemics Using Twitter Content Classification and Topic Modelling},
	author       = {Missier, Paolo and Romanovsky, Alexander and Miu, Tudor and Pal, Atinder and Daniilakis, Michael and Garcia, Alessandro and Cedrim, Diego and da Silva Sousa, Leonardo},
	year         = 2016,
	booktitle    = {Current Trends in Web Engineering},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {80--92},
	isbn         = {978-3-319-46963-8},
	editor       = {Casteleyn, Sven and Dolog, Peter and Pautasso, Cesare},
	abstract     = {Detecting and preventing outbreaks of mosquito-borne diseases such as Dengue and Zika in Brasil and other tropical regions has long been a priority for governments in affected areas. Streaming social media content, such as Twitter, is increasingly being used for health vigilance applications such as flu detection. However, previous work has not addressed the complexity of drastic seasonal changes on Twitter content across multiple epidemic outbreaks. In order to address this gap, this paper contrasts two complementary approaches to detecting Twitter content that is relevant for Dengue outbreak detection, namely supervised classification and unsupervised clustering using topic modelling. Each approach has benefits and shortcomings. Our classifier achieves a prediction accuracy of about 80 {\%} based on a small training set of about 1,000 instances, but the need for manual annotation makes it hard to track seasonal changes in the nature of the epidemics, such as the emergence of new types of virus in certain geographical locations. In contrast, LDA-based topic modelling scales well, generating cohesive and well-separated clusters from larger samples. While clusters can be easily re-generated following changes in epidemics, however, this approach makes it hard to clearly segregate relevant tweets into well-defined clusters.}
}
@inproceedings{mittal2002employing,
	title        = {Employing discrete Bayes error rate for discretization and feature selection tasks},
	author       = {Mittal, A. and Loong-Fah Cheong},
	year         = 2002,
	booktitle    = {2002 IEEE International Conference on Data Mining, 2002. Proceedings.},
	volume       = {},
	number       = {},
	pages        = {298--305},
	doi          = {10.1109/ICDM.2002.1183916}
}
@article{miyato2018virtual,
	title        = {Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning},
	author       = {Miyato, Takeru and Maeda, Shin-Ichi and Koyama, Masanori and Ishii, Shin},
	year         = 2019,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 41,
	number       = 8,
	pages        = {1979--1993},
	doi          = {10.1109/TPAMI.2018.2858821},
	url          = {https://doi.org/10.1109/TPAMI.2018.2858821},
	timestamp    = {Mon, 26 Oct 2020 09:04:26 +0100},
	biburl       = {https://dblp.org/rec/journals/pami/MiyatoMKI19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mnih2013learning,
	title        = {Learning word embeddings efficiently with noise-contrastive estimation},
	author       = {Mnih, Andriy and Kavukcuoglu, Koray},
	year         = 2013,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 26,
	url          = {https://proceedings.neurips.cc/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
	editor       = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger}
}
@book{mohri2018foundations,
	title        = {Foundations of Machine Learning},
	author       = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year         = 2012,
	publisher    = {The MIT Press},
	isbn         = {026201825X},
	abstract     = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.}
}
@inproceedings{moon2015meta,
	title        = {Meta learning of bounds on the Bayes classifier error},
	author       = {Moon, Kevin R. and Hero, Alfred O. and Delouille, B. Véronique},
	year         = 2015,
	booktitle    = {2015 IEEE Signal Processing and Signal Processing Education Workshop (SP/SPE)},
	volume       = {},
	number       = {},
	pages        = {13--18},
	doi          = {10.1109/DSP-SPE.2015.7369520}
}
@inproceedings{moosavi2016deepfool,
	title        = {DeepFool: {A} Simple and Accurate Method to Fool Deep Neural Networks},
	author       = {Seyed{-}Mohsen Moosavi{-}Dezfooli and Alhussein Fawzi and Pascal Frossard},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {2574--2582},
	doi          = {10.1109/CVPR.2016.282},
	url          = {https://doi.org/10.1109/CVPR.2016.282},
	timestamp    = {Sun, 02 Oct 2022 15:58:35 +0200},
	biburl       = {https://dblp.org/rec/conf/cvpr/Moosavi-Dezfooli16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{morris1938foundations,
	title        = {Foundations of the Theory of Signs},
	author       = {Morris, Charles William},
	year         = 1938,
	booktitle    = {International encyclopedia of unified science},
	publisher    = {Chicago University Press},
	pages        = {1--59}
}
@inproceedings{moschitti2006efficient,
	title        = {Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees},
	author       = {Moschitti, Alessandro},
	year         = 2006,
	booktitle    = {Machine Learning: ECML 2006},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {318--329},
	isbn         = {978-3-540-46056-5},
	editor       = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
	abstract     = {In this paper, we provide a study on the use of tree kernels to encode syntactic parsing information in natural language learning. In particular, we propose a new convolution kernel, namely the Partial Tree (PT) kernel, to fully exploit dependency trees. We also propose an efficient algorithm for its computation which is futhermore sped-up by applying the selection of tree nodes with non-null kernel. The experiments with Support Vector Machines on the task of semantic role labeling and question classification show that (a) the kernel running time is linear on the average case and (b) the PT kernel improves on the other tree kernels when applied to the appropriate parsing paradigm.}
}
@inproceedings{moschitti2007fast,
	title        = {Fast and Effective Kernels for Relational Learning from Texts},
	author       = {Moschitti, Alessandro and Zanzotto, Fabio Massimo},
	year         = 2007,
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	location     = {Corvalis, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICML '07},
	pages        = {649--656},
	doi          = {10.1145/1273496.1273578},
	isbn         = 9781595937933,
	url          = {https://doi.org/10.1145/1273496.1273578},
	abstract     = {In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.},
	numpages     = 8
}
@inproceedings{moschitti2008kernel,
	title        = {Kernel Methods, Syntax and Semantics for Relational Text Categorization},
	author       = {Moschitti, Alessandro},
	year         = 2008,
	booktitle    = {Proceedings of the 17th ACM Conference on Information and Knowledge Management},
	location     = {Napa Valley, California, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CIKM '08},
	pages        = {253--262},
	doi          = {10.1145/1458082.1458118},
	isbn         = 9781595939913,
	url          = {https://doi.org/10.1145/1458082.1458118},
	abstract     = {Previous work on Natural Language Processing for Information Retrieval has shown the inadequateness of semantic and syntactic structures for both document retrieval and categorization. The main reason is the high reliability and effectiveness of language models, which are sufficient to accurately solve such retrieval tasks. However, when the latter involve the computation of relational semantics between text fragments simple statistical models may result ineffective. In this paper, we show that syntactic and semantic structures can be used to greatly improve complex categorization tasks such as determining if an answer correctly responds to a question. Given the high complexity of representing semantic/syntactic structures in learning algorithms, we applied kernel methods along with Support Vector Machines to better exploit the needed relational information. Our experiments on answer classification on Web and TREC data show that our models greatly improve on bag-of-words.},
	numpages     = 10,
	keywords     = {text categorization, support vector machines, question answering, kernel methods, natural language processing}
}
@inproceedings{moschoglou2017agedb,
	title        = {AgeDB: The First Manually Collected, In-The-Wild Age Database},
	author       = {Moschoglou, Stylianos and Papaioannou, Athanasios and Sagonas, Christos and Deng, Jiankang and Kotsia, Irene and Zafeiriou, Stefanos},
	year         = 2017,
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	pages        = {51--59}
}
@book{mosteller2007inference,
	title        = {Inference and disputed authorship: The Federalist},
	author       = {Mosteller, Frederick and Wallace, David L.},
	year         = 2007,
	publisher    = {Center for the Study of Language and Information},
	place        = {Stanford, Calif}
}
@article{muller2022certified,
	title        = {Certified Training: Small Boxes are All You Need},
	author       = {Mark Niklas M{\"{u}}ller and Franziska Eckert and Marc Fischer and Martin T. Vechev},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2210.04871},
	doi          = {10.48550/arXiv.2210.04871},
	url          = {https://doi.org/10.48550/arXiv.2210.04871},
	eprinttype   = {arXiv},
	eprint       = {2210.04871},
	timestamp    = {Thu, 13 Oct 2022 14:33:15 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2210-04871.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{muller2022prima,
	title        = {PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations},
	author       = {M\"{u}ller, Mark Niklas and Makarchuk, Gleb and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
	year         = 2022,
	month        = {jan},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = {POPL},
	doi          = {10.1145/3498704},
	url          = {https://doi.org/10.1145/3498704},
	issue_date   = {January 2022},
	abstract     = {Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20\%, 30\%, and 34\% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.},
	articleno    = 43,
	numpages     = 33,
	keywords     = {Polyhedra, Robustness, Abstract Interpretation, Convexity}
}
@misc{mw_sentence,
	title        = {Sentence definition},
	author       = {Merriam-Webster},
	journal      = {Dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/sentence}
}
@misc{mw_similar,
	title        = {Similar definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/similar}
}
@misc{mw_similarity,
	title        = {Similarity definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/similarity}
}
@misc{mw_text,
	title        = {Text definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/text}
}
@article{nagwani2015summarizing,
	title        = {Summarizing large text collection using topic modeling and clustering based on MapReduce framework},
	author       = {Nagwani, N. K.},
	year         = 2015,
	month        = {Jun},
	day          = 26,
	journal      = {Journal of Big Data},
	volume       = 2,
	number       = 1,
	pages        = 6,
	doi          = {10.1186/s40537-015-0020-5},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-015-0020-5},
	abstract     = {Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis.}
}
@article{naili2017comparative,
	title        = {Comparative study of word embedding methods in topic segmentation},
	author       = {Marwa Naili and Anja Habacha Chaibi and Henda Hajjami {Ben Ghezala}},
	year         = 2017,
	journal      = {Procedia Computer Science},
	volume       = 112,
	pages        = {340--349},
	doi          = {https://doi.org/10.1016/j.procs.2017.08.009},
	issn         = {1877-0509},
	url          = {https://www.sciencedirect.com/science/article/pii/S1877050917313480},
	note         = {Knowledge-Based and Intelligent Information  \& Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
	keywords     = {Word embedding, LSA, Word2Vec, GloVe, Topic segmentation},
	abstract     = {The vector representations of words are very useful in different natural language processing tasks in order to capture the semantic meaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will be investigated in the field of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth by using different models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on the used language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.}
}
@article{nallapati2016abstractive,
	title        = {Abstractive text summarization using sequence-to-sequence rnns and beyond},
	author       = {Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1602.06023}
}
@inproceedings{nallapati2017summarunner,
	title        = {SummaRuNNer: {A} Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
	author       = {Ramesh Nallapati and Feifei Zhai and Bowen Zhou},
	year         = 2017,
	booktitle    = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA}},
	publisher    = {{AAAI} Press},
	pages        = {3075--3081},
	url          = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636},
	editor       = {Satinder Singh and Shaul Markovitch},
	timestamp    = {Tue, 19 Apr 2022 16:03:28 +0200},
	biburl       = {https://dblp.org/rec/conf/aaai/NallapatiZZ17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{naveed2011searching,
	title        = {Searching microblogs: coping with sparsity and document quality},
	author       = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'e}r{\^o}me and Alhadi, Arifah Che},
	year         = 2011,
	booktitle    = {Proceedings of the 20th ACM international conference on Information and knowledge management},
	pages        = {183--188}
}
@article{navigli2012babelnet,
	title        = {BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network},
	author       = {Roberto Navigli and Simone Paolo Ponzetto},
	year         = 2012,
	journal      = {Artificial Intelligence},
	volume       = 193,
	pages        = {217--250},
	doi          = {https://doi.org/10.1016/j.artint.2012.07.001},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370212000793},
	keywords     = {Knowledge acquisition, Word sense disambiguation, Graph algorithms, Semantic networks},
	abstract     = {We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.}
}
@misc{nelson_2011,
	title        = {Of monsters, men - and topic modeling},
	author       = {Nelson, Robert K.},
	year         = 2011,
	month        = {May},
	journal      = {The New York Times},
	publisher    = {The New York Times},
	url          = {http://opinionator.blogs.nytimes.com/2011/05/29/of-monsters-men-and-topic-modeling/}
}
@inproceedings{netzer2011reading,
	title        = {Reading Digits in Natural Images with Unsupervised Feature Learning},
	author       = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
	year         = 2011,
	booktitle    = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011},
	url          = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf}
}
@inproceedings{nevmyvaka2006reinforcement,
	title        = {Reinforcement Learning for Optimized Trade Execution},
	author       = {Nevmyvaka, Yuriy and Feng, Yi and Kearns, Michael},
	year         = 2006,
	booktitle    = {Proceedings of the 23rd International Conference on Machine Learning},
	location     = {Pittsburgh, Pennsylvania, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICML '06},
	pages        = {673--680},
	doi          = {10.1145/1143844.1143929},
	isbn         = 1595933832,
	url          = {https://doi.org/10.1145/1143844.1143929},
	abstract     = {We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.},
	numpages     = 8
}
@inproceedings{neyshabur2018towards,
	title        = {The role of over-parametrization in generalization of neural networks},
	author       = {Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=BygfghAcYX}
}
@inproceedings{nguyen2016ms,
	title        = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
	author       = {Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng},
	year         = 2016,
	booktitle    = {Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems {(NIPS} 2016), Barcelona, Spain, December 9, 2016},
	publisher    = {CEUR-WS.org},
	series       = {{CEUR} Workshop Proceedings},
	volume       = 1773,
	url          = {http://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
	editor       = {Tarek Richard Besold and Antoine Bordes and Artur S. d'Avila Garcez and Greg Wayne},
	timestamp    = {Wed, 12 Feb 2020 16:44:20 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{nguyen2022master,
	title        = {Master Face Attacks on Face Recognition Systems},
	author       = {Nguyen, Huy H. and Marcel, Sebastien and Yamagishi, Junichi and Echizen, Isao},
	year         = 2022,
	journal      = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
	volume       = 4,
	number       = 3,
	pages        = {398--411},
	doi          = {10.1109/TBIOM.2022.3166206}
}
@article{nielsen2014generalized,
	title        = {Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means},
	author       = {Frank Nielsen},
	year         = 2014,
	journal      = {Pattern Recognition Letters},
	volume       = 42,
	pages        = {25--34},
	doi          = {https://doi.org/10.1016/j.patrec.2014.01.002},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865514000166},
	keywords     = {Affinity coefficient, Divergence, Chernoff information, Bhattacharrya distance, Total variation distance, Quasi-arithmetic means},
	abstract     = {Bayesian classification labels observations based on given prior information, namely class-a priori and class-conditional probabilities. Bayes’ risk is the minimum expected classification cost that is achieved by the Bayes’ test, the optimal decision rule. When no cost incurs for correct classification and unit cost is charged for misclassification, Bayes’ test reduces to the maximum a posteriori decision rule, and Bayes risk simplifies to Bayes’ error, the probability of error. Since calculating this probability of error is often intractable, several techniques have been devised to bound it with closed-form formula, introducing thereby measures of similarity and divergence between distributions like the Bhattacharyya coefficient and its associated Bhattacharyya distance. The Bhattacharyya upper bound can further be tightened using the Chernoff information that relies on the notion of best error exponent. In this paper, we first express Bayes’ risk using the total variation distance on scaled distributions. We then elucidate and extend the Bhattacharyya and the Chernoff upper bound mechanisms using generalized weighted means. We provide as a byproduct novel notions of statistical divergences and affinity coefficients. We illustrate our technique by deriving new upper bounds for the univariate Cauchy and the multivariate t-distributions, and show experimentally that those bounds are not too distant to the computationally intractable Bayes’ error.}
}
@article{nikolenko2017topic,
	title        = {Topic Modelling for Qualitative Studies},
	author       = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
	year         = 2017,
	month        = {feb},
	journal      = {J. Inf. Sci.},
	publisher    = {Sage Publications, Inc.},
	address      = {USA},
	volume       = 43,
	number       = 1,
	pages        = {88--102},
	doi          = {10.1177/0165551515617393},
	issn         = {0165-5515},
	url          = {https://doi.org/10.1177/0165551515617393},
	issue_date   = {2 2017},
	abstract     = {Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation LDA. However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach ISLDA where certain predefined sets of keywords that define the topics researchers are interested in are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.},
	numpages     = 15,
	keywords     = {Latent Dirichlet allocation, LDA extensions, topic quality, topic modelling}
}
@inproceedings{niu2014semi,
	title        = {Semi-supervised Relational Topic Model for Weakly Annotated Image Recognition in Social Media},
	author       = {Niu, Zhenxing and Hua, Gang and Gao, Xinbo and Tian, Qi},
	year         = 2014,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inbook{nivre2006inductive,
	title        = {Inductive Dependency Parsing},
	year         = 2006,
	booktitle    = {Inductive Dependency Parsing},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {87--120},
	doi          = {10.1007/1-4020-4889-0_4},
	isbn         = {978-1-4020-4889-0},
	url          = {https://doi.org/10.1007/1-4020-4889-0_4}
}
@inproceedings{noraset2017definition,
	title        = {Definition modeling: Learning to define word embeddings in natural language},
	author       = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
	year         = 2017,
	booktitle    = {Thirty-First AAAI Conference on Artificial Intelligence}
}
@article{noshad2019learning,
	title        = {Learning to benchmark: Determining best achievable misclassification error from training data},
	author       = {Noshad, Morteza and Xu, Li and Hero, Alfred},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.07192}
}
@inproceedings{olteanu2014crisislex,
	title        = {Crisislex: A lexicon for collecting and filtering microblogged communications in crises},
	author       = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and Vieweg, Sarah},
	year         = 2014,
	booktitle    = {Eighth international AAAI conference on weblogs and social media}
}
@inproceedings{olteanu2015expect,
	title        = {What to Expect When the Unexpected Happens: Social Media Communications Across Crises},
	author       = {Olteanu, Alexandra and Vieweg, Sarah and Castillo, Carlos},
	year         = 2015,
	booktitle    = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work Social Computing},
	location     = {Vancouver, BC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CSCW '15},
	pages        = {994--1009},
	doi          = {10.1145/2675133.2675242},
	isbn         = 9781450329224,
	url          = {https://doi.org/10.1145/2675133.2675242},
	abstract     = {The use of social media to communicate timely information during crisis situations has become a common practice in recent years. In particular, the one-to-many nature of Twitter has created an opportunity for stakeholders to disseminate crisis-relevant messages, and to access vast amounts of information they may not otherwise have. Our goal is to understand what affected populations, response agencies and other stakeholders can expect-and not expect-from these data in various types of disaster situations. Anecdotal evidence suggests that different types of crises elicit different reactions from Twitter users, but we have yet to see whether this is in fact the case. In this paper, we investigate several crises-including natural hazards and human-induced disasters-in a systematic manner and with a consistent methodology. This leads to insights about the prevalence of different information types and sources across a variety of crisis situations.},
	numpages     = 16,
	keywords     = {emergency management, social media}
}
@misc{opennlp,
	title        = {The {Apache} {OpenNLP} library},
	author       = {The Apache Software Foundation},
	year         = 2017,
	note         = {http://opennlp.apache.org},
	howpublished = {Internet}
}
@article{osborne2004power,
	title        = {The power of outliers (and why researchers should always check for them)},
	author       = {Osborne, Jason W and Overbay, Amy},
	year         = 2004,
	journal      = {Practical Assessment, Research, and Evaluation},
	volume       = 9,
	number       = 1,
	pages        = 6
}
@article{osterreicher2003new,
	title        = {A new class of metric divergences on probability spaces and its applicability in statistics},
	author       = {{\"O}sterreicher, Ferdinand and Vajda, Igor},
	year         = 2003,
	month        = {Sep},
	day          = {01},
	journal      = {Annals of the Institute of Statistical Mathematics},
	volume       = 55,
	number       = 3,
	pages        = {639--653},
	doi          = {10.1007/BF02517812},
	issn         = {1572-9052},
	url          = {https://doi.org/10.1007/BF02517812},
	abstract     = {The classIf$\beta$, $\beta$$\epsilon$(0, ∞], off-divergences investigated in this paper is defined in terms of a class of entropies introduced by Arimoto (1971,Information and Control,19, 181--194). It contains the squared Hellinger distance (for $\beta$=1/2), the sumI(Q1{\textbardbl}(Q1+Q2)/2)+I(Q2{\textbardbl}(Q1+Q2)/2) of Kullback-Leibler divergences (for $\beta$=1) and half of the variation distance (for $\beta$=∞) and continuously extends the class of squared perimeter-type distances introduced by {\"O}sterreicher (1996,Kybernetika,32, 389--393) (for $\beta$$\epsilon$ (1, ∞]). It is shown that{\$}{\$}(I{\_}{\{}f{\_}{\backslash}beta  {\}} (Q{\_}1 ,Q{\_}2 ))^{\{}{\backslash}min ({\backslash}beta ,1/2){\}}{\$}{\$}are distances of probability distributionsQ1,Q2 for $\beta$ $\epsilon$ (0, ∞). The applicability of{\$}{\$}I{\_}{\{}f{\_}{\backslash}beta  {\}}{\$}{\$}-divergences in statistics is also considered. In particular, it is shown that the{\$}{\$}I{\_}{\{}f{\_}{\backslash}beta  {\}}{\$}{\$}-projections of appropriate empirical distributions to regular families define distribution estimates which are in the case of an i.i.d. sample of size'n consistent. The order of consistency is investigated as well.}
}
@article{otsuka1936faunal,
	title        = {The faunal character of the Japanese Pleistocene marine Mollusca, as evidence of climate having become colder during the Pleistocene in Japan},
	author       = {Otsuka, Yanosuke},
	year         = 1936,
	journal      = {Biogeograph Soc Japan},
	volume       = 6,
	number       = 16,
	pages        = {165--170}
}
@article{over2007duc,
	title        = {DUC in context},
	author       = {Over, Paul and Dang, Hoa and Harman, Donna},
	year         = 2007,
	journal      = {Information Processing  \& Management},
	publisher    = {Elsevier},
	volume       = 43,
	number       = 6,
	pages        = {1506--1520}
}
@article{palivela2021optimization,
	title        = {Optimization of paraphrase generation and identification using language models in natural language processing},
	author       = {Hemant Palivela},
	year         = 2021,
	journal      = {International Journal of Information Management Data Insights},
	volume       = 1,
	number       = 2,
	pages        = 100025,
	doi          = {https://doi.org/10.1016/j.jjimei.2021.100025},
	issn         = {2667-0968},
	url          = {https://www.sciencedirect.com/science/article/pii/S2667096821000185},
	keywords     = {Paraphrase identification, Paraphrase generation, Natural language generation, Language model, Encoder decoder, Transformer},
	abstract     = {Paraphrase Generation is one of the most important and challenging tasks in the field of Natural Language Generation. The paraphrasing techniques help to identify or to extract/generate phrases/sentences conveying the similar meaning. The paraphrasing task can be bifurcated into two sub-tasks namely, Paraphrase Identification (PI) and Paraphrase Generation (PG). Most of the existing proposed state-of-the-art systems have the potential to solve only one problem at a time. This paper proposes a light-weight unified model that can simultaneously classify whether given pair of sentences are paraphrases of each other and the model can also generate multiple paraphrases given an input sentence. Paraphrase Generation module aims to generate fluent and semantically similar paraphrases and the Paraphrase Identification system aims to classify whether sentences pair are paraphrases of each other or not. The proposed approach uses an amalgamation of data sampling or data variety with a granular fine-tuned Text-To-Text Transfer Transformer (T5) model. This paper proposes a unified approach which aims to solve the problems of Paraphrase Identification and generation by using carefully selected data-points and a fine-tuned T5 model. The highlight of this study is that the same light-weight model trained by keeping the objective of Paraphrase Generation can also be used for solving the Paraphrase Identification task. Hence, the proposed system is light-weight in terms of the model's size along with the data used to train the model which facilitates the quick learning of the model without having to compromise with the results. The proposed system is then evaluated against the popular evaluation metrics like BLEU (BiLingual Evaluation Understudy):, ROUGE (Recall-Oriented Understudy for Gisting Evaluation), METEOR, WER (Word Error Rate), and GLEU (Google-BLEU) for Paraphrase Generation and classification metrics like accuracy, precision, recall and F1-score for Paraphrase Identification system. The proposed model achieves state-of-the-art results on both the tasks of Paraphrase Identification and paraphrase Generation.}
}
@article{pang2019robust,
	title        = {Robust heterogeneous discriminative analysis for face recognition with single sample per person},
	author       = {Meng Pang and Yiu-ming Cheung and Binghui Wang and Risheng Liu},
	year         = 2019,
	journal      = {Pattern Recognition},
	volume       = 89,
	pages        = {91--107},
	doi          = {https://doi.org/10.1016/j.patcog.2019.01.005},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/S0031320319300111},
	keywords     = {Face recognition, Single sample per person, Heterogeneous representation, Fisher-like criterion, Joint majority voting},
	abstract     = {Single sample per person face recognition is one of the most challenging problems in face recognition (FR), where only single sample per person (SSPP) is enrolled in the gallery set for training. Although the existing patch-based methods have achieved great success in FR with SSPP, they still have limitations in feature extraction and identification stages when handling complex facial variations. In this work, we propose a new patch-based method called Robust Heterogeneous Discriminative Analysis (RHDA), for FR with SSPP. To enhance the robustness against complex facial variations, we first present a new graph-based Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing the similarities between neighboring patches from the different persons. Then, we introduce two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion strategy to combine the recognition outputs of above two distance metrics via a joint majority voting for identification. Experimental results on various benchmark datasets demonstrate the effectiveness of the proposed method.}
}
@inproceedings{pang2022robustness,
	title        = {Robustness and Accuracy Could Be Reconcilable by (Proper) Definition},
	author       = {Tianyu Pang and Min Lin and Xiao Yang and Jun Zhu and Shuicheng Yan},
	year         = 2022,
	booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {17258--17277},
	url          = {https://proceedings.mlr.press/v162/pang22a.html},
	editor       = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan Sabato},
	timestamp    = {Tue, 12 Jul 2022 17:36:52 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/PangLYZY22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{papernot2016limitations,
	title        = {The limitations of deep learning in adversarial settings},
	author       = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	year         = 2016,
	booktitle    = {2016 IEEE European Symposium on Security and Privacy (EuroS\&P)},
	pages        = {372--387},
	organization = {IEEE}
}
@article{papernot2016transferability,
	title        = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples},
	author       = {Nicolas Papernot and Patrick D. McDaniel and Ian J. Goodfellow},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1605.07277},
	url          = {http://arxiv.org/abs/1605.07277},
	eprinttype   = {arXiv},
	eprint       = {1605.07277},
	timestamp    = {Mon, 13 Aug 2018 16:48:28 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/PapernotMG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{papernot2017practical,
	title        = {Practical black-box attacks against machine learning},
	author       = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
	year         = 2017,
	booktitle    = {Proceedings of the 2017 ACM on Asia conference on computer and communications security},
	pages        = {506--519}
}
@article{paranyushkin2011identifying,
	title        = {Identifying the pathways for meaning circulation using text network analysis},
	author       = {Paranyushkin, Dmitry},
	year         = 2011,
	journal      = {Nodus Labs},
	volume       = 26,
	pages        = {1--26}
}
@article{parthasarathi2020evaluate,
	title        = {How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for Token-level Evaluation Metrics},
	author       = {Prasanna Parthasarathi and Joelle Pineau and Sarath Chandar},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2008.10427},
	url          = {https://arxiv.org/abs/2008.10427},
	eprinttype   = {arXiv},
	eprint       = {2008.10427},
	timestamp    = {Fri, 28 Aug 2020 12:11:44 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2008-10427.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{paszke2019pytorch,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	pages        = {8024--8035},
	url          = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{paulus2017deep,
	title        = {A Deep Reinforced Model for Abstractive Summarization},
	author       = {Romain Paulus and Caiming Xiong and Richard Socher},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HkAClQgA-},
	timestamp    = {Thu, 25 Jul 2019 14:25:58 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/PaulusXS18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{pawar2019challenging,
	title        = {Challenging the Boundaries of Unsupervised Learning for Semantic Similarity},
	author       = {Pawar, Atish and Mago, Vijay},
	year         = 2019,
	journal      = {IEEE Access},
	volume       = 7,
	pages        = {16291--16308},
	doi          = {10.1109/ACCESS.2019.2891692}
}
@article{pedersen2007measures,
	title        = {Measures of semantic similarity and relatedness in the biomedical domain},
	author       = {Ted Pedersen and Serguei V.S. Pakhomov and Siddharth Patwardhan and Christopher G. Chute},
	year         = 2007,
	journal      = {Journal of Biomedical Informatics},
	volume       = 40,
	number       = 3,
	pages        = {288--299},
	doi          = {https://doi.org/10.1016/j.jbi.2006.06.004},
	issn         = {1532-0464},
	url          = {https://www.sciencedirect.com/science/article/pii/S1532046406000645},
	keywords     = {Semantic similarity, Path based measures, Information content, Context vectors, SNOMED-CT},
	abstract     = {Measures of semantic similarity between concepts are widely used in Natural Language Processing. In this article, we show how six existing domain-independent measures can be adapted to the biomedical domain. These measures were originally based on WordNet, an English lexical database of concepts and relations. In this research, we adapt these measures to the SNOMED-CT® ontology of medical concepts. The measures include two path-based measures, and three measures that augment path-based measures with information content statistics from corpora. We also derive a context vector measure based on medical corpora that can be used as a measure of semantic relatedness. These six measures are evaluated against a newly created test bed of 30 medical concept pairs scored by three physicians and nine medical coders. We find that the medical coders and physicians differ in their ratings, and that the context vector measure correlates most closely with the physicians, while the path-based measures and one of the information content measures correlates most closely with the medical coders. We conclude that there is a role both for more flexible measures of relatedness based on information derived from corpora, as well as for measures that rely on existing ontological structures.}
}
@inproceedings{peterson2019human,
	title        = {Human Uncertainty Makes Classification More Robust},
	author       = {Peterson, Joshua C. and Battleday, Ruairidh M. and Griffiths, Thomas L. and Russakovsky, Olga},
	year         = 2019,
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}
}
@misc{phillips2013distances,
	title        = {L7-Distances},
	author       = {Phillips, Jeff M},
	year         = 2013,
	journal      = {cs5955},
	publisher    = {University of Utah},
	url          = {https://www.cs.utah.edu/~jeffp/teaching/cs5955/L7-Distances.pdf}
}
@inproceedings{pierazzi2020intriguing,
	title        = {Intriguing Properties of Adversarial ML Attacks in the Problem Space},
	author       = {Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
	year         = 2020,
	booktitle    = {2020 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {1332--1349},
	doi          = {10.1109/SP40000.2020.00073}
}
@inproceedings{plummer2015flickr30k,
	title        = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
	author       = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
	year         = 2015,
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{pomponi2022pixle,
	title        = {Pixle: a fast and effective black-box attack based on rearranging pixels},
	author       = {Pomponi, Jary and Scardapane, Simone and Uncini, Aurelio},
	year         = 2022,
	booktitle    = {2022 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1--7},
	doi          = {10.1109/IJCNN55064.2022.9892966}
}
@article{postaire1982unsupervised,
	title        = {An unsupervised Bayes classifier for normal patterns based on marginal densities analysis},
	author       = {J.-G. Postaire},
	year         = 1982,
	journal      = {Pattern Recognition},
	volume       = 15,
	number       = 2,
	pages        = {103--111},
	doi          = {https://doi.org/10.1016/0031-3203(82)90005-X},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/003132038290005X},
	keywords     = {Unsupervised classification, Normal mixture identification, Marginal densities, Minimum error-rate classification},
	abstract     = {In this paper, an approach to unsupervised pattern classification is discussed. The classification scheme is based on an approximation of the probability densities of each class under the assumption that the input patterns are of a normal mixture. The description of the marginal densities in terms of convexity allows one to determine, from a totally unlabelled set of samples, the number of components and, for each of them, approximate values of the mean vector, the covariance matrix and the a priori probability. Discriminant functions can then be constructed. Computer simulations show that the procedure yields decision rules whose performance remains close to the optimum Bayes minimum error-rate, while involving only a small amount of computation.}
}
@article{pymoo,
	title        = {pymoo: Multi-Objective Optimization in Python},
	author       = {J. {Blank} and K. {Deb}},
	year         = 2020,
	journal      = {IEEE Access},
	volume       = 8,
	number       = {},
	pages        = {89497--89509}
}
@misc{pytorchexample,
	title        = {Basic MNIST Example},
	author       = {Pytorch},
	journal      = {GitHub},
	url          = {https://github.com/pytorch/examples/tree/main/mnist}
}
@article{qu2018computing,
	title        = {Computing semantic similarity based on novel models of semantic representation using Wikipedia},
	author       = {Rong Qu and Yongyi Fang and Wen Bai and Yuncheng Jiang},
	year         = 2018,
	journal      = {Information Processing  \& Management},
	volume       = 54,
	number       = 6,
	pages        = {1002--1021},
	doi          = {https://doi.org/10.1016/j.ipm.2018.07.002},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457317309226},
	keywords     = {Semantic similarity, Concept similarity, Information content, Feature-based methods, Wikipedia},
	abstract     = {Computing Semantic Similarity (SS) between concepts is one of the most critical issues in many domains such as Natural Language Processing and Artificial Intelligence. Over the years, several SS measurement methods have been proposed by exploiting different knowledge resources. Wikipedia provides a large domain-independent encyclopedic repository and a semantic network for computing SS between concepts. Traditional feature-based measures rely on linear combinations of different properties with two main limitations, the insufficient information and the loss of semantic information. In this paper, we propose several hybrid SS measurement approaches by using the Information Content (IC) and features of concepts, which avoid the limitations introduced above. Considering integrating discrete properties into one component, we present two models of semantic representation, called CORM and CARM. Then, we compute SS based on these models and take the IC of categories as a supplement of SS measurement. The evaluation, based on several widely used benchmarks and a benchmark developed by ourselves, sustains the intuitions with respect to human judgments. In summary, our approaches are more efficient in determining SS between concepts and have a better human correlation than previous methods such as Word2Vec and NASARI.}
}
@article{radford2019language,
	title        = {Language models are unsupervised multitask learners},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = 2019,
	journal      = {OpenAI blog},
	volume       = 1,
	number       = 8,
	pages        = 9
}
@article{raffel2019exploring,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1910.10683},
	url          = {http://arxiv.org/abs/1910.10683},
	eprinttype   = {arXiv},
	eprint       = {1910.10683},
	timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{raghavan2007near,
	title        = {Near linear time algorithm to detect community structures in large-scale networks},
	author       = {Raghavan, Usha Nandini and Albert, R\'eka and Kumara, Soundar},
	year         = 2007,
	month        = {Sep},
	journal      = {Phys. Rev. E},
	publisher    = {American Physical Society},
	volume       = 76,
	pages        = {036106},
	doi          = {10.1103/PhysRevE.76.036106},
	url          = {https://link.aps.org/doi/10.1103/PhysRevE.76.036106},
	issue        = 3,
	numpages     = 11
}
@inproceedings{raghunathan2018certified,
	title        = {Certified Defenses against Adversarial Examples},
	author       = {Aditi Raghunathan and Jacob Steinhardt and Percy Liang},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1801.09344},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Bys4ob-Rb},
	timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/RaghunathanSL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{raghunathan2019adversarial,
	title        = {Adversarial Training Can Hurt Generalization},
	author       = {Aditi Raghunathan and Sang Michael Xie and Fanny Yang and John C. Duchi and Percy Liang},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1906.06032},
	url          = {http://arxiv.org/abs/1906.06032},
	eprinttype   = {arXiv},
	eprint       = {1906.06032},
	timestamp    = {Sat, 23 Jan 2021 01:14:32 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1906-06032.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{ragoza2017protein,
	title        = {Protein-Ligand Scoring with Convolutional Neural Networks},
	author       = {Ragoza, Matthew and Hochuli, Joshua and Idrobo, Elisa and Sunseri, Jocelyn and Koes, David Ryan},
	year         = 2017,
	journal      = {Journal of Chemical Information and Modeling},
	volume       = 57,
	number       = 4,
	pages        = {942--957},
	doi          = {10.1021/acs.jcim.6b00740},
	url          = {https://doi.org/10.1021/acs.jcim.6b00740},
	note         = {PMID: 28368587},
	eprint       = {https://doi.org/10.1021/acs.jcim.6b00740}
}
@inbook{rajaraman2011mining,
	title        = {Finding Similar Items},
	author       = {Leskovec, Jurij and Rajaraman, Anand and Ullman, Jeffrey David},
	year         = 2022,
	booktitle    = {Mining of massive datasets},
	publisher    = {Cambridge University Press},
	pages        = {76--78},
	url          = {http://infolab.stanford.edu/~ullman/mmds/ch3.pdf},
	place        = {Cambridge etc.}
}
@article{ramachandra2017presentation,
	title        = {Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey},
	author       = {Ramachandra, Raghavendra and Busch, Christoph},
	year         = 2017,
	month        = {mar},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 50,
	number       = 1,
	doi          = {10.1145/3038924},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3038924},
	issue_date   = {January 2018},
	abstract     = {The vulnerability of face recognition systems to presentation attacks (also known as direct attacks or spoof attacks) has received a great deal of interest from the biometric community. The rapid evolution of face recognition systems into real-time applications has raised new concerns about their ability to resist presentation attacks, particularly in unattended application scenarios such as automated border control. The goal of a presentation attack is to subvert the face recognition system by presenting a facial biometric artifact. Popular face biometric artifacts include a printed photo, the electronic display of a facial photo, replaying video using an electronic display, and 3D face masks. These have demonstrated a high security risk for state-of-the-art face recognition systems. However, several presentation attack detection (PAD) algorithms (also known as countermeasures or antispoofing methods) have been proposed that can automatically detect and mitigate such targeted attacks. The goal of this survey is to present a systematic overview of the existing work on face presentation attack detection that has been carried out. This paper describes the various aspects of face presentation attacks, including different types of face artifacts, state-of-the-art PAD algorithms and an overview of the respective research labs working in this domain, vulnerability assessments and performance evaluation metrics, the outcomes of competitions, the availability of public databases for benchmarking new PAD algorithms in a reproducible manner, and finally a summary of the relevant international standardization in this field. Furthermore, we discuss the open challenges and future work that need to be addressed in this evolving field of biometrics.},
	articleno    = 8,
	numpages     = 37,
	keywords     = {security, countermeasure, face recognition, antispoofing, attacks, Biometrics}
}
@inproceedings{ramachandra2019custom,
	title        = {Custom silicone Face Masks: Vulnerability of Commercial Face Recognition Systems \& Presentation Attack Detection},
	author       = {Ramachandra, Raghavendra and Venkatesh, Sushma and Raja, Kiran B. and Bhattacharjee, Sushil and Wasnik, Pankaj and Marcel, Sebastien and Busch, Christoph},
	year         = 2019,
	booktitle    = {2019 7th International Workshop on Biometrics and Forensics (IWBF)},
	volume       = {},
	number       = {},
	pages        = {1--6},
	doi          = {10.1109/IWBF.2019.8739236}
}
@article{ramesh2021automated,
	title        = {An automated essay scoring systems: a systematic literature review},
	author       = {Dadi Ramesh and Suresh Kumar Sanampudi},
	year         = 2022,
	journal      = {Artif. Intell. Rev.},
	volume       = 55,
	number       = 3,
	pages        = {2495--2527},
	doi          = {10.1007/s10462-021-10068-2},
	url          = {https://doi.org/10.1007/s10462-021-10068-2},
	timestamp    = {Fri, 01 Apr 2022 11:23:48 +0200},
	biburl       = {https://dblp.org/rec/journals/air/RameshS22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ramos2003using,
	title        = {Using tf-idf to determine word relevance in document queries},
	author       = {Ramos, Juan and others},
	year         = 2003,
	booktitle    = {Proceedings of the first instructional conference on machine learning},
	volume       = 242,
	number       = 1,
	pages        = {29--48},
	organization = {Citeseer}
}
@article{rasooli-tetrault-2015,
	title        = {Yara Parser: {A} Fast and Accurate Dependency Parser},
	author       = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
	year         = 2015,
	journal      = {Computing Research Repository},
	volume       = {arXiv:1503.06733},
	url          = {http://arxiv.org/abs/1503.06733},
	note         = {version 2}
}
@article{rauber2020foolbox,
	title        = {Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX},
	author       = {Jonas Rauber and Roland Zimmermann and Matthias Bethge and Wieland Brendel},
	year         = 2020,
	journal      = {Journal of Open Source Software},
	publisher    = {The Open Journal},
	volume       = 5,
	number       = 53,
	pages        = 2607,
	doi          = {10.21105/joss.02607},
	url          = {https://doi.org/10.21105/joss.02607}
}
@article{ravaut2022summaReranker,
	title        = {SummaReranker: {A} Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization},
	author       = {Mathieu Ravaut and Shafiq Joty and Nancy F. Chen},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2203.06569},
	doi          = {10.48550/arXiv.2203.06569},
	url          = {https://doi.org/10.48550/arXiv.2203.06569},
	eprinttype   = {arXiv},
	eprint       = {2203.06569},
	timestamp    = {Wed, 16 Mar 2022 16:41:29 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2203-06569.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{rawte2020comparative,
	title        = {A Comparative Analysis of Temporal Long Text Similarity: Application to Financial Documents},
	author       = {Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed J.},
	year         = 2021,
	booktitle    = {Mining Data for Financial Applications},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {77--91},
	isbn         = {978-3-030-66981-2},
	editor       = {Bitetta, Valerio and Bordino, Ilaria and Ferretti, Andrea and Gullo, Francesco and Ponti, Giovanni and Severini, Lorenzo},
	abstract     = {Temporal text documents exist in many real-world domains. These may span over long periods of time during which there tend to be many variations in the text. In particular, variations or the similarities in a pair of documents over two consecutive years could be meaningful. Most of the textual analysis work like text classification focuses on the entire text snippet as a data instance. It is therefore important to study such similarities besides the entire text document. In Natural Language Processing (NLP), the task of textual similarity is important for search and query retrieval. This task is also better known as Semantic Textual Similarity (STS) that aims to capture the semantics of two texts while comparing them. Also, state-of-the-art methods predominantly target short texts. Thus, measuring the semantic similarity between a pair of long texts is still a challenge. In this paper, we compare different text matching methods for the documents over two consecutive years. We focus on their similarities for our comparative analysis and evaluation of financial documents, namely public 10-K filings to the SEC (Securities and Exchange Commission). We further perform textual regression analysis on six quantitative bank variables including Return on Assets (ROA), Earnings per Share (EPS), Tobin's Q Ratio, Tier 1 Capital Ratio, Leverage Ratio, and Z-score, and show that textual features can be effective in predicting these variables.}
}
@article{renggli2020ease,
	title        = {Ease.Ml/Snoopy in Action: Towards Automatic Feasibility Analysis for Machine Learning Application Development},
	author       = {Renggli, Cedric and Rimanic, Luka and Kolar, Luka and Wu, Wentao and Zhang, Ce},
	year         = 2020,
	month        = {aug},
	journal      = {Proc. VLDB Endow.},
	publisher    = {VLDB Endowment},
	volume       = 13,
	number       = 12,
	pages        = {2837–2840},
	doi          = {10.14778/3415478.3415488},
	issn         = {2150-8097},
	url          = {https://doi.org/10.14778/3415478.3415488},
	issue_date   = {August 2020},
	abstract     = {We demonstrate ease.ml/snoopy, a data analytics system that performs feasibility analysis for machine learning (ML) applications before they are developed. Given a performance target of an ML application (e.g., accuracy above 0.95), ease.ml/snoopy provides a decisive answer to ML developers regarding whether the target is achievable or not. We formulate the feasibility analysis problem as an instance of Bayes error estimation. That is, for a data (distribution) on which the ML application should be performed, ease.ml/snoopy provides an estimate of the Bayes error - the minimum error rate that can be achieved by any classifier. It is well-known that estimating the Bayes error is a notoriously hard task. In ease.ml/snoopy we explore and employ estimators based on the combination of (1) nearest neighbor (NN) classifiers and (2) pre-trained feature transformations. To the best of our knowledge, this is the first work on Bayes error estimation that combines (1) and (2). In today's cost-driven business world, feasibility of an ML project is an ideal piece of information for ML application developers - ease.ml/snoopy plays the role of a reliable "consultant."},
	numpages     = 4
}
@inproceedings{renggli2021evaluating,
	title        = {Evaluating Bayes Error Estimators on Real-World Datasets with FeeBee},
	author       = {C{\'{e}}dric Renggli and Luka Rimanic and Nora Hollenstein and Ce Zhang},
	year         = 2021,
	booktitle    = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual},
	url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/045117b0e0a11a242b9765e79cbf113f-Abstract-round2.html},
	editor       = {Joaquin Vanschoren and Sai{-}Kit Yeung},
	timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/RenggliRH021.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{resnik1995using,
	title        = {Using Information Content to Evaluate Semantic Similarity in a Taxonomy},
	author       = {Philip Resnik},
	year         = 1995,
	booktitle    = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, {IJCAI} 95, Montr{\'{e}}al Qu{\'{e}}bec, Canada, August 20-25 1995, 2 Volumes},
	publisher    = {Morgan Kaufmann},
	pages        = {448--453},
	url          = {http://ijcai.org/Proceedings/95-1/Papers/059.pdf},
	timestamp    = {Tue, 20 Aug 2019 16:17:30 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/Resnik95.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{rice2021robustness,
	title        = {Robustness between the worst and average case},
	author       = {Rice, Leslie and Bair, Anna and Zhang, Huan and Kolter, J Zico},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {27840--27851}
}
@phdthesis{rich2017collaborative,
	title        = {Collaborative scientific publishing: a new research ecosystem},
	author       = {Rich, Travis Travis Sebastian},
	year         = 2017,
	school       = {Massachusetts Institute of Technology}
}
@book{ripley1996pattern,
	title        = {Pattern Recognition and Neural Networks},
	author       = {Ripley, Brian D.},
	year         = 1996,
	publisher    = {Cambridge University Press},
	doi          = {10.1017/CBO9780511812651},
	place        = {Cambridge}
}
@inproceedings{ritter2012open,
	title        = {Open Domain Event Extraction from Twitter},
	author       = {Ritter, Alan and Mausam and Etzioni, Oren and Clark, Sam},
	year         = 2012,
	booktitle    = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '12},
	pages        = {1104--1112},
	doi          = {10.1145/2339530.2339704},
	isbn         = 9781450314626,
	url          = {https://doi.org/10.1145/2339530.2339704},
	abstract     = {Tweets are the most up-to-date and inclusive stream of in- formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter's unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal-- the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately extracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and classifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14\% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our system can be viewed at http://statuscalendar.com; Our NLP tools are available at http://github.com/aritter/ twitter_nlp.},
	numpages     = 9,
	keywords     = {social media, information extraction}
}
@article{roberts2014structural,
	title        = {Structural Topic Models for Open-Ended Survey Responses},
	author       = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	year         = 2014,
	journal      = {American Journal of Political Science},
	volume       = 58,
	number       = 4,
	pages        = {1064--1082},
	doi          = {https://doi.org/10.1111/ajps.12103},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12103},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12103},
	abstract     = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.}
}
@article{roberts2016model,
	title        = {A Model of Text for Experimentation in the Social Sciences},
	author       = {Margaret E. Roberts and Brandon M. Stewart and Edoardo M. Airoldi},
	year         = 2016,
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = 111,
	number       = 515,
	pages        = {988--1003},
	doi          = {10.1080/01621459.2016.1141684},
	url          = {https://doi.org/10.1080/01621459.2016.1141684},
	eprint       = {https://doi.org/10.1080/01621459.2016.1141684}
}
@article{robertson1976relevance,
	title        = {Relevance weighting of search terms},
	author       = {Robertson, Stephen E and Jones, K Sparck},
	year         = 1976,
	journal      = {Journal of the American Society for Information science},
	publisher    = {Wiley Online Library},
	volume       = 27,
	number       = 3,
	pages        = {129--146}
}
@inproceedings{robey2022probabilistically,
	title        = {Probabilistically Robust Learning: Balancing Average and Worst-case Performance},
	author       = {Robey, Alexander and Chamon, Luiz and Pappas, George J. and Hassani, Hamed},
	year         = 2022,
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {18667--18686},
	url          = {https://proceedings.mlr.press/v162/robey22a.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/robey22a/robey22a.pdf},
	abstract     = {Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness. Our code is available at: https://github.com/arobey1/advbench.}
}
@article{rodriguez2003determining,
	title        = {Determining semantic similarity among entity classes from different ontologies},
	author       = {Rodriguez, M.A. and Egenhofer, M.J.},
	year         = 2003,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 15,
	number       = 2,
	pages        = {442--456},
	doi          = {10.1109/TKDE.2003.1185844}
}
@article{rosvall2008maps,
	title        = {Maps of random walks on complex networks reveal community structure},
	author       = {Martin Rosvall  and Carl T. Bergstrom},
	year         = 2008,
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = 105,
	number       = 4,
	pages        = {1118--1123},
	doi          = {10.1073/pnas.0706851105},
	url          = {https://www.pnas.org/doi/abs/10.1073/pnas.0706851105},
	eprint       = {https://www.pnas.org/doi/pdf/10.1073/pnas.0706851105},
	abstract     = {To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of \&gt;6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network—including physics, chemistry, molecular biology, and medicine—information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences.}
}
@incollection{rummel1976understanding,
	title        = {The Vector Approach},
	author       = {Rummel, Rudolph J},
	year         = 1976,
	journal      = {Honolulu: Department of Political Science, University of Hawaii},
	booktitle    = {Understanding correlation},
	url          = {http://www.hawaii.edu/powerkills/UC.HTM#C5},
	chapter      = 5
}
@article{sai2019re,
	title        = {Re-Evaluating ADEM: A Deeper Look at Scoring Dialogue Responses},
	author       = {Sai, Ananya B. and Gupta, Mithun Das and Khapra, Mitesh M. and Srinivasan, Mukundhan},
	year         = 2019,
	month        = {Jul.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {6220--6227},
	doi          = {10.1609/aaai.v33i01.33016220},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4581},
	abstractnote = {&lt;p&gt;Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. ADEM (Lowe et al. 2017) formulated the automatic evaluation of dialogue systems as a learning problem and showed that such a model was able to predict responses which correlate significantly with human judgements, both at utterance and system level. Their system was shown to have beaten word-overlap metrics such as BLEU with large margins. We start with the question of whether an adversary can game the ADEM model. We design a battery of targeted attacks at the neural network based ADEM evaluation system and show that automatic evaluation of dialogue systems still has a long way to go. ADEM can get confused with a variation as simple as reversing the word order in the text! We report experiments on several such adversarial scenarios that draw out counterintuitive scores on the dialogue responses. We take a systematic look at the scoring function proposed by ADEM and connect it to linear system theory to predict the shortcomings evident in the system. We also devise an attack that can fool such a system to rate a response generation system as favorable. Finally, we allude to future research directions of using the adversarial attacks to design a truly automated dialogue evaluation system.&lt;/p&gt;}
}
@article{sai2022survey,
	title        = {A Survey of Evaluation Metrics Used for NLG Systems},
	author       = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
	year         = 2022,
	month        = {jan},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 55,
	number       = 2,
	doi          = {10.1145/3485766},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3485766},
	issue_date   = {March 2023},
	abstract     = {In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.},
	articleno    = 26,
	numpages     = 39,
	keywords     = {correlations, question generation, image captioning, data-to-text generation, Automatic evaluation metrics, question answering, abstractive summarization}
}
@article{salazar2023proxy,
	title        = {A proxy learning curve for the Bayes classifier},
	author       = {Addisson Salazar and Luis Vergara and Enrique Vidal},
	year         = 2023,
	journal      = {Pattern Recognition},
	volume       = 136,
	pages        = 109240,
	doi          = {https://doi.org/10.1016/j.patcog.2022.109240},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/S0031320322007191},
	keywords     = {Classification, Parameter learning, Sample size, Training set size, Probability of error},
	abstract     = {In this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.}
}
@inproceedings{salman2019convex,
	title        = {A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks},
	author       = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@article{salman2019provably,
	title        = {Provably robust deep learning via adversarially trained smoothed classifiers},
	author       = {Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@article{salton1965smart,
	title        = {The SMART automatic document retrieval systems—an illustration},
	author       = {Salton, Gerard and Lesk, Michael E},
	year         = 1965,
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 8,
	number       = 6,
	pages        = {391--398}
}
@article{salton1983modern,
	title        = {Modern information retrieval},
	author       = {Salton, Gerard},
	year         = 1983,
	journal      = {(No Title)},
	publisher    = {mcgraw-hill}
}
@article{salton1988term,
	title        = {Term-weighting approaches in automatic text retrieval},
	author       = {Salton, Gerard and Buckley, Christopher},
	year         = 1988,
	journal      = {Information processing \& management},
	publisher    = {Elsevier},
	volume       = 24,
	number       = 5,
	pages        = {513--523}
}
@article{sanchez2011ontology,
	title        = {Ontology-based information content computation},
	author       = {David Sánchez and Montserrat Batet and David Isern},
	year         = 2011,
	journal      = {Knowledge-Based Systems},
	volume       = 24,
	number       = 2,
	pages        = {297--303},
	doi          = {https://doi.org/10.1016/j.knosys.2010.10.001},
	issn         = {0950-7051},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950705110001619},
	keywords     = {Information content, Semantic similarity, Ontologies, Taxonomic knowledge, WordNet},
	abstract     = {The information content (IC) of a concept provides an estimation of its degree of generality/concreteness, a dimension which enables a better understanding of concept's semantics. As a result, IC has been successfully applied to the automatic assessment of the semantic similarity between concepts. In the past, IC has been estimated as the probability of appearance of concepts in corpora. However, the applicability and scalability of this method are hampered due to corpora dependency and data sparseness. More recently, some authors proposed IC-based measures using taxonomical features extracted from an ontology for a particular concept, obtaining promising results. In this paper, we analyse these ontology-based approaches for IC computation and propose several improvements aimed to better capture the semantic evidence modelled in the ontology for the particular concept. Our approach has been evaluated and compared with related works (both corpora and ontology-based ones) when applied to the task of semantic similarity estimation. Results obtained for a widely used benchmark show that our method enables similarity estimations which are better correlated with human judgements than related works.}
}
@article{sanchez2012ontology,
	title        = {Ontology-based semantic similarity: A new feature-based approach},
	author       = {David Sánchez and Montserrat Batet and David Isern and Aida Valls},
	year         = 2012,
	journal      = {Expert Systems with Applications},
	volume       = 39,
	number       = 9,
	pages        = {7718--7728},
	doi          = {https://doi.org/10.1016/j.eswa.2012.01.082},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417412000954},
	keywords     = {Semantic similarity, Semantic relatedness, Ontologies, Feature-based similarity, WordNet},
	abstract     = {Estimation of the semantic likeness between words is of great importance in many applications dealing with textual data such as natural language processing, knowledge acquisition and information retrieval. Semantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent years, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an structured knowledge representation. Thanks to the possibilities that ontologies enable regarding semantic interpretation of terms many ontology-based similarity measures have been developed. According to the principle in which those measures base the similarity assessment and the way in which ontologies are exploited or complemented with other sources several families of measures can be identified. In this paper, we survey and classify most of the ontology-based approaches developed in order to evaluate their advantages and limitations and compare their expected performance both from theoretical and practical points of view. We also present a new ontology-based measure relying on the exploitation of taxonomical features. The evaluation and comparison of our approach's results against those reported by related works under a common framework suggest that our measure provides a high accuracy without some of the limitations observed in other works.}
}
@article{sanchez2013semantic,
	title        = {A semantic similarity method based on information content exploiting multiple ontologies},
	author       = {David Sánchez and Montserrat Batet},
	year         = 2013,
	journal      = {Expert Systems with Applications},
	volume       = 40,
	number       = 4,
	pages        = {1393--1399},
	doi          = {https://doi.org/10.1016/j.eswa.2012.08.049},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S095741741201010X},
	keywords     = {Information content, Semantic similarity, Ontologies, MeSH, SNOMED CT},
	abstract     = {The quantification of the semantic similarity between terms is an important research area that configures a valuable tool for text understanding. Among the different paradigms used by related works to compute semantic similarity, in recent years, information theoretic approaches have shown promising results by computing the information content (IC) of concepts from the knowledge provided by ontologies. These approaches, however, are hampered by the coverage offered by the single input ontology. In this paper, we propose extending IC-based similarity measures by considering multiple ontologies in an integrated way. Several strategies are proposed according to which ontology the evaluated terms belong. Our proposal has been evaluated by means of a widely used benchmark of medical terms and MeSH and SNOMED CT as ontologies. Results show an improvement in the similarity assessment accuracy when multiple ontologies are considered.}
}
@misc{sandhaus2008new,
	title        = {The New York Times annotated corpus},
	author       = {Sandhaus, Evan},
	year         = 2008,
	journal      = {Linguistic Data Consortium},
	publisher    = {LDC2008T19.  Web Download. Philadelphia: Linguistic Data Consortium},
	url          = {https://catalog.ldc.upenn.edu/LDC2008T19}
}
@article{sanh2019distilbert,
	title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper and lighter},
	author       = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1910.01108},
	url          = {http://arxiv.org/abs/1910.01108},
	eprinttype   = {arXiv},
	eprint       = {1910.01108},
	timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sanyal2020benign,
	title        = {How Benign is Benign Overfitting ?},
	author       = {Amartya Sanyal and Puneet K. Dokania and Varun Kanade and Philip H. S. Torr},
	year         = 2021,
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=g-wu9TMPODo},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/SanyalDKT21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sarwar2002incremental,
	title        = {Incremental singular value decomposition algorithms for highly scalable recommender systems},
	author       = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
	year         = 2002,
	booktitle    = {Fifth international conference on computer and information science},
	volume       = 1,
	number       = {012002},
	pages        = {27--8},
	organization = {Citeseer}
}
@article{sawant2019age,
	title        = {Age invariant face recognition: a survey on facial aging databases, techniques and effect of aging},
	author       = {Sawant, Manisha M. and Bhurchandi, Kishor M.},
	year         = 2019,
	month        = {Aug},
	day          = {01},
	journal      = {Artificial Intelligence Review},
	volume       = 52,
	number       = 2,
	pages        = {981--1008},
	doi          = {10.1007/s10462-018-9661-z},
	issn         = {1573-7462},
	url          = {https://doi.org/10.1007/s10462-018-9661-z},
	abstract     = {Age invariant face recognition (AIFR) is highly required in many applications like law enforcement, national databases and security. Recognizing faces across aging is difficult even for humans; hence, it presents a unique challenge for computer vision systems. Face recognition under various intra-person variations such as expression, pose and occlusion has been an intensively researched field. However, age invariant face recognition still faces many challenges due to age related biological transformations in presence of the other appearance variations. In this paper, we present a comprehensive review of literature on cross age face recognition. Starting with the biological effects of aging, this paper presents a survey of techniques, effects of aging on performance analysis and facial aging databases. The published AIFR techniques are reviewed and categorized into generative, discriminative and deep learning methods on the basis of face representation and learning techniques. Analysis of the effect of aging on the performance of age-invariant face recognition system is an important dimension. Hence, such analysis is reviewed and summarized. In addition, important facial aging databases are briefly described in terms of the number of subjects and images per subject along with their age ranges. We finally present discussions on the findings, conclusions and future directions for new researchers.}
}
@article{schapire1990strength,
	title        = {The strength of weak learnability},
	author       = {Schapire, Robert E.},
	year         = 1990,
	month        = {Jun},
	day          = {01},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = 5,
	number       = 2,
	pages        = {197--227},
	doi          = {10.1007/BF00116037},
	issn         = {1573-0565},
	url          = {https://doi.org/10.1007/BF00116037},
	abstract     = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.}
}
@inproceedings{schroff2015facenet,
	title        = {FaceNet: A unified embedding for face recognition and clustering},
	author       = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year         = 2015,
	booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {815--823},
	doi          = {10.1109/CVPR.2015.7298682}
}
@article{schwinn2023exploring,
	title        = {Exploring misclassifications of robust neural networks to enhance adversarial attacks},
	author       = {Schwinn, Leo and Raab, Ren{\'e} and Nguyen, An and Zanca, Dario and Eskofier, Bjoern},
	year         = 2023,
	month        = {Mar},
	day          = 21,
	journal      = {Applied Intelligence},
	doi          = {10.1007/s10489-023-04532-5},
	issn         = {1573-7497},
	url          = {https://doi.org/10.1007/s10489-023-04532-5},
	abstract     = {Progress in making neural networks more robust against adversarial attacks is mostly marginal, despite the great efforts of the research community. Moreover, the robustness evaluation is often imprecise, making it challenging to identify promising approaches. We do an observational study on the classification decisions of 19 different state-of-the-art neural networks trained to be robust against adversarial attacks. This analysis gives a new indication of the limits of the robustness of current models on a common benchmark. In addition, our findings suggest that current untargeted adversarial attacks induce misclassification toward only a limited amount of different classes. Similarly, we find that previous attacks under-explore the perturbation space during optimization. This leads to unsuccessful attacks for samples where the initial gradient direction is not a good approximation of the final adversarial perturbation direction. Additionally, we observe that both over- and under-confidence in model predictions result in an inaccurate assessment of model robustness. Based on these observations, we propose a novel loss function for adversarial attacks that consistently improves their efficiency and success rate compared to prior attacks for all 30 analyzed models.}
}
@article{scikit-learn,
	title        = {Scikit-learn: Machine Learning in {P}ython},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = 2011,
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2825--2830}
}
@article{scott1998dewey,
	title        = {Dewey decimal classification},
	author       = {Scott, Mona L and SCOTT, MONA L},
	year         = 1998,
	journal      = {Libraries Unlimited}
}
@inproceedings{sehwag2019analyzing,
	title        = {Analyzing the Robustness of Open-World Machine Learning},
	author       = {Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek},
	year         = 2019,
	booktitle    = {Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
	location     = {London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {AISec'19},
	pages        = {105--116},
	doi          = {10.1145/3338501.3357372},
	isbn         = 9781450368339,
	url          = {https://doi.org/10.1145/3338501.3357372},
	abstract     = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing \o{}odAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that \o{}odAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
	numpages     = 12,
	keywords     = {adversarial example, open world recognition, deep learning}
}
@article{sekeh2020learning,
	title        = {Learning to Bound the Multi-Class Bayes Error},
	author       = {Sekeh, Salimeh Yasaei and Oselio, Brandon and Hero, Alfred O.},
	year         = 2020,
	journal      = {IEEE Transactions on Signal Processing},
	volume       = 68,
	number       = {},
	pages        = {3793--3807},
	doi          = {10.1109/TSP.2020.2994807}
}
@inproceedings{sengupta2016frontal,
	title        = {Frontal to profile face verification in the wild},
	author       = {Sengupta, Soumyadip and Chen, Jun-Cheng and Castillo, Carlos and Patel, Vishal M. and Chellappa, Rama and Jacobs, David W.},
	year         = 2016,
	booktitle    = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	volume       = {},
	number       = {},
	pages        = {1--9},
	doi          = {10.1109/WACV.2016.7477558}
}
@incollection{sep-natural-deduction,
	title        = {{Natural Deduction Systems in Logic}},
	author       = {Pelletier, Francis Jeffry and Hazen, Allen},
	year         = 2021,
	booktitle    = {The {Stanford} Encyclopedia of Philosophy},
	publisher    = {Metaphysics Research Lab, Stanford University},
	editor       = {Edward N. Zalta},
	howpublished = {\url{https://plato.stanford.edu/archives/win2021/entries/natural-deduction/}},
	edition      = {{W}inter 2021}
}
@inproceedings{severyn2012structural,
	title        = {Structural Relationships for Large-Scale Learning of Answer Re-Ranking},
	author       = {Severyn, Aliaksei and Moschitti, Alessandro},
	year         = 2012,
	booktitle    = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Portland, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '12},
	pages        = {741--750},
	doi          = {10.1145/2348283.2348383},
	isbn         = 9781450314725,
	url          = {https://doi.org/10.1145/2348283.2348383},
	abstract     = {Supervised learning applied to answer re-ranking can highly improve on the overall accuracy of question answering (QA) systems. The key aspect is that the relationships and properties of the question/answer pair composed of a question and the supporting passage of an answer candidate, can be efficiently compared with those captured by the learnt model.In this paper, we define novel supervised approaches that exploit structural relationships between a question and their candidate answer passages to learn a re-ranking model. We model structural representations of both questions and answers and their mutual relationships by just using an off-the-shelf shallow syntactic parser. We encode structures in Support Vector Machines (SVMs) by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces. Such models together with the latest approach to fast kernel-based learning enabled the training of our rerankers on hundreds of thousands of instances, which previously rendered intractable for kernelized SVMs. The results on two different QA datasets, e.g., Answerbag and Jeopardy! data, show that our models deliver large improvement on passage re-ranking tasks, reducing the error in Recall of BM25 baseline by about 18\%. One of the key findings of this work is that, despite its simplicity, shallow syntactic trees allow for learning complex relational structures, which exhibits a steep learning curve with the increase in the training size.},
	numpages     = 10,
	keywords     = {kernel methods, large-scale learning, support vector machines, question answering, structural kernels}
}
@inproceedings{shahmirzadi2019text,
	title        = {Text Similarity in Vector Space Models: A Comparative Study},
	author       = {Shahmirzadi, Omid and Lugowski, Adam and Younge, Kenneth},
	year         = 2019,
	booktitle    = {2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)},
	pages        = {659--666},
	doi          = {10.1109/ICMLA.2019.00120}
}
@inproceedings{sharif2016accessorize,
	title        = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},
	author       = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	location     = {Vienna, Austria},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CCS '16},
	pages        = {1528--1540},
	doi          = {10.1145/2976749.2978392},
	isbn         = 9781450341394,
	url          = {https://doi.org/10.1145/2976749.2978392},
	abstract     = {Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk.In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.},
	numpages     = 13,
	keywords     = {face recognition, face detection, adversarial machine learning, neural networks}
}
@inproceedings{sharif2018nneval,
	title        = {NNEval: Neural Network based Evaluation Metric for Image Captioning},
	author       = {Sharif, Naeha and White, Lyndon and Bennamoun, Mohammed and Shah, Syed Afaq Ali},
	year         = 2018,
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@book{shawe2004kernel,
	title        = {Kernel Methods for Pattern Analysis},
	author       = {John Shawe{-}Taylor and Nello Cristianini},
	year         = 2004,
	publisher    = {Cambridge University Press},
	isbn         = {978-0-521-81397-6},
	url          = {http://www.cambridge.org/gb/knowledge/isbn/item1169757/Kernel\%20Methods\%20for\%20Pattern\%20Analysis/?site\_locale=en\_GB},
	timestamp    = {Thu, 05 May 2011 16:48:09 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0026002.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{shen2014automatic,
	title        = {Automatic Fake Followers Detection in Chinese Micro-blogging System},
	author       = {Shen, Yi and Yu, Jianjun and Dong, Kejun and Nan, Kai},
	year         = 2014,
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {596--607},
	isbn         = {978-3-319-06605-9},
	editor       = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	abstract     = {Micro-blogging, which has greatly influenced people's life, is experiencing fantastic success in the worldwide. However, during its rapid development, it has encountered the problem of content pollution. Various pollution in the micro-blogging platforms has hurt the credibility of micro-blogging and caused significantly negative effect. In this paper, we mainly focus on detecting fake followers which may lead to a problematic situation on social media networks. By extracting major features of fake followers in Sina Weibo, we propose a binary classifier to distinguish fake followers from the legitimate users. The experiments show that all the proposed features are important and our method greatly outperforms to detect fake followers. We also present an elaborate analysis on the phenomenon of fake followers, infer the supported algorithms and principles behind them, and finally provide several suggestions for micro-blogging systems and ordinary users to deal with the fake followers.}
}
@inproceedings{shi2021fast,
	title        = {Fast Certified Robust Training with Short Warmup},
	author       = {Shi, Zhouxing and Wang, Yihan and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {18335--18349},
	url          = {https://proceedings.neurips.cc/paper/2021/file/988f9153ac4fd966ea302dd9ab9bae15-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{shi2021neural,
	title        = {Neural Abstractive Text Summarization with Sequence-to-Sequence Models},
	author       = {Shi, Tian and Keneshloo, Yaser and Ramakrishnan, Naren and Reddy, Chandan K.},
	year         = 2021,
	month        = {jan},
	journal      = {ACM/IMS Trans. Data Sci.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3419106},
	issn         = {2691-1922},
	url          = {https://doi.org/10.1145/3419106},
	issue_date   = {February 2021},
	abstract     = {In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efficiency and parallelism for training a model. In this article, we provide a comprehensive literature survey on different seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were first proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in NATS on the two recently released datasets, namely, Newsroom and Bytecup.},
	articleno    = 1,
	numpages     = 37,
	keywords     = {beam search, sequence-to-sequence models, pointer-generator network, attention model, deep reinforcement learning, Abstractive text summarization}
}
@article{shimanaka2019machine,
	title        = {Machine Translation Evaluation with {BERT} Regressor},
	author       = {Hiroki Shimanaka and Tomoyuki Kajiwara and Mamoru Komachi},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1907.12679},
	url          = {http://arxiv.org/abs/1907.12679},
	eprinttype   = {arXiv},
	eprint       = {1907.12679},
	timestamp    = {Fri, 02 Aug 2019 09:43:42 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1907-12679.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{shorten2019survey,
	title        = {A survey on Image Data Augmentation for Deep Learning},
	author       = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year         = 2019,
	month        = {Jul},
	day          = {06},
	journal      = {Journal of Big Data},
	volume       = 6,
	number       = 1,
	pages        = 60,
	doi          = {10.1186/s40537-019-0197-0},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-019-0197-0},
	abstract     = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.}
}
@article{shu2017fake,
	title        = {Fake News Detection on Social Media: A Data Mining Perspective},
	author       = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
	year         = 2017,
	month        = {sep},
	journal      = {SIGKDD Explor. Newsl.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 19,
	number       = 1,
	pages        = {22--36},
	doi          = {10.1145/3137597.3137600},
	issn         = {1931-0145},
	url          = {https://doi.org/10.1145/3137597.3137600},
	issue_date   = {June 2017},
	abstract     = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
	numpages     = 15
}
@article{siddharthan2006syntactic,
	title        = {Syntactic simplification and text cohesion},
	author       = {Siddharthan, Advaith},
	year         = 2006,
	journal      = {Research on Language and Computation},
	publisher    = {Springer},
	volume       = 4,
	number       = 1,
	pages        = {77--109}
}
@inproceedings{silva2018duplicate,
	title        = {Duplicate question detection in stack overflow: A reproducibility study},
	author       = {Silva, Rodrigo F. G. and Paixão, Klérisson and de Almeida Maia, Marcelo},
	year         = 2018,
	booktitle    = {2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
	volume       = {},
	number       = {},
	pages        = {572--581},
	doi          = {10.1109/SANER.2018.8330262}
}
@article{silva2020opportunities,
	title        = {Opportunities and Challenges in Deep Learning Adversarial Robustness: {A} Survey},
	author       = {Samuel Henrique Silva and Peyman Najafirad},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2007.00753},
	url          = {https://arxiv.org/abs/2007.00753},
	eprinttype   = {arXiv},
	eprint       = {2007.00753},
	timestamp    = {Mon, 06 Jul 2020 15:26:01 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2007-00753.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{simon2019sciences,
	title        = {The Sciences of the Artificial, reissue of the third edition with a new introduction by John Laird},
	author       = {Simon, Herbert Alexander},
	year         = 2019,
	publisher    = {MIT press}
}
@misc{simonoff_2010,
	title        = {Statistics and Data Analysis (COR1-GB.1305)},
	author       = {Simonoff, Jeffrey},
	year         = 2010
}
@article{singh2019abstract,
	title        = {An Abstract Domain for Certifying Neural Networks},
	author       = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
	year         = 2019,
	month        = {jan},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = {POPL},
	doi          = {10.1145/3290354},
	url          = {https://doi.org/10.1145/3290354},
	issue_date   = {January 2019},
	abstract     = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
	articleno    = 41,
	numpages     = 30,
	keywords     = {Adversarial attacks, Abstract Interpretation, Deep Learning}
}
@article{singh2019beyond,
	title        = {Beyond the single neuron convex barrier for neural network certification},
	author       = {Singh, Gagandeep and Ganvir, Rupanshu and P{\"u}schel, Markus and Vechev, Martin},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@inproceedings{singh2021brightness,
	title        = {On Brightness Agnostic Adversarial Examples Against Face Recognition Systems},
	author       = {Singh, Inderjeet and Momiyama, Satoru and Kakizaki, Kazuya and Araki, Toshinori},
	year         = 2021,
	booktitle    = {2021 International Conference of the Biometrics Special Interest Group (BIOSIG)},
	volume       = {},
	number       = {},
	pages        = {1--5},
	doi          = {10.1109/BIOSIG52210.2021.9548291}
}
@article{sinoara2019knowledge,
	title        = {Knowledge-enhanced document embeddings for text classification},
	author       = {Roberta A. Sinoara and Jose Camacho-Collados and Rafael G. Rossi and Roberto Navigli and Solange O. Rezende},
	year         = 2019,
	journal      = {Knowledge-Based Systems},
	volume       = 163,
	pages        = {955--971},
	doi          = {https://doi.org/10.1016/j.knosys.2018.10.026},
	issn         = {0950-7051},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950705118305124},
	keywords     = {Semantic representation, Document embeddings, Text classification, Text mining},
	abstract     = {Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.}
}
@misc{smith2007robustness,
	title        = {Robustness},
	author       = {Smith, Martha K.},
	year         = 2007,
	url          = {https://web.ma.utexas.edu/users/mks/384G07/robustness.pdf},
	note         = {Department of Mathematics, University of Texas at Austin, 1 University Station C1200, Austin, TX 78712},
	howpublished = {Lecture Notes in M374G/M384G/CAM384T: Regression Analysis}
}
@inproceedings{sohn2016improved,
	title        = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	author       = {Sohn, Kihyuk},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 29,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@article{song2021classification,
	title        = {Classification aware neural topic model for COVID-19 disinformation categorisation},
	author       = {Song, Xingyi and Petrak, Johann and Jiang, Ye and Singh, Iknoor and Maynard, Diana and Bontcheva, Kalina},
	year         = 2021,
	journal      = {PloS one},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = 16,
	number       = 2,
	pages        = {e0247086}
}
@article{souganciouglu2017biosses,
	title        = {{BIOSSES: a semantic sentence similarity estimation system for the biomedical domain}},
	author       = {Soğancıoğlu, Gizem and Öztürk, Hakime and Özgür, Arzucan},
	year         = 2017,
	month        = {07},
	journal      = {Bioinformatics},
	volume       = 33,
	number       = 14,
	pages        = {i49-i58},
	doi          = {10.1093/bioinformatics/btx238},
	issn         = {1367-4803},
	url          = {https://doi.org/10.1093/bioinformatics/btx238},
	abstract     = {{The amount of information available in textual format is rapidly increasing in the biomedical domain. Therefore, natural language processing (NLP) applications are becoming increasingly important to facilitate the retrieval and analysis of these data. Computing the semantic similarity between sentences is an important component in many NLP tasks including text retrieval and summarization. A number of approaches have been proposed for semantic sentence similarity estimation for generic English. However, our experiments showed that such approaches do not effectively cover biomedical knowledge and produce poor results for biomedical text.We propose several approaches for sentence-level semantic similarity computation in the biomedical domain, including string similarity measures and measures based on the distributed vector representations of sentences learned in an unsupervised manner from a large biomedical corpus. In addition, ontology-based approaches are presented that utilize general and domain-specific ontologies. Finally, a supervised regression based model is developed that effectively combines the different similarity computation metrics. A benchmark data set consisting of 100 sentence pairs from the biomedical literature is manually annotated by five human experts and used for evaluating the proposed methods.The experiments showed that the supervised semantic sentence similarity computation approach obtained the best performance (0.836 correlation with gold standard human annotations) and improved over the state-of-the-art domain-independent systems up to 42.6\% in terms of the Pearson correlation metric.A web-based system for biomedical semantic sentence similarity computation, the source code, and the annotated benchmark data set are available at: http://tabilab.cmpe.boun.edu.tr/BIOSSES/.}},
	eprint       = {https://academic.oup.com/bioinformatics/article-pdf/33/14/i49/25157316/btx238.pdf}
}
@software{spacy,
	title        = {{spaCy: Industrial-strength Natural Language Processing in Python}},
	author       = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	year         = 2020,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.1212303},
	url          = {https://doi.org/10.5281/zenodo.1212303}
}
@article{spruill2007asymptotic,
	title        = {Asymptotic Distribution of Coordinates  on High Dimensional Spheres},
	author       = {Marcus Spruill},
	year         = 2007,
	journal      = {Electron. Commun. Probab.},
	volume       = 12,
	pages        = {no. 23, 234--247},
	doi          = {10.1214/ECP.v12-1294},
	issn         = {1083-589X},
	url          = {http://ecp.ejpecp.org/article/view/1294},
	fjournal     = {Electronic Communications in Probability},
	keywords     = {empiric distribution; dependent arrays; micro-canonical ensemble;Minkowski area; isoperimetry},
	abstract     = {The coordinates $x_i$ of a point $x = (x_1, x_2, \dots, x_n)$ chosen at     random according to a uniform distribution on the $\ell_2(n)$-sphere of     radius $n^{1/2}$  have approximately a normal distribution when $n$ is large. The coordinates $x_i$   of points uniformly distributed on the $\ell_1(n)$-sphere of radius $n$  have  approximately a double exponential distribution.  In these and all   the $\ell_p(n),1 \le p \le \infty,$ convergence of the distribution of coordinates   as the dimension $n$ increases is at the rate $\sqrt{n}$ and is described   precisely in terms of weak convergence of a normalized empirical process to   a limiting Gaussian process, the sum of a Brownian bridge and a simple normal process.}
}
@misc{stats_twitter,
	title        = {Twitter Usage Statistics},
	title        = {Twitter usage statistics},
	author       = {Internet Live Statistics},
	year         = 2016,
	journal      = {Twitter Usage Statistics - Internet Live Stats},
	url          = {http://www.internetlivestats.com/twitter-statistics/},
	note         = {http://www.internetlivestats.com/twitter-statistics/},
	howpublished = {Internet}
}
@inproceedings{stent2005evaluating,
	title        = {Evaluating Evaluation Methods for Generation in the Presence of Variation},
	author       = {Stent, Amanda and Marge, Matthew and Singhai, Mohit},
	year         = 2005,
	booktitle    = {Computational Linguistics and Intelligent Text Processing},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {341--351},
	isbn         = {978-3-540-30586-6},
	editor       = {Gelbukh, Alexander},
	abstract     = {Recent years have seen increasing interest in automatic metrics for the evaluation of generation systems. When a system can generate syntactic variation, automatic evaluation becomes more difficult. In this paper, we compare the performance of several automatic evaluation metrics using a corpus of automatically generated paraphrases. We show that these evaluation metrics can at least partially measure adequacy (similarity in meaning), but are not good measures of fluency (syntactic correctness). We make several proposals for improving the evaluation of generation systems that produce variation.}
}
@inproceedings{su1992new,
	title        = {A new quantitative quality measure for machine translation systems},
	author       = {Su, Keh-Yih and Wu, Ming-Wen and Chang, Jing-Shin},
	year         = 1992,
	booktitle    = {COLING 1992 Volume 2: The 14th International Conference on Computational Linguistics}
}
@article{su2019one,
	title        = {One Pixel Attack for Fooling Deep Neural Networks},
	author       = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
	year         = 2019,
	journal      = {IEEE Transactions on Evolutionary Computation},
	volume       = 23,
	number       = 5,
	pages        = {828--841},
	doi          = {10.1109/TEVC.2019.2890858}
}
@article{sun2015deepid3,
	title        = {DeepID3: Face Recognition with Very Deep Neural Networks},
	author       = {Yi Sun and Ding Liang and Xiaogang Wang and Xiaoou Tang},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1502.00873},
	url          = {http://arxiv.org/abs/1502.00873},
	eprinttype   = {arXiv},
	eprint       = {1502.00873},
	timestamp    = {Tue, 10 Dec 2019 15:37:26 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/SunLWT15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{sun2020ernie,
	title        = {ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding},
	author       = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	year         = 2020,
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 34,
	number       = {05},
	pages        = {8968--8975},
	doi          = {10.1609/aaai.v34i05.6428},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6428},
	abstractnote = {&lt;p&gt;Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.&lt;/p&gt;}
}
@inproceedings{sun2022causality,
	title        = {Causality-Based Neural Network Repair},
	author       = {Sun, Bing and Sun, Jun and Pham, Long H. and Shi, Jie},
	year         = 2022,
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {338--349},
	doi          = {10.1145/3510003.3510080},
	isbn         = 9781450392211,
	url          = {https://doi.org/10.1145/3510003.3510080},
	abstract     = {Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the 'guilty' neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91\% on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98\% to less than 1\%. For safety property repair tasks, CARE reduces the property violation rate to less than 1\%. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.},
	numpages     = 12
}
@article{surian2016characterizing,
	title        = {Characterizing Twitter Discussions About HPV Vaccines Using Topic Modeling and Community Detection},
	author       = {Surian, Didi and Nguyen, Dat Quoc and Kennedy, Georgina and Johnson, Mark and Coiera, Enrico and Dunn, Adam G},
	year         = 2016,
	month        = {Aug},
	day          = 29,
	journal      = {J Med Internet Res},
	volume       = 18,
	number       = 8,
	pages        = {e232},
	doi          = {10.2196/jmir.6045},
	issn         = {1438-8871},
	url          = {http://www.jmir.org/2016/8/e232/},
	url          = {https://doi.org/10.2196/jmir.6045},
	url          = {http://www.ncbi.nlm.nih.gov/pubmed/27573910},
	keywords     = {topic modelling; graph algorithms analysis; social media; public health surveillance},
	abstract     = {Background: In public health surveillance, measuring how information enters and spreads through online communities may help us understand geographical variation in decision making associated with poor health outcomes. Objective: Our aim was to evaluate the use of community structure and topic modeling methods as a process for characterizing the clustering of opinions about human papillomavirus (HPV) vaccines on Twitter. Methods: The study examined Twitter posts (tweets) collected between October 2013 and October 2015 about HPV vaccines. We tested Latent Dirichlet Allocation and Dirichlet Multinomial Mixture (DMM) models for inferring topics associated with tweets, and community agglomeration (Louvain) and the encoding of random walks (Infomap) methods to detect community structure of the users from their social connections. We examined the alignment between community structure and topics using several common clustering alignment measures and introduced a statistical measure of alignment based on the concentration of specific topics within a small number of communities. Visualizations of the topics and the alignment between topics and communities are presented to support the interpretation of the results in context of public health communication and identification of communities at risk of rejecting the safety and efficacy of HPV vaccines. Results: We analyzed 285,417 Twitter posts (tweets) about HPV vaccines from 101,519 users connected by 4,387,524 social connections. Examining the alignment between the community structure and the topics of tweets, the results indicated that the Louvain community detection algorithm together with DMM produced consistently higher alignment values and that alignments were generally higher when the number of topics was lower. After applying the Louvain method and DMM with 30 topics and grouping semantically similar topics in a hierarchy, we characterized 163,148 (57.16{\%}) tweets as evidence and advocacy, and 6244 (2.19{\%}) tweets describing personal experiences. Among the 4548 users who posted experiential tweets, 3449 users (75.84{\%}) were found in communities where the majority of tweets were about evidence and advocacy. Conclusions: The use of community detection in concert with topic modeling appears to be a useful way to characterize Twitter communities for the purpose of opinion surveillance in public health applications. Our approach may help identify online communities at risk of being influenced by negative opinions about public health interventions such as HPV vaccines.}
}
@inproceedings{szegedy2013intriguing,
	title        = {Intriguing properties of neural networks},
	author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
	year         = 2014,
	booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1312.6199},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:35:25 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{szegedy2016rethinking,
	title        = {Rethinking the Inception Architecture for Computer Vision},
	author       = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	year         = 2016,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {2818--2826}
}
@article{tack2022consistency,
	title        = {Consistency Regularization for Adversarial Robustness},
	author       = {Tack, Jihoon and Yu, Sihyun and Jeong, Jongheon and Kim, Minseon and Hwang, Sung Ju and Shin, Jinwoo},
	year         = 2022,
	month        = {Jun.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 36,
	number       = 8,
	pages        = {8414--8422},
	doi          = {10.1609/aaai.v36i8.20817},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20817},
	abstractnote = {Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary `consistency’ regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.}
}
@inproceedings{taigman2014deepface,
	title        = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},
	author       = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year         = 2014,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {1701--1708}
}
@article{tambon2023probabilistic,
	title        = {A probabilistic framework for mutation testing in deep neural networks},
	author       = {Florian Tambon and Foutse Khomh and Giuliano Antoniol},
	year         = 2023,
	journal      = {Information and Software Technology},
	volume       = 155,
	pages        = 107129,
	doi          = {https://doi.org/10.1016/j.infsof.2022.107129},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584922002385},
	keywords     = {Deep Learning, Mutation Testing, Bayesian Probability},
	abstract     = {Context: Mutation Testing (MT) is an important tool in traditional Software Engineering (SE) white-box testing. It aims to artificially inject faults in a system to evaluate a test suite's capability to detect them, assuming that the test suite defects finding capability will then translate to real faults. If MT has long been used in SE, it is only recently that it started gaining the attention of the Deep Learning (DL) community, with researchers adapting it to improve the testability of DL models and improve the trustworthiness of DL systems. Objective: If several techniques have been proposed for MT, most of them neglected the stochasticity inherent to DL resulting from the training phase. Even the latest MT approaches in DL, which propose to tackle MT through a statistical approach, might give inconsistent results. Indeed, as their statistic is based on a fixed set of sampled training instances, it can lead to different results across instances set when results should be consistent for any instance. Methods: In this work, we propose a Probabilistic Mutation Testing (PMT) approach that alleviates the inconsistency problem and allows for a more consistent decision on whether a mutant is killed or not. Results: We show that PMT effectively allows a more consistent and informed decision on mutations through evaluation using three models and eight mutation operators used in previously proposed MT methods. We also analyze the trade-off between the approximation error and the cost of our method, showing that relatively small error can be achieved for a manageable cost. Conclusion: Our results showed the limitation of current MT practices in DNN and the need to rethink them. We believe PMT is the first step in that direction which effectively removes the lack of consistency across test executions of previous methods caused by the stochasticity of DNN training.}
}
@inproceedings{tan2019efficientnet,
	title        = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
	author       = {Tan, Mingxing and Le, Quoc},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {6105--6114},
	url          = {https://proceedings.mlr.press/v97/tan19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf}
}
@inproceedings{tempo1996probabilistic,
	title        = {Probabilistic robustness analysis: explicit bounds for the minimum number of samples},
	author       = {Tempo, R. and Bai, E.W. and Dabbene, F.},
	year         = 1996,
	booktitle    = {Proceedings of 35th IEEE Conference on Decision and Control},
	volume       = 3,
	number       = {},
	pages        = {3424--3428 vol.3},
	doi          = {10.1109/CDC.1996.573690}
}
@inproceedings{theisen2021evaluating,
	title        = {Evaluating State-of-the-Art Classification Models Against Bayes Optimality},
	author       = {Theisen, Ryan and Wang, Huan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {9367--9377},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0ccd2b894f717df5ebc12f4282ee70-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@book{theodoridis2006pattern,
	title        = {Pattern Recognition, Fourth Edition},
	author       = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
	year         = 2008,
	publisher    = {Academic Press, Inc.},
	address      = {USA},
	isbn         = 1597492728,
	edition      = {4th},
	abstract     = {This book considers classical and current theory and practice, of both supervised and unsupervised pattern recognition, to build a complete background for professionals and students of engineering. The authors, leading experts in the field of pattern recognition, have provided an up-to-date, self-contained volume encapsulating this wide spectrum of information. The very latest methods are incorporated in this edition: semi-supervised learning, combining clustering algorithms, and relevance feedback.This edition includes many more worked examples and diagrams (in two colour) to help give greater understanding of the methods and their application. Computer-based problems will be included with MATLAB code. An accompanying book contains extra worked examples and MATLAB code of all the examples used in this book.Thoroughly developed to include many more worked examples to give greater understanding of this mathematically oriented subjectMany more diagrams included--now in two color--to provide greater insight through visual presentationAn accompanying manual includes Matlab code of the methods and algorithms in the book, together with solved problems and real-life data sets in medical imaging, remote sensing and audio recognition. The Manual is available separately or at a special packaged price (ISBN: 9780123744869).Latest hot topics included to further the reference value of the text including semi-supervised learning, combining clustering algorithms, and relevance feedback.}
}
@article{thornton2021predation,
	title        = {The Predation Game: Does dividing attention affect patterns of human foraging?},
	author       = {Thornton, Ian M. and Tagu, J{\'e}r{\^o}me and Zdravkovi{\'{c}}, Sun{\v{c}}ica and Kristj{\'a}nsson, {\'A}rni},
	year         = 2021,
	month        = {May},
	day          = {06},
	journal      = {Cognitive Research: Principles and Implications},
	volume       = 6,
	number       = 1,
	pages        = 35,
	doi          = {10.1186/s41235-021-00299-w},
	issn         = {2365-7464},
	url          = {https://doi.org/10.1186/s41235-021-00299-w},
	abstract     = {Attention is known to play an important role in shaping the behaviour of both human and animal foragers. Here, in three experiments, we built on previous interactive tasks to create an online foraging game for studying divided attention in human participants exposed to the (simulated) risk of predation. Participants used a ``sheep'' icon to collect items from different target categories randomly distributed across the display. Each trial also contained ``wolf'' objects, whose movement was inspired by classic studies of multiple object tracking. When participants needed to physically avoid the wolves, foraging patterns changed, with an increased tendency to switch between target categories and a decreased ability to prioritise high reward targets, relative to participants who could safely ignore them. However, when the wolves became dangerous by periodically changing form (briefly having big eyes) instead of by approaching the sheep, foraging patterns were unaffected. Spatial disruption caused by the need to rapidly shift position---rather the cost of reallocating attention---therefore appears to influence foraging in this context. These results thus confirm that participants can efficiently alternate between target selection and tracking moving objects, replicating earlier single-target search findings. Future studies may need to increase the perceived risk or potential costs associated with simulated danger, in order to elicit the extended run behaviour predicted by animal models of foraging, but absent in the current data.}
}
@inproceedings{tian2009using,
	title        = {Using Latent Dirichlet Allocation for automatic categorization of software},
	author       = {Tian, Kai and Revelle, Meghan and Poshyvanyk, Denys},
	year         = 2009,
	booktitle    = {2009 6th IEEE International Working Conference on Mining Software Repositories},
	volume       = {},
	number       = {},
	pages        = {163--166},
	doi          = {10.1109/MSR.2009.5069496}
}
@misc{tian2020sticking,
	title        = {Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation},
	author       = {Ran Tian and Shashi Narayan and Thibault Sellam and Ankur P. Parikh},
	year         = 2020,
	url          = {https://openreview.net/forum?id=HkxU2pNYPH}
}
@article{tien2019sentence,
	title        = {Sentence modeling via multiple word embeddings and multi-level comparison for semantic textual similarity},
	author       = {Nguyen Huy Tien and Nguyen Minh Le and Yamasaki Tomohiro and Izuha Tatsuya},
	year         = 2019,
	journal      = {Information Processing  \& Management},
	volume       = 56,
	number       = 6,
	pages        = 102090,
	doi          = {https://doi.org/10.1016/j.ipm.2019.102090},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457319301335},
	keywords     = {Multiple word embeddings, Sentence embedding, Semantic, Similarity, Multi-level comparison},
	abstract     = {Recently, using a pretrained word embedding to represent words achieves success in many natural language processing tasks. According to objective functions, different word embedding models capture different aspects of linguistic properties. However, the Semantic Textual Similarity task, which evaluates similarity/relation between two sentences, requires to take into account of these linguistic aspects. Therefore, this research aims to encode various characteristics from multiple sets of word embeddings into one embedding and then learn similarity/relation between sentences via this novel embedding. Representing each word by multiple word embeddings, the proposed MaxLSTM-CNN encoder generates a novel sentence embedding. We then learn the similarity/relation between our sentence embeddings via Multi-level comparison. Our method M-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e., measure textual similarity, identify paraphrase, recognize textual entailment). Our model does not use hand-crafted features (e.g., alignment features, Ngram overlaps, dependency features) as well as does not require pre-trained word embeddings to have the same dimension.}
}
@inproceedings{tillmann1997accelerated,
	title        = {Accelerated {DP} based search for statistical translation},
	author       = {Christoph Tillmann and Stephan Vogel and Hermann Ney and A. Zubiaga and Hassan Sawaf},
	year         = 1997,
	booktitle    = {Fifth European Conference on Speech Communication and Technology, {EUROSPEECH} 1997, Rhodes, Greece, September 22-25, 1997},
	publisher    = {{ISCA}},
	url          = {http://www.isca-speech.org/archive/eurospeech\_1997/e97\_2667.html},
	editor       = {George Kokkinakis and Nikos Fakotakis and Evangelos Dermatas},
	timestamp    = {Tue, 16 Nov 2021 11:37:43 +0100},
	biburl       = {https://dblp.org/rec/conf/interspeech/TillmannVNZS97.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tjeng2017evaluating,
	title        = {Evaluating Robustness of Neural Networks with Mixed Integer Programming},
	author       = {Vincent Tjeng and Kai Yuanqing Xiao and Russ Tedrake},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1711.07356},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HyGIdiRqtm},
	timestamp    = {Tue, 10 Aug 2021 17:46:21 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TjengXT19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{tolga_2020,
	title        = {Feeding flying foxes},
	author       = {Tolga},
	year         = 2020,
	month        = {Aug},
	journal      = {Tolga Bat Hospital},
	url          = {https://tolgabathospital.org/bats/feeding/?portfolioCats=110}
}
@inproceedings{tramer2017ensemble,
	title        = {Ensemble Adversarial Training: Attacks and Defenses},
	author       = {Florian Tram{\`{e}}r and Alexey Kurakin and Nicolas Papernot and Ian J. Goodfellow and Dan Boneh and Patrick D. McDaniel},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rkZvSe-RZ},
	timestamp    = {Sun, 02 Oct 2022 16:05:32 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TramerKPGBM18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tramer2020adaptive,
	title        = {On Adaptive Attacks to Adversarial Example Defenses},
	author       = {Tramer, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1633--1645},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/11f38f8ecd71867b42433548d1078e38-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{trigueros2018enhancing,
	title        = {Enhancing convolutional neural networks for face recognition with occlusion maps and batch triplet loss},
	author       = {Daniel {Sáez Trigueros} and Li Meng and Margaret Hartnett},
	year         = 2018,
	journal      = {Image and Vision Computing},
	volume       = 79,
	pages        = {99--108},
	doi          = {https://doi.org/10.1016/j.imavis.2018.09.011},
	issn         = {0262-8856},
	url          = {https://www.sciencedirect.com/science/article/pii/S0262885618301562},
	keywords     = {Face recognition, Convolutional neural networks, Facial occlusions, Distance metric learning},
	abstract     = {Despite the recent success of convolutional neural networks for computer vision applications, unconstrained face recognition remains a challenge. In this work, we make two contributions to the field. Firstly, we consider the problem of face recognition with partial occlusions and show how current approaches might suffer significant performance degradation when dealing with this kind of face images. We propose a simple method to find out which parts of the human face are more important to achieve a high recognition rate, and use that information during training to force a convolutional neural network to learn discriminative features from all the face regions more equally, including those that typical approaches tend to pay less attention to. We test the accuracy of the proposed method when dealing with real-life occlusions using the AR face database. Secondly, we propose a novel loss function called batch triplet loss that improves the performance of the triplet loss by adding an extra term to the loss function to cause minimisation of the standard deviation of both positive and negative scores. We show consistent improvement in the Labeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments to the convolutional neural network training.}
}
@inproceedings{trockman2021orthogonalizing,
	title        = {Orthogonalizing Convolutional Layers with the Cayley Transform},
	author       = {Asher Trockman and J Zico Kolter},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Pbj8H_jEHYv}
}
@inproceedings{tsipras2018robustness,
	title        = {Robustness May Be at Odds with Accuracy},
	author       = {Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SyxAb30cY7},
	timestamp    = {Thu, 25 Jul 2019 14:26:02 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TsiprasSETM19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tumer1996estimating,
	title        = {Estimating the Bayes error rate through classifier combining},
	author       = {Tumer, K. and Ghosh, J.},
	year         = 1996,
	booktitle    = {Proceedings of 13th International Conference on Pattern Recognition},
	volume       = 2,
	number       = {},
	pages        = {695--699 vol.2},
	doi          = {10.1109/ICPR.1996.546912}
}
@article{tversky1977features,
	title        = {Features of similarity.},
	author       = {Tversky, Amos},
	year         = 1977,
	journal      = {Psychological review},
	publisher    = {American Psychological Association},
	volume       = 84,
	number       = 4,
	pages        = 327
}
@article{usman2022antidotert,
	title        = {AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks},
	author       = {Muhammad Usman and Youcheng Sun and Divya Gopinath and Corina S. Pasareanu},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2202.01179},
	url          = {https://arxiv.org/abs/2202.01179},
	eprinttype   = {arXiv},
	eprint       = {2202.01179},
	timestamp    = {Wed, 09 Feb 2022 15:43:35 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2202-01179.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{utah,
	title        = {E5: 7 domains of food},
	author       = {UTAH, University of Utah Health},
	year         = 2020,
	month        = {Nov},
	journal      = {University of Utah Health},
	url          = {https://healthcare.utah.edu/the-scope/shows.php?shows=1_moomp8q5}
}
@article{vaishnavi2022accelerating,
	title        = {Accelerating Certified Robustness Training via Knowledge Transfer},
	author       = {Pratik Vaishnavi and Kevin Eykholt and Amir Rahmati},
	year         = 2022,
	journal      = {CoRR},
	volume       = {abs/2210.14283},
	doi          = {10.48550/arXiv.2210.14283},
	url          = {https://doi.org/10.48550/arXiv.2210.14283},
	eprinttype   = {arXiv},
	eprint       = {2210.14283},
	timestamp    = {Mon, 31 Oct 2022 12:04:42 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2210-14283.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{valencia1991studies,
	title        = {Studies on natural logic and categorial grammar},
	author       = {Valencia, V{\'i}ctor Manuel S{\'a}nchez},
	year         = 1991,
	publisher    = {Universiteit van Amsterdam}
}
@article{van2005management,
	title        = {Management Research as a Design Science: Articulating the Research Products of Mode 2 Knowledge Production in Management},
	author       = {Van Aken, Joan Ernst},
	year         = 2005,
	journal      = {British Journal of Management},
	volume       = 16,
	number       = 1,
	pages        = {19--36},
	doi          = {https://doi.org/10.1111/j.1467-8551.2005.00437.x},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8551.2005.00437.x},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8551.2005.00437.x},
	abstract     = {The relevance problem of academic management research in organization and management is an old and thorny one. Recent discussions on this issue have resulted in proposals to use more Mode 2 knowledge production in our field. These discussions focused mainly on the process of research itself and less on the products produced by this process. Here the focus is on the so-called field-tested and grounded technological rule as a possible product of Mode 2 research with the potential to improve the relevance of academic research in management. Technological rules can be seen as solution-oriented knowledge. Such knowledge may be called Management Theory, while more description-oriented knowledge may be called Organization Theory. In this article the nature of technological rules in management is discussed, as well as their development, their use in actual management practice and the potential for cross-fertilization between Management Theory and Organization Theory.}
}
@article{van2021human,
	title        = {Human evaluation of automatically generated text: Current trends and best practice guidelines},
	author       = {Chris {van der Lee} and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer},
	year         = 2021,
	journal      = {Computer Speech  \& Language},
	volume       = 67,
	pages        = 101151,
	doi          = {https://doi.org/10.1016/j.csl.2020.101151},
	issn         = {0885-2308},
	url          = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
	keywords     = {Natural Language Generation, Human evaluation, Recommendations, Literature review, Open science, Ethics},
	abstract     = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.}
}
@book{vapnik1999nature,
	title        = {The nature of statistical learning theory},
	author       = {Vapnik, Vladimir},
	year         = 1999,
	publisher    = {Springer science  \& business media}
}
@inproceedings{vaswani2017attention,
	title        = {Attention is All you Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	url          = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{vedantam2015cider,
	title        = {CIDEr: Consensus-Based Image Description Evaluation},
	author       = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
	year         = 2015,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{villegas2018characterizing,
	title        = {Characterizing context-aware recommender systems: A systematic literature review},
	author       = {Villegas, Norha M and S{\'a}nchez, Cristian and D{\'\i}az-Cely, Javier and Tamura, Gabriel},
	year         = 2018,
	journal      = {Knowledge-Based Systems},
	publisher    = {Elsevier},
	volume       = 140,
	pages        = {173--200}
}
@inproceedings{virmaux2018lipschitz,
	title        = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	author       = {Virmaux, Aladin and Scaman, Kevin},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2018/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@inproceedings{wadler1989theorems,
	title        = {Theorems for Free!},
	author       = {Wadler, Philip},
	year         = 1989,
	booktitle    = {Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture},
	location     = {Imperial College, London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FPCA '89},
	pages        = {347--359},
	doi          = {10.1145/99370.99404},
	isbn         = {0897913280},
	url          = {https://doi.org/10.1145/99370.99404},
	numpages     = 13
}
@article{wald1992sequential,
	title        = {{Sequential Tests of Statistical Hypotheses}},
	author       = {A. Wald},
	year         = 1945,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 16,
	number       = 2,
	pages        = {117 -- 186},
	doi          = {10.1214/aoms/1177731118},
	url          = {https://doi.org/10.1214/aoms/1177731118}
}
@inproceedings{wang2006topics,
	title        = {Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends},
	author       = {Wang, Xuerui and McCallum, Andrew},
	year         = 2006,
	booktitle    = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Philadelphia, PA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '06},
	pages        = {424--433},
	doi          = {10.1145/1150402.1150450},
	isbn         = 1595933395,
	url          = {https://doi.org/10.1145/1150402.1150450},
	abstract     = {This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time. Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp. Thus, the meaning of a particular topic can be relied upon as constant, but the topics' occurrence and correlations change significantly over time. We present results on nine months of personal email, 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses, showing improved topics, better timestamp prediction, and interpretable trends.},
	numpages     = 10,
	keywords     = {temporal analysis, graphical models, topic modeling}
}
@inproceedings{wang2012tm,
	title        = {TM-LDA: Efficient Online Modeling of Latent Topic Transitions in Social Media},
	author       = {Wang, Yu and Agichtein, Eugene and Benzi, Michele},
	year         = 2012,
	booktitle    = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '12},
	pages        = {123--131},
	doi          = {10.1145/2339530.2339552},
	isbn         = 9781450314626,
	url          = {https://doi.org/10.1145/2339530.2339552},
	abstract     = {Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.},
	numpages     = 9,
	keywords     = {topic transition modeling, temporal language models, mining social media data}
}
@inproceedings{wang2016theoretical,
	title        = {A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples},
	author       = {Beilun Wang and Ji Gao and Yanjun Qi},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HkcM7yVKl},
	timestamp    = {Thu, 04 Apr 2019 13:20:10 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/WangGQ17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2017deep,
	title        = {Deep Metric Learning With Angular Loss},
	author       = {Wang, Jian and Zhou, Feng and Wen, Shilei and Liu, Xiao and Lin, Yuanqing},
	year         = 2017,
	month        = {Oct},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{wang2017residual,
	title        = {Residual Attention Network for Image Classification},
	author       = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year         = 2017,
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{wang2017unleash,
	title        = {Unleash the Black Magic in Age: A Multi-Task Deep Neural Network Approach for Cross-Age Face Verification},
	author       = {Wang, Xiaolong and Zhou, Yin and Kong, Deguang and Currey, Jon and Li, Dawei and Zhou, Jiayu},
	year         = 2017,
	booktitle    = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
	volume       = {},
	number       = {},
	pages        = {596--603},
	doi          = {10.1109/FG.2017.75}
}
@article{wang2018additive,
	title        = {Additive Margin Softmax for Face Verification},
	author       = {Wang, Feng and Cheng, Jian and Liu, Weiyang and Liu, Haijun},
	year         = 2018,
	journal      = {IEEE Signal Processing Letters},
	volume       = 25,
	number       = 7,
	pages        = {926--930},
	doi          = {10.1109/LSP.2018.2822810}
}
@inproceedings{wang2018cosface,
	title        = {CosFace: Large Margin Cosine Loss for Deep Face Recognition},
	author       = {Wang, Hao and Wang, Yitong and Zhou, Zheng and Ji, Xing and Gong, Dihong and Zhou, Jingchao and Li, Zhifeng and Liu, Wei},
	year         = 2018,
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{wang2018formal,
	title        = {Formal Security Analysis of Neural Networks using Symbolic Intervals},
	author       = {Shiqi Wang and Kexin Pei and Justin Whitehouse and Junfeng Yang and Suman Jana},
	year         = 2018,
	month        = aug,
	booktitle    = {27th USENIX Security Symposium (USENIX Security 18)},
	publisher    = {USENIX Association},
	address      = {Baltimore, MD},
	pages        = {1599--1614},
	isbn         = {978-1-939133-04-5},
	url          = {https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi}
}
@inproceedings{wang2018happiness,
	title        = {Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation},
	author       = {Wang, Jia and Feng, Yungang and Naghizade, Elham and Rashidi, Lida and Lim, Kwan Hui and Lee, Kate},
	year         = 2018,
	booktitle    = {Companion Proceedings of the The Web Conference 2018},
	location     = {Lyon, France},
	publisher    = {International World Wide Web Conferences Steering Committee},
	address      = {Republic and Canton of Geneva, CHE},
	series       = {WWW '18},
	pages        = {1401--1405},
	doi          = {10.1145/3184558.3191583},
	isbn         = 9781450356404,
	url          = {https://doi.org/10.1145/3184558.3191583},
	abstract     = {Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.},
	numpages     = 5,
	keywords     = {social networks, sentiment analysis, location recommendation}
}
@inproceedings{wang2019improving,
	title        = {Improving adversarial robustness requires revisiting misclassified examples},
	author       = {Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{wang2020improving,
	title        = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},
	author       = {Yisen Wang and Difan Zou and Jinfeng Yi and James Bailey and Xingjun Ma and Quanquan Gu},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rklOg6EFwS},
	timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/0001ZY0MG20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wang2020mis,
	title        = {Mis-Classified Vector Guided Softmax Loss for Face Recognition},
	author       = {Wang, Xiaobo and Zhang, Shifeng and Wang, Shuo and Fu, Tianyu and Shi, Hailin and Mei, Tao},
	year         = 2020,
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 34,
	number       = {07},
	pages        = {12241--12248},
	doi          = {10.1609/aaai.v34i07.6906},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6906},
	abstractnote = {&lt;p&gt;Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (&lt;em&gt;e.g.&lt;/em&gt;, angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative features mining for discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and feature mining into a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives. Our code is available at http://www.cbsr.ia.ac.cn/users/xiaobowang/.&lt;/p&gt;}
}
@inproceedings{wang2021crafting,
	title        = {Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection},
	author       = {Wang, Chenran and Zhang, Danyi and Huang, Suye and Li, Xiangyang and Ding, Leah},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems},
	location     = {Virtual Event, Hong Kong},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASSS '21},
	pages        = {23–28},
	doi          = {10.1145/3457340.3458302},
	isbn         = 9781450384032,
	url          = {https://doi.org/10.1145/3457340.3458302},
	abstract     = {While machine learning based spam detectors have proven useful, spammers are learning to bypass the detectors by modifying their email content. Adversarial attacks on machine learning models have been observed in domains such as image classification. Applying such adversarial attack algorithms to craft spam emails to evade spam email detectors, however, has limitations. Such algorithms generate adversarial perturbations in the feature space. Different from image data, translating the adversarial perturbations from the feature space to text formats, as in emails, changes the effectiveness of the adversarial perturbations. It can reduce the attack success rate in the case of spam email detection. In this paper, we study the feasibility of adversarial attacks on machine learning based spam detectors and propose two novel text crafting methods leveraging adversarial perturbations generated by the adversarial example generation algorithms to improve the attack effectiveness. One method tries to approximate the feature values and the other adds special words to original emails. In experimentation, we use PGD as an example to demonstrate and compare the effectiveness of our attack methods on spam email detectors. We also examine the transferability of the proposed attack methods on different machine learning models.},
	numpages     = 6,
	keywords     = {spam detection, crafting adversarial email, attack transferability, adversarial machine learning}
}
@article{wang2021deep,
	title        = {Deep face recognition: A survey},
	author       = {Mei Wang and Weihong Deng},
	year         = 2021,
	journal      = {Neurocomputing},
	volume       = 429,
	pages        = {215--244},
	doi          = {https://doi.org/10.1016/j.neucom.2020.10.081},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231220316945},
	keywords     = {Deep face recognition, Deep learning, Face processing, Face recognition database, Loss function, Deep network architecture},
	abstract     = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: “one-to-many augmentation” and “many-to-one normalization”. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.}
}
@inproceedings{wang2021enhancing,
	title        = {Enhancing the Transferability of Adversarial Attacks Through Variance Tuning},
	author       = {Wang, Xiaosen and He, Kun},
	year         = 2021,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {1924--1933}
}
@inproceedings{wang2021robot,
	title        = {RobOT: Robustness-Oriented Testing for Deep Learning Systems},
	author       = {Wang, Jingyi and Chen, Jialuo and Sun, Youcheng and Ma, Xingjun and Wang, Dongxia and Sun, Jun and Cheng, Peng},
	year         = 2021,
	booktitle    = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
	volume       = {},
	number       = {},
	pages        = {300--311},
	doi          = {10.1109/ICSE43902.2021.00038}
}
@inproceedings{wang2022mlfw,
	title        = {MLFW: A Database for Face Recognition on Masked Faces},
	author       = {Wang, Chengrui and Fang, Han and Zhong, Yaoyao and Deng, Weihong},
	year         = 2022,
	booktitle    = {Biometric Recognition},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {180--188},
	isbn         = {978-3-031-20233-9},
	editor       = {Deng, Weihong and Feng, Jianjiang and Huang, Di and Kan, Meina and Sun, Zhenan and Zheng, Fang and Wang, Wenfeng and He, Zhaofeng},
	abstract     = {As more and more people begin to wear masks due to current COVID-19 pandemic, existing face recognition systems may encounter severe performance degradation when recognizing masked faces. To figure out the impact of masks on face recognition model, we build a simple but effective tool to generate masked faces from unmasked faces automatically, and construct a new database called Masked LFW (MLFW) based on Cross-Age LFW (CALFW) database. The mask on the masked face generated by our method has good visual consistency with the original face. Moreover, we collect various mask templates, covering most of the common styles appeared in the daily life, to achieve diverse generation effects. Considering realistic scenarios, we design three kinds of combinations of face pairs. The recognition accuracy of SOTA models declines 5{\%}--16{\%} on MLFW database compared with the accuracy on the original images. MLFW database can be viewed and downloaded at http://whdeng.cn/mlfw.}
}
@article{watson2022systematic,
	title        = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research},
	author       = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
	year         = 2022,
	month        = {mar},
	journal      = {ACM Trans. Softw. Eng. Methodol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 31,
	number       = 2,
	doi          = {10.1145/3485275},
	issn         = {1049-331X},
	url          = {https://doi.org/10.1145/3485275},
	issue_date   = {April 2022},
	abstract     = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23&nbsp;unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.},
	articleno    = 32,
	numpages     = 58,
	keywords     = {Deep learning, neural networks, software engineering, literature review, machine learning}
}
@inproceedings{weinberger2005distance,
	title        = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
	author       = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
	year         = 2005,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = 18,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
	editor       = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt}
}
@article{wen2020adapting,
	title        = {{Adapting and evaluating a deep learning language model for clinical why-question answering}},
	author       = {Wen, Andrew and Elwazir, Mohamed Y and Moon, Sungrim and Fan, Jungwei},
	year         = 2020,
	month        = {02},
	journal      = {JAMIA Open},
	volume       = 3,
	number       = 1,
	pages        = {16--20},
	doi          = {10.1093/jamiaopen/ooz072},
	issn         = {2574-2531},
	url          = {https://doi.org/10.1093/jamiaopen/ooz072},
	abstract     = {{To adapt and evaluate a deep learning language model for answering why-questions based on patient-specific clinical text.Bidirectional encoder representations from transformers (BERT) models were trained with varying data sources to perform SQuAD 2.0 style why-question answering (why-QA) on clinical notes. The evaluation focused on: (1) comparing the merits from different training data and (2) error analysis.The best model achieved an accuracy of 0.707 (or 0.760 by partial match). Training toward customization for the clinical language helped increase 6\% in accuracy.The error analysis suggested that the model did not really perform deep reasoning and that clinical why-QA might warrant more sophisticated solutions.The BERT model achieved moderate accuracy in clinical why-QA and should benefit from the rapidly evolving technology. Despite the identified limitations, it could serve as a competent proxy for question-driven clinical information extraction.}},
	eprint       = {https://academic.oup.com/jamiaopen/article-pdf/3/1/16/33419137/ooz072.pdf}
}
@inproceedings{wen2020time,
	title        = {Time Series Data Augmentation for Deep Learning: A Survey},
	author       = {Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
	year         = 2021,
	month        = 8,
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {4653--4660},
	doi          = {10.24963/ijcai.2021/631},
	url          = {https://doi.org/10.24963/ijcai.2021/631},
	note         = {Survey Track},
	editor       = {Zhi-Hua Zhou}
}
@inproceedings{weng2010twitterrank,
	title        = {TwitterRank: Finding Topic-Sensitive Influential Twitterers},
	author       = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
	year         = 2010,
	booktitle    = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
	location     = {New York, New York, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WSDM '10},
	pages        = {261--270},
	doi          = {10.1145/1718487.1718520},
	isbn         = 9781605588896,
	url          = {https://doi.org/10.1145/1718487.1718520},
	abstract     = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4\% of the users in Twitter follow more than 80\% of their followers, and (2) 80.5\% of the users have 80\% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
	numpages     = 10,
	keywords     = {twitter, pagerank, influential}
}
@inproceedings{weng2018evaluating,
	title        = {Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},
	author       = {Tsui{-}Wei Weng and Huan Zhang and Pin{-}Yu Chen and Jinfeng Yi and Dong Su and Yupeng Gao and Cho{-}Jui Hsieh and Luca Daniel},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BkUHlMZ0b},
	timestamp    = {Sat, 31 Aug 2019 16:23:15 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/WengZCYSGHD18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{west2021misinformation,
	title        = {Misinformation in and about science},
	author       = {Jevin D. West  and Carl T. Bergstrom},
	year         = 2021,
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = 118,
	number       = 15,
	pages        = {e1912444117},
	doi          = {10.1073/pnas.1912444117},
	url          = {https://www.pnas.org/doi/abs/10.1073/pnas.1912444117},
	eprint       = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912444117},
	abstract     = {Humans learn about the world by collectively acquiring information, filtering it, and sharing what we know. Misinformation undermines this process. The repercussions are extensive. Without reliable and accurate sources of information, we cannot hope to halt climate change, make reasoned democratic decisions, or control a global pandemic. Most analyses of misinformation focus on popular and social media, but the scientific enterprise faces a parallel set of problems—from hype and hyperbole to publication bias and citation misdirection, predatory publishing, and filter bubbles. In this perspective, we highlight these parallels and discuss future research directions and interventions.}
}
@book{widdows2004geometry,
	title        = {Geometry and Meaning},
	author       = {Dominic Widdows},
	year         = 2004,
	publisher    = {{CSLI} Publications},
	series       = {{CSLI} lecture notes series},
	volume       = 172,
	isbn         = {978-1-57586-448-8},
	timestamp    = {Fri, 08 Nov 2013 14:46:31 +0100},
	biburl       = {https://dblp.org/rec/books/daglib/0031968.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{williams1959regression,
	title        = {Regression Analysis},
	author       = {Williams, E.J.},
	year         = 1959,
	publisher    = {Wiley},
	series       = {Wiley publication in applied statistics},
	url          = {https://books.google.co.in/books?id=uWkNogEACAAJ},
	lccn         = 59011815
}
@inproceedings{winkler1990string,
	title        = {String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage},
	author       = {Winkler, William E.},
	year         = 1990,
	booktitle    = {Proceedings of the Section on Survey Research},
	location     = {Wachington, DC},
	publisher    = {ERIC},
	pages        = {354--359},
	url          = {https://eric.ed.gov/?id=ED325505},
	added-at     = {2006-02-14T15:21:42.000+0100},
	interhash    = {15e98ae8cdc43683f96562982e68f1b8},
	intrahash    = {a2dcfaedc01b2ffcab7afaffc6bd03bc},
	keywords     = {purge sunters information felligi integration merge},
	timestamp    = {2006-02-14T15:21:42.000+0100}
}
@article{wise1996yap3,
	title        = {YAP3: Improved Detection of Similarities in Computer Program and Other Texts},
	author       = {Wise, Michael J.},
	year         = 1996,
	month        = {mar},
	journal      = {SIGCSE Bull.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 28,
	number       = 1,
	pages        = {130--134},
	doi          = {10.1145/236462.236525},
	issn         = {0097-8418},
	url          = {https://doi.org/10.1145/236462.236525},
	issue_date   = {March 1996},
	abstract     = {In spite of years of effort, plagiarism in student assignment submissions still causes considerable difficulties for course designers; if students' work is not their own, how can anyone be certain they have learnt anything? YAP is a system for detecting suspected plagiarism in computer programs and other texts submitted by students. The paper reviews YAP3, the third version of YAP, focusing on its novel underlying algorithm - Running-Karp-Rabin Greedy-String-Tiling (or RKS-GST), whose development arose from the observation with YAP and other systems that students shuffle independent code segments. YAP3 is able to detect transposed subsequences, and is less perturbed by spurious additional statements. The paper concludes with a discussion of recent extension of YAP to English texts, further illustrating the flexibility of the YAP approach.},
	numpages     = 5
}
@article{wiyatno2019adversarial,
	title        = {Adversarial Examples in Modern Machine Learning: {A} Review},
	author       = {Rey Reza Wiyatno and Anqi Xu and Ousmane Dia and Archy de Berker},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1911.05268},
	url          = {http://arxiv.org/abs/1911.05268},
	eprinttype   = {arXiv},
	eprint       = {1911.05268},
	timestamp    = {Mon, 02 Dec 2019 13:44:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1911-05268.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wong1987modeling,
	title        = {On modeling of information retrieval concepts in vector spaces},
	author       = {Wong, S. K. Michael and Ziarko, Wojciech and Raghavan, Vijay V. and Wong, Patrick CN},
	year         = 1987,
	journal      = {ACM Transactions on Database Systems (TODS)},
	publisher    = {ACM New York, NY, USA},
	volume       = 12,
	number       = 2,
	pages        = {299--321}
}
@article{wong2009atec,
	title        = {ATEC: automatic evaluation of machine translation via word choice and word order},
	author       = {Wong, Billy and Kit, Chunyu},
	year         = 2009,
	month        = {Sep},
	day          = {01},
	journal      = {Machine Translation},
	volume       = 23,
	number       = 2,
	pages        = {141--155},
	doi          = {10.1007/s10590-009-9061-x},
	issn         = {1573-0573},
	url          = {https://doi.org/10.1007/s10590-009-9061-x},
	abstract     = {We propose a novel metric ATEC for automatic MT evaluation based on explicit assessment of word choice and word order in an MT output in comparison to its reference translation(s), the two most fundamental factors in the construction of meaning for a sentence. The former is assessed by matching word forms at various linguistic levels, including surface form, stem, sound and sense, and further by weighing the informativeness of each word. The latter is quantified in term of the discordance of word position and word sequence between a translation candidate and its reference. In the evaluations using the MetricsMATR08 data set and the LDC MTC2 and MTC4 corpora, ATEC demonstrates an impressive positive correlation to human judgments at the segment level, highly comparable to the few state-of-the-art evaluation metrics.}
}
@inproceedings{wong2018provable,
	title        = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
	author       = {Wong, Eric and Kolter, Zico},
	year         = 2018,
	booktitle    = {International conference on machine learning},
	pages        = {5286--5295},
	organization = {PMLR}
}
@inproceedings{wong2020fast,
	title        = {Fast is better than free: Revisiting adversarial training},
	author       = {Eric Wong and Leslie Rice and J. Zico Kolter},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJx040EFvH},
	timestamp    = {Wed, 16 Dec 2020 15:31:31 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/WongRK20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{wouters1999citation,
	title        = {The citation culture},
	author       = {Wouters, Paulus Franciscus and others},
	year         = 1999,
	school       = {Universiteit van Amsterdam Amsterdam}
}
@article{wu2018light,
	title        = {A Light CNN for Deep Face Representation With Noisy Labels},
	author       = {Wu, Xiang and He, Ran and Sun, Zhenan and Tan, Tieniu},
	year         = 2018,
	journal      = {IEEE Transactions on Information Forensics and Security},
	volume       = 13,
	number       = 11,
	pages        = {2884--2896},
	doi          = {10.1109/TIFS.2018.2833032}
}
@article{xiao2017fashion,
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1708.07747},
	url          = {http://arxiv.org/abs/1708.07747},
	eprinttype   = {arXiv},
	eprint       = {1708.07747},
	timestamp    = {Mon, 13 Aug 2018 16:47:27 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xie2019improving,
	title        = {Improving Transferability of Adversarial Examples With Input Diversity},
	author       = {Xie, Cihang and Zhang, Zhishuai and Zhou, Yuyin and Bai, Song and Wang, Jianyu and Ren, Zhou and Yuille, Alan L.},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{xie2020adversarial,
	title        = {Adversarial examples improve image recognition},
	author       = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {819--828}
}
@inproceedings{xu2017feature,
	title        = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks},
	author       = {Weilin Xu and David Evans and Yanjun Qi},
	year         = 2018,
	booktitle    = {25th Annual Network and Distributed System Security Symposium, {NDSS} 2018, San Diego, California, USA, February 18-21, 2018},
	publisher    = {The Internet Society},
	url          = {http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018\_03A-4\_Xu\_paper.pdf},
	timestamp    = {Thu, 17 Jun 2021 16:04:48 +0200},
	biburl       = {https://dblp.org/rec/conf/ndss/Xu0Q18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xu2020automatic,
	title        = {Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond},
	author       = {Xu, Kaidi and Shi, Zhouxing and Zhang, Huan and Wang, Yihan and Chang, Kai-Wei and Huang, Minlie and Kailkhura, Bhavya and Lin, Xue and Hsieh, Cho-Jui},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1129--1141},
	url          = {https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{yang2012discriminative,
	title        = {Discriminative Feature Selection by Nonparametric Bayes Error Minimization},
	author       = {Yang, Shuang Hong and Hu, Bao-Gang},
	year         = 2012,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 24,
	number       = 8,
	pages        = {1422--1434},
	doi          = {10.1109/TKDE.2011.92}
}
@article{yang2018text,
	title        = {Text Mining of Twitter Data Using a Latent Dirichlet Allocation Topic Model and Sentiment Analysis},
	author       = {Sidi Yang and  Haiyi Zhang},
	year         = 2018,
	journal      = {International Journal of Computer and Information Engineering},
	publisher    = {World Academy of Science, Engineering and Technology},
	volume       = 12,
	number       = 7,
	pages        = {525--529},
	issn         = {eISSN: 1307-6892},
	url          = {https://publications.waset.org/vol/139},
	abstract     = {Twitter is a microblogging platform, where millions of users daily share their attitudes, views, and opinions. Using a probabilistic Latent Dirichlet Allocation (LDA) topic model to discern the most popular topics in the Twitter data is an effective way to analyze a large set of tweets to find a set of topics in a computationally efficient manner. Sentiment analysis provides an effective method to show the emotions and sentiments found in each tweet and an efficient way to summarize the results in a manner that is clearly understood. The primary goal of this paper is to explore text mining, extract and analyze useful information from unstructured text using two approaches: LDA topic modelling and sentiment analysis by examining Twitter plain text data in English. These two methods allow people to dig data more effectively and efficiently. LDA topic model and sentiment analysis can also be applied to provide insight views in business and scientific fields.},
	ee           = {https://publications.waset.org/pdf/10009246},
	bibsource    = {https://publications.waset.org/},
	index        = {Open Science Index 139, 2018}
}
@article{yang2019xlnet,
	title        = {Xlnet: Generalized autoregressive pretraining for language understanding},
	author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@article{yang2020closer,
	title        = {A closer look at accuracy vs. robustness},
	author       = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {8588--8601}
}
@article{yang2021characterizing,
	title        = {Characterizing the Evasion Attackability of Multi-label Classifiers},
	author       = {Yang, Zhuo and Han, Yufei and Zhang, Xiangliang},
	year         = 2021,
	month        = {May},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 12,
	pages        = {10647--10655},
	doi          = {10.1609/aaai.v35i12.17273},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17273},
	abstractnote = {Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator.}
}
@inproceedings{yang2021reachability,
	title        = {Reachability analysis of deep ReLU neural networks using facet-vertex incidence.},
	author       = {Yang, Xiaodong and Johnson, Taylor T and Tran, Hoang-Dung and Yamaguchi, Tomoya and Hoxha, Bardh and Prokhorov, Danil V},
	year         = 2021,
	booktitle    = {HSCC},
	volume       = 21,
	pages        = {19--21}
}
@inproceedings{yao2015incorporating,
	title        = {Incorporating Probabilistic Knowledge into Topic Models},
	author       = {Yao, Liang and Zhang, Yin and Wei, Baogang and Qian, Hongze and Wang, Yibing},
	year         = 2015,
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {586--597},
	isbn         = {978-3-319-18032-8},
	editor       = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
	abstract     = {Probabilistic Topic Models could be used to extract low-dimension aspects from document collections. However, such models without any human knowledge often produce aspects that are not interpretable. In recent years, a number of knowledge-based models have been proposed, which allow the user to input prior knowledge of the domain to produce more coherent and meaningful topics. In this paper, we incorporate human knowledge in the form of probabilistic knowledge base into topic models. By combining latent Dirichlet allocation, a widely used topic model with Probase, a large-scale probabilistic knowledge base, we improve the semantic coherence significantly. Our evaluation results will demonstrate the effectiveness of our method.}
}
@inproceedings{yeung2017improved,
	title        = {Improved performance of face recognition using CNN with constrained triplet loss layer},
	author       = {Yeung, Henry Wing Fung and Li, Jiaxi and Chung, Yuk Ying},
	year         = 2017,
	booktitle    = {2017 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1948--1955},
	doi          = {10.1109/IJCNN.2017.7966089}
}
@inproceedings{yin2014two,
	title        = {Two Sides of a Coin: Separating Personal Communication and Public Dissemination Accounts in Twitter},
	author       = {Yin, Peifeng and Ram, Nilam and Lee, Wang-Chien and Tucker, Conrad and Khandelwal, Shashank and Salath{\'e}, Marcel},
	year         = 2014,
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {163--175},
	isbn         = {978-3-319-06608-0},
	editor       = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	abstract     = {There are millions of accounts in Twitter. In this paper, we categorize twitter accounts into two types, namely Personal Communication Account (PCA) and Public Dissemination Account (PDA). PCAs are accounts operated by individuals and are used to express that individual's thoughts and feelings. PDAs, on the other hand, refer to accounts owned by non-individuals such as companies, governments, etc. Generally, Tweets in PDA (i) disseminate a specific type of information (e.g., job openings, shopping deals, car accidents) rather than sharing an individual's personal life; and (ii) may be produced by non-human entities (e.g., bots). We aim to develop techniques for identifying PDAs so as to (i) facilitate social scientists to reduce ``noise'' in their study of human behaviors, and (ii) to index them for potential recommendation to users looking for specific types of information. Through analysis, we find these two types of accounts follow different temporal, spatial and textual patterns. Accordingly we develop probabilistic models based on these features to identify PDAs. We also conduct a series of experiments to evaluate those algorithms for cleaning the Twitter data stream.}
}
@inproceedings{yin2015answering,
	title        = {Answering questions with complex semantic constraints on open knowledge bases},
	author       = {Yin, Pengcheng and Duan, Nan and Kao, Ben and Bao, Junwei and Zhou, Ming},
	year         = 2015,
	booktitle    = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
	pages        = {1301--1310}
}
@misc{yu2022survey,
	title        = {Survey of Query-based Text Summarization},
	author       = {Hang Yu},
	year         = 2022,
	eprint       = {2211.11548},
	archiveprefix = {arXiv},
	primaryclass = {cs.IR}
}
@inproceedings{zagoruyko2016wide,
	title        = {Wide Residual Networks},
	author       = {Sergey Zagoruyko and Nikos Komodakis},
	year         = 2016,
	month        = {September},
	booktitle    = {Proceedings of the British Machine Vision Conference (BMVC)},
	publisher    = {BMVA Press},
	pages        = {87.1--87.12},
	doi          = {10.5244/C.30.87},
	isbn         = {1-901725-59-6},
	url          = {https://dx.doi.org/10.5244/C.30.87},
	articleno    = 87,
	numpages     = 12,
	editor       = {Richard C. Wilson, Edwin R. Hancock and William A. P. Smith}
}
@article{zeiler2012adadelta,
	title        = {{ADADELTA:} An Adaptive Learning Rate Method},
	author       = {Matthew D. Zeiler},
	year         = 2012,
	journal      = {CoRR},
	volume       = {abs/1212.5701},
	url          = {http://arxiv.org/abs/1212.5701},
	eprinttype   = {arXiv},
	eprint       = {1212.5701},
	timestamp    = {Mon, 13 Aug 2018 16:45:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1212-5701.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{zhang2004statistical,
	title        = {{Statistical behavior and consistency of classification methods based on convex risk minimization}},
	author       = {Tong Zhang},
	year         = 2004,
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 32,
	number       = 1,
	pages        = {56 -- 85},
	doi          = {10.1214/aos/1079120130},
	url          = {https://doi.org/10.1214/aos/1079120130},
	keywords     = {boosting, classification, consistency, kernel methods, large margin methods}
}
@article{zhang2007gene,
	title        = {Gene selection for classification of microarray data based on the Bayes error},
	author       = {Zhang, Ji-Gang and Deng, Hong-Wen},
	year         = 2007,
	month        = {Oct},
	day          = {03},
	journal      = {BMC Bioinformatics},
	volume       = 8,
	number       = 1,
	pages        = 370,
	doi          = {10.1186/1471-2105-8-370},
	issn         = {1471-2105},
	url          = {https://doi.org/10.1186/1471-2105-8-370},
	abstract     = {With DNA microarray data, selecting a compact subset of discriminative genes from thousands of genes is a critical step for accurate classification of phenotypes for, e.g., disease diagnosis. Several widely used gene selection methods often select top-ranked genes according to their individual discriminative power in classifying samples into distinct categories, without considering correlations among genes. A limitation of these gene selection methods is that they may result in gene sets with some redundancy and yield an unnecessary large number of candidate genes for classification analyses. Some latest studies show that incorporating gene to gene correlations into gene selection can remove redundant genes and improve classification accuracy.}
}
@inproceedings{zhang2017mixup,
	title        = {mixup: Beyond Empirical Risk Minimization},
	author       = {Hongyi Zhang and Moustapha Ciss{\'{e}} and Yann N. Dauphin and David Lopez{-}Paz},
	year         = 2018,
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=r1Ddp1-Rb},
	timestamp    = {Thu, 25 Jul 2019 14:25:50 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCDL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2018efficient,
	title        = {Efficient Neural Network Robustness Certification with General Activation Functions},
	author       = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	booktitle    = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	location     = {Montr\'{e}al, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'18},
	volume       = 31,
	pages        = {4944–4953},
	abstract     = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
	numpages     = 10
}
@inproceedings{zhang2019adacos,
	title        = {AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations},
	author       = {Zhang, Xiao and Zhao, Rui and Qiao, Yu and Wang, Xiaogang and Li, Hongsheng},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {10823--10832}
}
@article{zhang2019bertscore,
	title        = {BERTScore: Evaluating Text Generation with {BERT}},
	author       = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1904.09675},
	url          = {http://arxiv.org/abs/1904.09675},
	eprinttype   = {arXiv},
	eprint       = {1904.09675},
	timestamp    = {Wed, 03 Jun 2020 10:08:39 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09675.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2019limitations,
	title        = {The Limitations of Adversarial Training and the Blind-Spot Attack},
	author       = {Huan Zhang and Hongge Chen and Zhao Song and Duane S. Boning and Inderjit S. Dhillon and Cho{-}Jui Hsieh},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HylTBhA5tQ},
	timestamp    = {Wed, 02 Dec 2020 16:43:27 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCSBDH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2019measuring,
	title        = {Measuring Similarity between Brands Using Followers' Post in Social Media},
	author       = {Zhang, Yiwei and Wang, Xueting and Sakai, Yoshiaki and Yamasaki, Toshihiko},
	year         = 2020,
	booktitle    = {Proceedings of the ACM Multimedia Asia},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MMAsia '19},
	doi          = {10.1145/3338533.3366600},
	isbn         = 9781450368414,
	url          = {https://doi.org/10.1145/3338533.3366600},
	abstract     = {In this paper, we propose a new measure to estimate the similarity between brands via posts of brands' followers on social network services (SNS). Our method was developed with the intention of exploring the brands that customers are likely to jointly purchase. Nowadays, brands use social media for targeted advertising because influencing users' preferences can greatly affect the trends in sales. We assume that data on SNS allows us to make quantitative comparisons between brands. Our proposed algorithm analyzes the daily photos and hashtags posted by each brand's followers. By clustering them and converting them to histograms, we can calculate the similarity between brands. We evaluated our proposed algorithm with purchase logs, credit card information, and answers to the questionnaires. The experimental results show that the purchase data maintained by a mall or a credit card company can predict the co-purchase very well, but not the customer's willingness to buy products of new brands. On the other hand, our method can predict the users' interest on brands with a correlation value over 0.53, which is pretty high considering that such interest to brands are high subjective and individual dependent.},
	articleno    = 6,
	numpages     = 6,
	keywords     = {Brands, Computational Marketing, Hashtag, Social Media}
}
@inproceedings{zhang2019mixup,
	title        = {mixup: Beyond empirical risk minimization},
	author       = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations (ICLR)}
}
@inproceedings{zhang2019theoretically,
	title        = {Theoretically Principled Trade-off between Robustness and Accuracy},
	author       = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {7472--7482},
	url          = {https://proceedings.mlr.press/v97/zhang19p.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf},
	abstract     = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of &nbsp;2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L_2 perturbation distance.}
}
@finproceedings{zhang2019towards,
	title        = {Towards Stable and Efficient Training of Verifiably Robust Neural Networks},
	author       = {Huan Zhang and Hongge Chen and Chaowei Xiao and Sven Gowal and Robert Stanforth and Bo Li and Duane S. Boning and Cho{-}Jui Hsieh},
	year         = 2020,
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Skxuk1rFwB},
	timestamp    = {Fri, 04 Dec 2020 15:21:06 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCXGSLBH20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2020black,
	title        = {Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework},
	author       = {Zhang, Dinghuai and Ye, Mao and Gong, Chengyue and Zhu, Zhanxing and Liu, Qiang},
	year         = 2020,
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'20},
	isbn         = 9781713829546,
	abstract     = {Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for ℓ2 perturbation. We propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distributions and leverage it to design new families of non-Gaussian smoothing distributions that work more efficiently for different ℓp settings, including ℓ1, ℓ2 and ℓ∞ attacks. Our proposed methods achieve better certification results than previous works and provide a new perspective on randomized smoothing certification.},
	articleno    = 195,
	numpages     = 11
}
@inproceedings{zhang2020pegasus,
	title        = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive Summarization},
	author       = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
	year         = 2020,
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 119,
	pages        = {11328--11339},
	url          = {http://proceedings.mlr.press/v119/zhang20ae.html},
	timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
	biburl       = {https://dblp.org/rec/conf/icml/ZhangZSL20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{zhang2021understanding,
	title        = {Understanding deep learning (still) requires rethinking generalization},
	author       = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year         = 2021,
	month        = {feb},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	address      = {New York, NY, USA},
	volume       = 64,
	number       = 3,
	pages        = {107--115},
	doi          = {10.1145/3446776},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/3446776},
	issue_date   = {March 2021},
	abstract     = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.We interpret our experimental findings by comparison with traditional models.We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
	numpages     = 9
}
@inproceedings{zhang2023proa,
	title        = {PRoA: A Probabilistic Robustness Assessment Against Functional Perturbations},
	author       = {Zhang, Tianle and Ruan, Wenjie and Fieldsend, Jonathan E.},
	year         = 2023,
	booktitle    = {Machine Learning and Knowledge Discovery in Databases},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {154--170},
	isbn         = {978-3-031-26409-2},
	editor       = {Amini, Massih-Reza and Canu, St{\'e}phane and Fischer, Asja and Guns, Tias and Kralj Novak, Petra and Tsoumakas, Grigorios},
	abstract     = {In safety-critical deep learning applications robustness measurement is a vital pre-deployment phase. However, existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand, these methods attempt to claim that no perturbations can ``fool'' deep neural networks (DNNs), which may be too stringent in practice. On the other hand, existing works rigorously consider {\$}{\$}L{\_}p{\$}{\$}Lpbounded additive perturbations on the pixel space, although perturbations, such as colour shifting and geometric transformations, are more practically and frequently occurring in the real world. Thus, from the practical standpoint, we present a novel and general probabilistic robustness assessment method (PRoA) based on the adaptive concentration, and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model, i.e., the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations, and PRoA can scale well to various large-scale deep neural networks compared to existing state-of-the-art baselines. For the purpose of reproducibility, we release our tool on GitHub: https://github.com/TrustAI/PRoA.}
}
@inproceedings{zhao2011comparing,
	title        = {Comparing Twitter and Traditional Media Using Topic Models},
	author       = {Zhao, Wayne Xin and Jiang, Jing and Weng, Jianshu and He, Jing and Lim, Ee-Peng and Yan, Hongfei and Li, Xiaoming},
	year         = 2011,
	booktitle    = {Advances in Information Retrieval},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {338--349},
	isbn         = {978-3-642-20161-5},
	editor       = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
	abstract     = {Twitter as a new form of social media can potentially contain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter. We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into consideration topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for downstream IR or DM applications.}
}
@article{zhao2013beyond,
	title        = {Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications},
	author       = {Ming-Jie Zhao and Narayanan Edakunni and Adam Pocock and Gavin Brown},
	year         = 2013,
	journal      = {Journal of Machine Learning Research},
	volume       = 14,
	number       = 32,
	pages        = {1033--1090},
	url          = {http://jmlr.org/papers/v14/zhao13a.html}
}
@inproceedings{zhao2017gated,
	title        = {Gated Neural Network for Sentence Compression Using Linguistic Knowledge},
	author       = {Yang Zhao and Hajime Senuma and Xiaoyu Shen and Akiko Aizawa},
	year         = 2017,
	booktitle    = {Natural Language Processing and Information Systems - 22nd International Conference on Applications of Natural Language to Information Systems, {NLDB} 2017, Li{\`{e}}ge, Belgium, June 21-23, 2017, Proceedings},
	publisher    = {Springer},
	series       = {Lecture Notes in Computer Science},
	volume       = 10260,
	pages        = {480--491},
	doi          = {10.1007/978-3-319-59569-6\_56},
	url          = {https://doi.org/10.1007/978-3-319-59569-6\_56},
	editor       = {Flavius Frasincar and Ashwin Ittoo and Le Minh Nguyen and Elisabeth M{\'{e}}tais},
	timestamp    = {Tue, 24 Sep 2019 17:17:46 +0200},
	biburl       = {https://dblp.org/rec/conf/nldb/ZhaoSSA17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhao2020rdcface,
	title        = {RDCFace: Radial Distortion Correction for Face Recognition},
	author       = {Zhao, He and Ying, Xianghua and Shi, Yongjie and Tong, Xin and Wen, Jingsi and Zha, Hongbin},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{zheng2015towards,
	title        = {Towards Lifestyle Understanding: Predicting Home and Vacation Locations from User's Online Photo Collections},
	author       = {Zheng, Danning and Hu, Tianran and You, Quanzeng and Kautz, Henry and Luo, Jiebo},
	year         = 2021,
	month        = {Aug.},
	journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
	volume       = 9,
	number       = 1,
	pages        = {553--560},
	url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/14591},
	abstractnote = {&lt;p&gt; Semantic place labeling has been actively studied in the past few years due to its importance in understanding human mobility and lifestyle patterns. In the last decade, the rapid growth of geotagged multimedia data from online social networks provides a valuable opportunity to predict people's POI locations from temporal, spatial and visual cues. Among the massive amount of social media data, one important type of data is the geotagged web images from image-sharing websites. In this paper, we develop a reliable photo classifier based on the Convolutional Neutral Networks to classify the photo-taking scene of real-life photos. We then present a novel approach to home location and vacation locations prediction by fusing together the visual content of photos and the spatiotemporal features of people's mobility patterns. Using a well-trained classifier, we showed that the robust fusion of visual and spatiotemporal features achieves significant accuracy improvement over each of the features alone for both home and vacation detection. &lt;/p&gt;}
}
@article{zheng2018cross,
	title        = {Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments},
	author       = {Zheng, Tianyue and Deng, Weihong},
	year         = 2018,
	journal      = {Beijing University of Posts and Telecommunications, Tech. Rep},
	volume       = 5,
	pages        = 7
}
@article{zheng2018survey,
	title        = {A Survey of Location Prediction on Twitter},
	author       = {Zheng, Xin and Han, Jialong and Sun, Aixin},
	year         = 2018,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 30,
	number       = 9,
	pages        = {1652--1671},
	doi          = {10.1109/TKDE.2018.2807840}
}
@article{zhou2020semi,
	title        = {Semi-Supervised Trajectory Understanding with POI Attention for End-to-End Trip Recommendation},
	author       = {Zhou, Fan and Wu, Hantao and Trajcevski, Goce and Khokhar, Ashfaq and Zhang, Kunpeng},
	year         = 2020,
	month        = {feb},
	journal      = {ACM Trans. Spatial Algorithms Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3378890},
	issn         = {2374-0353},
	url          = {https://doi.org/10.1145/3378890},
	issue_date   = {June 2020},
	abstract     = {Trip planning/recommendation is an important task for a plethora of applications in urban settings (e.g., tourism, transportation, social outings), relying on services provided by Location-Based Social Networks (LBSN). To provide greater context-awareness in trajectory planning, LBSNs combine historical trajectories of users for generating various hand-crafted features—e.g., geo-tags of photos taken by tourists and textual characteristics derived from reviews. Those features are used to learn tourists' preferences, which are then used to generate a travel plan recommendation. However, many such features are extracted based on prior knowledge or empirical analysis specific to particular datasets, rendering the corresponding solutions not to be generalizable to diverse data sources. Thus, one important question for managing mobility is how to learn an accurate tour planning model based solely on POI visits or user check-ins and without the efforts of hand-crafted feature engineering. Inspired by recent successes of deep learning in sequence learning, we develop a solution to the tour planning problem based on the semi-supervised learning paradigm. An important aspect of our solution is that it does not involve any feature engineering. Specifically, we propose the Trip Recommendation method via trajectory Encoder and Decoder—a novel end-to-end approach encoding historical trajectories into vectors, while capturing both the intrinsic characteristics of individual POIs and the transition patterns among POIs. We also incorporate historical attention mechanism in our sequence-to-sequence trip recommendation task to improve the effectiveness. Experiments conducted on multiple publicly available LBSN datasets demonstrate significantly superior performance of our method.},
	articleno    = 13,
	numpages     = 25,
	keywords     = {attention mechanism, Trip recommendation, semi-supervised learning, recurrent neural networks, encoder-decoder}
}
@article{zhu2016computing,
	title        = {Computing Semantic Similarity of Concepts in Knowledge Graphs},
	author       = {Zhu, Ganggao and Iglesias, Carlos A.},
	year         = 2017,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 29,
	number       = 1,
	pages        = {72--85},
	doi          = {10.1109/TKDE.2016.2610428}
}
@inproceedings{ziegler2019fine,
	title        = {Fine-Grained Access Control in Industrial Internet of Things - Evaluating Outsourced Attribute-Based Encryption},
	author       = {Dominik Ziegler and Josef Sabongui and Gerald Palfinger},
	year         = 2019,
	booktitle    = {{ICT} Systems Security and Privacy Protection - 34th {IFIP} {TC} 11 International Conference, {SEC} 2019, Lisbon, Portugal, June 25-27, 2019, Proceedings},
	publisher    = {Springer},
	series       = {{IFIP} Advances in Information and Communication Technology},
	volume       = 562,
	pages        = {91--104},
	doi          = {10.1007/978-3-030-22312-0\_7},
	url          = {https://doi.org/10.1007/978-3-030-22312-0\_7},
	editor       = {Gurpreet Dhillon and Fredrik Karlsson and Karin Hedstr{\"{o}}m and Andr{\'{e}} Z{\'{u}}quete},
	timestamp    = {Wed, 17 Feb 2021 10:36:26 +0100},
	biburl       = {https://dblp.org/rec/conf/sec/ZieglerSP19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zou2007illumination,
	title        = {Illumination Invariant Face Recognition: A Survey},
	author       = {Zou, Xuan and Kittler, Josef and Messer, Kieron},
	year         = 2007,
	booktitle    = {2007 First IEEE International Conference on Biometrics: Theory, Applications, and Systems},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/BTAS.2007.4401921}
}
@article{zubiaga2016analysing,
	title        = {Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads},
	author       = {Zubiaga, Arkaitz AND Liakata, Maria AND Procter, Rob AND Wong Sak Hoi, Geraldine AND Tolmie, Peter},
	year         = 2016,
	month        = {03},
	journal      = {PLOS ONE},
	publisher    = {Public Library of Science},
	volume       = 11,
	number       = 3,
	pages        = {1--29},
	doi          = {10.1371/journal.pone.0150989},
	url          = {https://doi.org/10.1371/journal.pone.0150989},
	abstract     = {As breaking news unfolds people increasingly rely on social media to stay abreast of the latest updates. The use of social media in such situations comes with the caveat that new information being released piecemeal may encourage rumours, many of which remain unverified long after their point of release. Little is known, however, about the dynamics of the life cycle of a social media rumour. In this paper we present a methodology that has enabled us to collect, identify and annotate a dataset of 330 rumour threads (4,842 tweets) associated with 9 newsworthy events. We analyse this dataset to understand how users spread, support, or deny rumours that are later proven true or false, by distinguishing two levels of status in a rumour life cycle i.e., before and after its veracity status is resolved. The identification of rumours associated with each event, as well as the tweet that resolved each rumour as true or false, was performed by journalist members of the research team who tracked the events in real time. Our study shows that rumours that are ultimately proven true tend to be resolved faster than those that turn out to be false. Whilst one can readily see users denying rumours once they have been debunked, users appear to be less capable of distinguishing true from false rumours when their veracity remains in question. In fact, we show that the prevalent tendency for users is to support every unverified rumour. We also analyse the role of different types of users, finding that highly reputable users such as news organisations endeavour to post well-grounded statements, which appear to be certain and accompanied by evidence. Nevertheless, these often prove to be unverified pieces of information that give rise to false rumours. Our study reinforces the need for developing robust machine learning techniques that can provide assistance in real time for assessing the veracity of rumours. The findings of our study provide useful insights for achieving this aim.}
}
@book{zwillinger1999crc,
	title        = {CRC standard probability and statistics tables and formulae},
	author       = {Zwillinger, Daniel and Kokoska, Stephen},
	year         = 1999,
	publisher    = {Crc Press}
}
