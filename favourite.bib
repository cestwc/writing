@inproceedings{abadi2016deep,
	title        = {Deep Learning with Differential Privacy},
	author       = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	year         = {2016},
	booktitle    = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	location     = {Vienna, Austria},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CCS '16},
	pages        = {308–318},
	doi          = {10.1145/2976749.2978318},
	isbn         = {9781450341394},
	url          = {https://doi.org/10.1145/2976749.2978318},
	abstract     = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
	numpages     = {11},
	keywords     = {deep learning, differential privacy}
}
@article{abadi2016tensorflow,
	title        = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	author       = {Mart{\'{\i}}n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Gregory S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian J. Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal J{\'{o}}zefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Man{\'{e}} and Rajat Monga and Sherry Moore and Derek Gordon Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul A. Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda B. Vi{\'{e}}gas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year         = {2016},
	journal      = {CoRR},
	volume       = {abs/1603.04467},
	url          = {http://arxiv.org/abs/1603.04467},
	eprinttype   = {arXiv},
	eprint       = {1603.04467},
	timestamp    = {Mon, 13 Aug 2018 16:47:09 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/AbadiABBCCCDDDG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{abate20072d,
	title        = {2D and 3D face recognition: A survey},
	author       = {Andrea F. Abate and Michele Nappi and Daniel Riccio and Gabriele Sabatino},
	year         = {2007},
	journal      = {Pattern Recognition Letters},
	volume       = {28},
	number       = {14},
	pages        = {1885--1906},
	doi          = {https://doi.org/10.1016/j.patrec.2006.12.018},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865507000189},
	note         = {Image: Information and Control},
	keywords     = {2D/3D face recognition, Face databases},
	abstract     = {Government agencies are investing a considerable amount of resources into improving security systems as result of recent terrorist events that dangerously exposed flaws and weaknesses in today's safety mechanisms. Badge or password-based authentication procedures are too easy to hack. Biometrics represents a valid alternative but they suffer of drawbacks as well. Iris scanning, for example, is very reliable but too intrusive; fingerprints are socially accepted, but not applicable to non-consentient people. On the other hand, face recognition represents a good compromise between what's socially acceptable and what's reliable, even when operating under controlled conditions. In last decade, many algorithms based on linear/nonlinear methods, neural networks, wavelets, etc. have been proposed. Nevertheless, Face Recognition Vendor Test 2002 shown that most of these approaches encountered problems in outdoor conditions. This lowered their reliability compared to state of the art biometrics. This paper provides an "ex cursus" of recent face recognition research trends in 2D imagery and 3D model based algorithms. To simplify comparisons across different approaches, tables containing different collection of parameters (such as input size, recognition rate, number of addressed problems) are provided. This paper concludes by proposing possible future directions.}
}
@article{adomavicius2005toward,
	title        = {Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions},
	author       = {Adomavicius, Gediminas and Tuzhilin, Alexander},
	year         = {2005},
	journal      = {IEEE transactions on knowledge and data engineering},
	publisher    = {IEEE},
	volume       = {17},
	number       = {6},
	pages        = {734--749}
}
@misc{adult_2,
	title        = {{Adult}},
	author       = {Becker, Barry and Kohavi, Ronny},
	year         = {1996},
	note         = {{DOI}: https://doi.org/10.24432/C5XW20},
	howpublished = {UCI Machine Learning Repository}
}
@inproceedings{agarwal2021trade,
	title        = {Trade-offs between fairness and privacy in machine learning},
	author       = {Agarwal, Sushant},
	year         = {2021},
	booktitle    = {IJCAI 2021 Workshop on AI for Social Good}
}
@inproceedings{aggarwal2019black,
	title        = {Black box fairness testing of machine learning models},
	author       = {Aggarwal, Aniya and Lohia, Pranay and Nagar, Seema and Dey, Kuntal and Saha, Diptikalyan},
	year         = {2019},
	booktitle    = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	location     = {Tallinn, Estonia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ESEC/FSE 2019},
	pages        = {625–635},
	doi          = {10.1145/3338906.3338937},
	isbn         = {9781450355728},
	url          = {https://doi.org/10.1145/3338906.3338937},
	abstract     = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
	numpages     = {11},
	keywords     = {Symbolic Execution, Local Explainability, Individual Discrimination, Fairness Testing}
}
@article{agresti1998approximate,
	title        = {Approximate is Better than "Exact" for Interval Estimation of Binomial Proportions},
	author       = {Alan   Agresti  and  Brent A.   Coull},
	year         = {1998},
	journal      = {The American Statistician},
	publisher    = {Taylor \& Francis},
	volume       = {52},
	number       = {2},
	pages        = {119--126},
	doi          = {10.1080/00031305.1998.10480550},
	url          = {https://doi.org/10.1080/00031305.1998.10480550},
	eprint       = {https://doi.org/10.1080/00031305.1998.10480550}
}
@article{aiello2013sensing,
	title        = {Sensing Trending Topics in Twitter},
	author       = {Aiello, Luca Maria and Petkos, Georgios and Martin, Carlos and Corney, David and Papadopoulos, Symeon and Skraba, Ryan and Göker, Ayse and Kompatsiaris, Ioannis and Jaimes, Alejandro},
	year         = {2013},
	journal      = {IEEE Transactions on Multimedia},
	volume       = {15},
	number       = {6},
	pages        = {1268--1282},
	doi          = {10.1109/TMM.2013.2265080}
}
@article{al2020ensemble,
	title        = {An Ensemble Deep Learning-Based Cyber-Attack Detection in Industrial Control System},
	author       = {Al-Abassi, Abdulrahman and Karimipour, Hadis and Dehghantanha, Ali and Parizi, Reza M.},
	year         = {2020},
	journal      = {IEEE Access},
	volume       = {8},
	number       = {},
	pages        = {83965--83973},
	doi          = {10.1109/ACCESS.2020.2992249}
}
@article{albarghouthi2017fairsquare,
	title        = {FairSquare: probabilistic verification of program fairness},
	author       = {Albarghouthi, Aws and D'Antoni, Loris and Drews, Samuel and Nori, Aditya V.},
	year         = {2017},
	month        = {oct},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {1},
	number       = {OOPSLA},
	doi          = {10.1145/3133904},
	url          = {https://doi.org/10.1145/3133904},
	issue_date   = {October 2017},
	abstract     = {With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that we aggressively investigate fairness and bias in decision-making programs. First, we show that a number of recently proposed formal definitions of fairness can be encoded as probabilistic program properties. Second, with the goal of enabling rigorous reasoning about fairness, we design a novel technique for verifying probabilistic properties that admits a wide class of decision-making programs. Third, we present FairSquare, the first verification tool for automatically certifying that a program meets a given fairness property. We evaluate FairSquare on a range of decision-making programs. Our evaluation demonstrates FairSquare’s ability to verify fairness for a range of different programs, which we show are out-of-reach for state-of-the-art program analysis techniques.},
	articleno    = {80},
	numpages     = {30},
	keywords     = {Algorithmic Fairness, Probabilistic Inference, Probabilistic Programming}
}
@article{alguliyev2019cosum,
	title        = {COSUM: Text summarization based on clustering and optimization},
	author       = {Alguliyev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R. and Abdi, Asad and Idris, Norisma},
	year         = {2019},
	journal      = {Expert Systems},
	volume       = {36},
	number       = {1},
	pages        = {e12340},
	doi          = {https://doi.org/10.1111/exsy.12340},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12340},
	note         = {e12340 EXSY-Jan-16-010.R2},
	keywords     = {adaptive differential evolution algorithm, content coverage, harmonic mean, information diversity, k-means, optimization model, sentence clustering, text summarization},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12340},
	abstract     = {Abstract Text summarization is a process of extracting salient information from a source text and presenting that information to the user in a condensed form while preserving its main content. In the text summarization, most of the difficult problems are providing wide topic coverage and diversity in a summary. Research based on clustering, optimization, and evolutionary algorithm for text summarization has recently shown good results, making this a promising area. In this paper, for a text summarization, a two-stage sentences selection model based on clustering and optimization techniques, called COSUM, is proposed. At the first stage, to discover all topics in a text, the sentences set is clustered by using k-means method. At the second stage, for selection of salient sentences from clusters, an optimization model is proposed. This model optimizes an objective function that expressed as a harmonic mean of the objective functions enforcing the coverage and diversity of the selected sentences in the summary. To provide readability of a summary, this model also controls length of sentences selected in the candidate summary. For solving the optimization problem, an adaptive differential evolution algorithm with novel mutation strategy is developed. The method COSUM was compared with the 14 state-of-the-art methods: DPSO-EDASum; LexRank; CollabSum; UnifiedRank; 0-1 non-linear; query, cluster, summarize; support vector machine; fuzzy evolutionary optimization model; conditional random fields; MA-SingleDocSum; NetSum; manifold ranking; ESDS-GHS-GLO; and differential evolution, using ROUGE tool kit on the DUC2001 and DUC2002 data sets. Experimental results demonstrated that COSUM outperforms the state-of-the-art methods in terms of ROUGE-1 and ROUGE-2 measures.}
}
@article{allamanis2018survey,
	title        = {A survey of machine learning for big code and naturalness},
	author       = {Allamanis, Miltiadis and Barr, Earl T and Devanbu, Premkumar and Sutton, Charles},
	year         = {2018},
	journal      = {ACM Computing Surveys (CSUR)},
	publisher    = {ACM New York, NY, USA},
	volume       = {51},
	number       = {4},
	pages        = {1--37}
}
@article{allerton1969sentence,
	title        = {The sentence as a linguistic unit},
	author       = {D.J. Allerton},
	year         = {1969},
	journal      = {Lingua},
	volume       = {22},
	pages        = {27--46},
	doi          = {https://doi.org/10.1016/0024-3841(69)90042-4},
	issn         = {0024-3841},
	url          = {https://www.sciencedirect.com/science/article/pii/0024384169900424}
}
@article{allison1986bit,
	title        = {A bit-string longest-common-subsequence algorithm},
	author       = {Lloyd Allison and Trevor I. Dix},
	year         = {1986},
	journal      = {Information Processing Letters},
	volume       = {23},
	number       = {5},
	pages        = {305--310},
	doi          = {https://doi.org/10.1016/0020-0190(86)90091-8},
	issn         = {0020-0190},
	url          = {https://www.sciencedirect.com/science/article/pii/0020019086900918},
	keywords     = {Longest common subsequence, edit distance, bit string},
	abstract     = {A longest-common-subsequence algorithm is described which operates in terms of bit or bit-string operations. It offers a speedup of the order of the word-length on a conventional computer.}
}
@article{alrajeh2020combining,
	title        = {Combining experts' causal judgments},
	author       = {Dalal Alrajeh and Hana Chockler and Joseph Y. Halpern},
	year         = {2020},
	journal      = {Artificial Intelligence},
	volume       = {288},
	pages        = {103355},
	doi          = {https://doi.org/10.1016/j.artint.2020.103355},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370220301065},
	keywords     = {Causality, Intervention, Combining causal judgments, Complexity},
	abstract     = {Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts' opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts' causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being compatible, and show how compatible causal models can be merged. We then use it as the basis for combining experts' causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples.}
}
@article{altinel2018semantic,
	title        = {Semantic text classification: A survey of past and recent advances},
	author       = {Berna Altınel and Murat Can Ganiz},
	year         = {2018},
	journal      = {Information Processing  \& Management},
	volume       = {54},
	number       = {6},
	pages        = {1129--1153},
	doi          = {https://doi.org/10.1016/j.ipm.2018.08.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457317305757},
	keywords     = {Text classification, Semantic text classification, Knowledge-based systems, Corpus-based systems, Neural language models, Deep learning},
	abstract     = {Automatic text classification is the task of organizing documents into pre-determined classes, generally using machine learning algorithms. Generally speaking, it is one of the most important methods to organize and make use of the gigantic amounts of information that exist in unstructured textual format. Text classification is a widely studied research area of language processing and text mining. In traditional text classification, a document is represented as a bag of words where the words in other words terms are cut from their finer context i.e. their location in a sentence or in a document. Only the broader context of document is used with some type of term frequency information in the vector space. Consequently, semantics of words that can be inferred from the finer context of its location in a sentence and its relations with neighboring words are usually ignored. However, meaning of words, semantic connections between words, documents and even classes are obviously important since methods that capture semantics generally reach better classification performances. Several surveys have been published to analyze diverse approaches for the traditional text classification methods. Most of these surveys cover application of different semantic term relatedness methods in text classification up to a certain degree. However, they do not specifically target semantic text classification algorithms and their advantages over the traditional text classification. In order to fill this gap, we undertake a comprehensive discussion of semantic text classification vs. traditional text classification. This survey explores the past and recent advancements in semantic text classification and attempts to organize existing approaches under five fundamental categories; domain knowledge-based approaches, corpus-based approaches, deep learning based approaches, word/character sequence enhanced approaches and linguistic enriched approaches. Furthermore, this survey highlights the advantages of semantic text classification algorithms over the traditional text classification algorithms.}
}
@unpublished{alvarez2017review,
	title        = {A review of word embedding and document similarity algorithms applied to academic text},
	author       = {Alvarez, Jon Ezeiza},
	year         = {2017},
	url          = {https://ad-publications.cs.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017}
}
@inproceedings{alzantot2019genattack,
	title        = {GenAttack: Practical Black-Box Attacks with Gradient-Free Optimization},
	author       = {Alzantot, Moustafa and Sharma, Yash and Chakraborty, Supriyo and Zhang, Huan and Hsieh, Cho-Jui and Srivastava, Mani B.},
	year         = {2019},
	booktitle    = {Proceedings of the Genetic and Evolutionary Computation Conference},
	location     = {Prague, Czech Republic},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {GECCO '19},
	pages        = {1111--1119},
	doi          = {10.1145/3321707.3321749},
	isbn         = {9781450361118},
	url          = {https://doi.org/10.1145/3321707.3321749},
	abstract     = {Deep neural networks are vulnerable to adversarial examples, even in the black-box setting, where the attacker is restricted solely to query access. Existing black-box approaches to generating adversarial examples typically require a significant number of queries, either for training a substitute network or performing gradient estimation. We introduce GenAttack, a gradient-free optimization technique that uses genetic algorithms for synthesizing adversarial examples in the black-box setting. Our experiments on different datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can successfully generate visually imperceptible adversarial examples against state-of-the-art image recognition models with orders of magnitude fewer queries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack required roughly 2,126 and 2,568 times fewer queries respectively, than ZOO, the prior state-of-the-art black-box attack. In order to scale up the attack to large-scale high-dimensional ImageNet models, we perform a series of optimizations that further improve the query efficiency of our attack leading to 237 times fewer queries against the Inception-v3 model than ZOO. Furthermore, we show that GenAttack can successfully attack some state-of-the-art ImageNet defenses, including ensemble adversarial training and non-differentiable or randomized input transformations. Our results suggest that evolutionary algorithms open up a promising area of research into effective black-box attacks.},
	numpages     = {9},
	keywords     = {deep learning, adversarial examples, computer vision, genetic algorithm}
}
@inproceedings{amati2021topic,
	title        = {Topic Modeling by Community Detection Algorithms},
	author       = {Amati, Giambattista and Angelini, Simone and Cruciani, Antonio and Fusco, Gianmarco and Gaudino, Giancarlo and Pasquini, Daniele and Vocca, Paola},
	year         = {2021},
	booktitle    = {Proceedings of the 2021 Workshop on Open Challenges in Online Social Networks},
	location     = {Virtual Event, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {OASIS '21},
	pages        = {15--20},
	doi          = {10.1145/3472720.3483622},
	isbn         = {9781450386326},
	url          = {https://doi.org/10.1145/3472720.3483622},
	abstract     = {We first estimate the number of Italian users active on Twitter in the last year by filtering the Italian flow of Twitter. We show that our filter misses about the 6.86\% of the Italian flow, while 86.80\% of the selected tweets belongs to the Italian language. Given this accuracy of the Italian Twitter's Firehose filter, we are able to assess the actual number of the Italian active users (AUs) of this platform. We then introduce a massive text document clustering algorithm that is easily applicable and scalable to the Twitter social network. Instead of a topic modeling approach based on features selection and any conventional clustering algorithm, such as LDA, we apply community detection algorithms on the weighted hashtag graph . In order to scale with the graph size, we apply two linear community detection algorithms, CoDA and Louvain. Once the hashtags have been assigned to clusters, both the most numerous clusters and hashtags were associated with topics of general interest, such as sports, politics, health etc. In this way we are able to provide significant statistics of the topics covered on Twitter in the past year.},
	numpages     = {6},
	keywords     = {data analysis, topic modelling, lda, community detection, twitter, louvain}
}
@article{amer2020set,
	title        = {A set theory based similarity measure for text clustering and classification},
	author       = {Amer, Ali A. and Abdalla, Hassan I.},
	year         = {2020},
	month        = {Sep},
	day          = {14},
	journal      = {Journal of Big Data},
	volume       = {7},
	number       = {1},
	pages        = {74},
	doi          = {10.1186/s40537-020-00344-3},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-020-00344-3},
	abstract     = {Similarity measures have long been utilized in information retrieval and machine learning domains for multi-purposes including text retrieval, text clustering, text summarization, plagiarism detection, and several other text-processing applications. However, the problem with these measures is that, until recently, there has never been one single measure recorded to be highly effective and efficient at the same time. Thus, the quest for an efficient and effective similarity measure is still an open-ended challenge. This study, in consequence, introduces a new highly-effective and time-efficient similarity measure for text clustering and classification. Furthermore, the study aims to provide a comprehensive scrutinization for seven of the most widely used similarity measures, mainly concerning their effectiveness and efficiency. Using the K-nearest neighbor algorithm (KNN) for classification, the K-means algorithm for clustering, and the bag of word (BoW) model for feature selection, all similarity measures are carefully examined in detail. The experimental evaluation has been made on two of the most popular datasets, namely, Reuters-21 and Web-KB. The obtained results confirm that the proposed set theory-based similarity measure (STB-SM), as a pre-eminent measure, outweighs all state-of-art measures significantly with regards to both effectiveness and efficiency.}
}
@article{amir2017sentence,
	title        = {Sentence similarity based on semantic kernels for intelligent text retrieval},
	author       = {Amir, Samir and Tanasescu, Adrian and Zighed, Djamel A.},
	year         = {2017},
	month        = {Jun},
	day          = {01},
	journal      = {Journal of Intelligent Information Systems},
	volume       = {48},
	number       = {3},
	pages        = {675--689},
	doi          = {10.1007/s10844-016-0434-3},
	issn         = {1573-7675},
	url          = {https://doi.org/10.1007/s10844-016-0434-3},
	abstract     = {We propose a new approach to compute semantic similarity between sentences. It is based on the semantic kernel, composed of subject, verb, and object that, we suppose, summarize the general meaning of each sentence. Thanks to linguistics resources available such as Stanford Parser, many features are then extracted from the semantic kernels and aggregated by mean of weights. The weighting is produced by a supervised machine learning technique on a training data set provided by human experts as ground truth. The cross validation shows good performances. Thanks to this similarity measure between sentences, one can build an intelligent text retrieval engine more sensitive to the semantic content, specifically suited for short texts than the classical methods based on bag of words. An application is being developed for highlighting parts of speech in scientific articles.}
}
@inproceedings{amir2021smt,
	title        = {An SMT-based approach for verifying binarized neural networks},
	author       = {Amir, Guy and Wu, Haoze and Barrett, Clark and Katz, Guy},
	year         = {2021},
	booktitle    = {Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27--April 1, 2021, Proceedings, Part II 27},
	pages        = {203--222},
	organization = {Springer}
}
@book{amsler1980structure,
	title        = {The structure of the Merriam-Webster pocket dictionary},
	author       = {Amsler, Robert Alfred},
	year         = {1980},
	publisher    = {The University of Texas at Austin}
}
@article{ananthakrishnan2007some,
	title        = {Some issues in automatic evaluation of english-hindi mt: more blues for bleu},
	author       = {Ananthakrishnan, R and Bhattacharyya, Pushpak and Sasikumar, M and Shah, Ritesh M},
	year         = {2007},
	journal      = {ICON},
	volume       = {64}
}
@inproceedings{anderson2016spice,
	title        = {SPICE: Semantic Propositional Image Caption Evaluation},
	author       = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	year         = {2016},
	booktitle    = {Computer Vision -- ECCV 2016},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {382--398},
	isbn         = {978-3-319-46454-1},
	editor       = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	abstract     = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?}
}
@article{Ando2005,
	title        = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	author       = {Ando, Rie Kubota and Zhang, Tong},
	year         = {2005},
	month        = dec,
	journal      = {Journal of Machine Learning Research},
	publisher    = {JMLR.org},
	volume       = {6},
	pages        = {1817--1853},
	issn         = {1532-4435},
	acmid        = {1194905},
	issue_date   = {12/1/2005},
	numpages     = {37}
}
@inproceedings{andrew2007scalable,
	title        = {Scalable training of {L1}-regularized log-linear models},
	author       = {Andrew, Galen and Gao, Jianfeng},
	year         = {2007},
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	pages        = {33--40}
}
@inproceedings{andriushchenko2020square,
	title        = {Square Attack: A Query-Efficient Black-Box Adversarial Attack via Random Search},
	author       = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
	year         = {2020},
	booktitle    = {Computer Vision -- ECCV 2020},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {484--501},
	isbn         = {978-3-030-58592-1},
	editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	abstract     = {We propose the Square Attack, a score-based black-box {\$}{\$}l{\_}2{\$}{\$}l2- and {\$}{\$}l{\_}{\backslash}infty {\$}{\$}l∞-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-of-the-art {\$}{\$}l{\_}{\backslash}infty {\$}{\$}l∞-attack of Al-Dujaili {\&} O'Reilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.}
}
@article{andriushchenko2020understanding,
	title        = {Understanding and improving fast adversarial training},
	author       = {Andriushchenko, Maksym and Flammarion, Nicolas},
	year         = {2020},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {33},
	pages        = {16048--16059}
}
@incollection{angwin2022machine,
	title        = {Machine bias},
	author       = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	year         = {2022},
	booktitle    = {Ethics of data and analytics},
	publisher    = {Auerbach Publications},
	pages        = {254--264}
}
@inproceedings{antol2015vqa,
	title        = {VQA: Visual Question Answering},
	author       = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
	year         = {2015},
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@book{antoniou2004semantic,
	title        = {A semantic web primer},
	author       = {Antoniou, Grigoris and Van Harmelen, Frank},
	year         = {2004},
	publisher    = {MIT press}
}
@article{antos1999lower,
	title        = {Lower bounds for Bayes error estimation},
	author       = {Antos, A. and Devroye, L. and Gyorfi, L.},
	year         = {1999},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {21},
	number       = {7},
	pages        = {643--645},
	doi          = {10.1109/34.777375}
}
@inproceedings{arora2017simple,
	title        = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
	author       = {Sanjeev Arora and Yingyu Liang and Tengyu Ma},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SyK00v5xx},
	timestamp    = {Sun, 08 Aug 2021 16:40:51 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/AroraLM17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{asuncion2007uci,
	title        = {{Bank Marketing}},
	author       = {Moro, S. and Rita, P. and Cortez, P.},
	year         = {2014},
	note         = {{DOI}: https://doi.org/10.24432/C5K306},
	howpublished = {UCI Machine Learning Repository}
}
@article{asyrofi2021biasfinder,
	title        = {BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems},
	author       = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Yusuf, Imam Nur Bani and Kang, Hong Jin and Thung, Ferdian and Lo, David},
	year         = {2022},
	journal      = {IEEE Transactions on Software Engineering},
	volume       = {48},
	number       = {12},
	pages        = {5087--5101},
	doi          = {10.1109/TSE.2021.3136169}
}
@inproceedings{athalye2018obfuscated,
	title        = {Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples},
	author       = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	year         = {2018},
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {80},
	pages        = {274--283},
	url          = {https://proceedings.mlr.press/v80/athalye18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf},
	abstract     = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.}
}
@inproceedings{athalye2018synthesizing,
	title        = {Synthesizing Robust Adversarial Examples},
	author       = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	year         = {2018},
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {80},
	pages        = {284--293},
	url          = {https://proceedings.mlr.press/v80/athalye18b.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/athalye18b/athalye18b.pdf},
	abstract     = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.}
}
@misc{author2024irreduciblerobustness,
	title        = {Irreducible robustness error},
	author       = {Anonymous},
	year         = {2024},
	note         = {Accessed: 2024-01-19},
	howpublished = {\url{https://github.com/soumission-anonyme/irreducible-robustness-error}}
}
@article{awad2018moral,
	title        = {The Moral Machine experiment},
	author       = {Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
	year         = {2018},
	month        = {Nov},
	day          = {01},
	journal      = {Nature},
	volume       = {563},
	number       = {7729},
	pages        = {59--64},
	doi          = {10.1038/s41586-018-0637-6},
	issn         = {1476-4687},
	url          = {https://doi.org/10.1038/s41586-018-0637-6},
	abstract     = {With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondents' demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available.}
}
@article{azeem2019machine,
	title        = {Machine learning techniques for code smell detection: A systematic literature review and meta-analysis},
	author       = {Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing},
	year         = {2019},
	journal      = {Information and Software Technology},
	publisher    = {Elsevier},
	volume       = {108},
	pages        = {115--138}
}
@inproceedings{baader2024expressivity,
	title        = {Expressivity of Re{LU}-Networks under Convex Relaxations},
	author       = {Maximilian Baader and Mark Niklas Mueller and Yuhao Mao and Martin Vechev},
	year         = {2024},
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=awHTL3Hpto}
}
@inproceedings{badanidiyuru2024optimal,
	title        = {Optimal Unbiased Randomizers for Regression with Label Differential Privacy},
	author       = {Badanidiyuru Varadaraja, Ashwinkumar and Ghazi, Badih and Kamath, Pritish and Kumar, Ravi and Leeman, Ethan and Manurangsi, Pasin and Varadarajan, Avinash V and Zhang, Chiyuan},
	year         = {2023},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {36},
	pages        = {60226--60246},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/bd5d436621dd3ee728b11c067d32d488-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@article{bagheri2020etm,
	title        = {ETM: Enrichment by topic modeling for automated clinical sentence classification to detect patients' disease history},
	author       = {Bagheri, Ayoub and Sammani, Arjan and van der Heijden, Peter G. M. and Asselbergs, Folkert W. and Oberski, Daniel L.},
	year         = {2020},
	month        = {Oct},
	day          = {01},
	journal      = {Journal of Intelligent Information Systems},
	volume       = {55},
	number       = {2},
	pages        = {329--349},
	doi          = {10.1007/s10844-020-00605-w},
	issn         = {1573-7675},
	url          = {https://doi.org/10.1007/s10844-020-00605-w},
	abstract     = {Given the rapid rate at which text data are being digitally gathered in the medical domain, there is growing need for automated tools that can analyze clinical notes and classify their sentences in electronic health records (EHRs). This study uses EHR texts to detect patients' disease history from clinical sentences. However, in EHRs, sentences are less topic-focused and shorter than that in general domain, which leads to the sparsity of co-occurrence patterns and the lack of semantic features. To tackle this challenge, current approaches for clinical sentence classification are dependent on external information to improve classification performance. However, this is implausible owing to a lack of universal medical dictionaries. This study proposes the ETM (enrichment by topic modeling) algorithm, based on latent Dirichlet allocation, to smoothen the semantic representations of short sentences. The ETM enriches text representation by incorporating probability distributions generated by an unsupervised algorithm into it. It considers the length of the original texts to enhance representation by using an internal knowledge acquisition procedure. When it comes to clinical predictive modeling, interpretability improves the acceptance of the model. Thus, for clinical sentence classification, the ETM approach employs an initial TFiDF (term frequency inverse document frequency) representation, where we use the support vector machine and neural network algorithms for the classification task. We conducted three sets of experiments on a data set consisting of clinical cardiovascular notes from the Netherlands to test the sentence classification performance of the proposed method in comparison with prevalent approaches. The results show that the proposed ETM approach outperformed state-of-the-art baselines.}
}
@inproceedings{bahdanau2014neural,
	title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
	author       = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year         = {2015},
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1409.0473},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bai2021recent,
	title        = {Recent Advances in Adversarial Training for Adversarial Robustness},
	author       = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	year         = {2021},
	month        = {8},
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {4312--4321},
	doi          = {10.24963/ijcai.2021/591},
	url          = {https://doi.org/10.24963/ijcai.2021/591},
	note         = {Survey Track},
	editor       = {Zhi-Hua Zhou}
}
@inproceedings{bak2020improved,
	title        = {Improved Geometric Path Enumeration for Verifying ReLU Neural Networks},
	author       = {Bak, Stanley and Tran, Hoang-Dung and Hobbs, Kerianne and Johnson, Taylor T.},
	year         = {2020},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {66--96},
	isbn         = {978-3-030-53288-8},
	editor       = {Lahiri, Shuvendu K. and Wang, Chao},
	abstract     = {Neural networks provide quick approximations to complex functions, and have been increasingly used in perception as well as control tasks. For use in mission-critical and safety-critical applications, however, it is important to be able to analyze what a neural network can and cannot do. For feed-forward neural networks with ReLU activation functions, although exact analysis is NP-complete, recently-proposed verification methods can sometimes succeed.}
}
@article{balagani2007relationship,
	title        = {On the Relationship Between Dependence Tree Classification Error and Bayes Error Rate},
	author       = {Balagani, Kiran S. and Phoha, Vir V.},
	year         = {2007},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {29},
	number       = {10},
	pages        = {1866--1868},
	doi          = {10.1109/TPAMI.2007.1184}
}
@inproceedings{balunovic2020adversarial,
	title        = {Adversarial Training and Provable Defenses: Bridging the Gap},
	author       = {Mislav Balunovic and Martin T. Vechev},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SJxSDxrKDr},
	timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/BalunovicV20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{banerjee2003extended,
	title        = {Extended Gloss Overlaps as a Measure of Semantic Relatedness},
	author       = {Banerjee, Satanjeev and Pedersen, Ted},
	year         = {2003},
	booktitle    = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
	location     = {Acapulco, Mexico},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {IJCAI'03},
	pages        = {805--810},
	abstract     = {This paper presents a new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses). This measure is unique in that it extends the glosses of the concepts under consideration to include the glosses of other concepts to which they are related according to a given concept hierarchy. We show that this new measure reasonably correlates to human judgments. We introduce a new method of word sense disambiguation based on extended gloss overlaps, and demonstrate that it fares well on the SENSEVAL-2 lexical sample data.},
	numpages     = {6}
}
@misc{bank_marketing_222,
	title        = {{Bank Marketing}},
	author       = {Moro, S. and Rita, P. and Cortez, P.},
	year         = {2014},
	note         = {{DOI}: https://doi.org/10.24432/C5K306},
	howpublished = {UCI Machine Learning Repository}
}
@techreport{bar2015composing,
	title        = {Composing Measures for Computing Text Similarity},
	author       = {Daniel B{\"a}r and Torsten Zesch and Iryna Gurevych},
	year         = {2015},
	month        = {January},
	address      = {Darmstadt, Germany},
	url          = {http://tuprints.ulb.tu-darmstadt.de/4342/},
	language     = {en},
	keywords     = {Text Similarity Plagiarism Paraphrase Recognition},
	abstract     = {We present a comprehensive study of computing similarity between texts. We start from the observation that while the concept of similarity is well grounded in psychology, text similarity is much less well-defined in the natural language processing community. We thus define the notion of text similarity and distinguish it from related tasks such as textual entailment and near-duplicate detection. We then identify multiple text dimensions, i.e. characteristics inherent to texts that can be used to judge text similarity, for which we provide empirical evidence. We discuss state-of-the-art text similarity measures previously proposed in the literature, before continuing with a thorough discussion of common evaluation metrics and datasets. Based on the analysis, we devise an architecture which combines text similarity measures in a unified classification framework. We apply our system in two evaluation settings, for which it consistently outperforms prior work and competing systems: (a) an intrinsic evaluation in the context of the Semantic Textual Similarity Task as part of the Semantic Evaluation (SemEval) exercises, and (b) an extrinsic evaluation for the detection of text reuse. As a basis for future work, we introduce DKPro Similarity, an open source software package which streamlines the development of text similarity measures and complete experimental setups.}
}
@article{barbon2017artificial,
	title        = {Artificial and Natural Topic Detection in Online Social Networks},
	author       = {Barbon Jr, Sylvio and Tavares, Gabriel Marques and Kido, Guilherme Sakaji},
	year         = {2017},
	month        = {Mar.},
	journal      = {iSys - Brazilian Journal of Information Systems},
	volume       = {10},
	number       = {1},
	pages        = {80--98},
	doi          = {10.5753/isys.2017.329},
	url          = {https://sol.sbc.org.br/journals/index.php/isys/article/view/329},
	abstractnote = {Online Social Networks (OSNs), such as Twitter, offer attractive means of social interactions and communications, but also raise privacy and security issues. The OSNs provide valuable information to marketing and competitiveness based on users posts and opinions stored inside a huge volume of data from several themes, topics, and subjects. In order to mining the topics discussed on an OSN we present a novel application of Louvain method for TopicModeling based on communities detection in graphs by modularity. The proposed approach succeeded in finding topics in five different datasets composed of textual content from Twitter and Youtube. Another important contribution achieved was about the presence of texts posted by spammers. In this case, a particular behavior observed by graph community architecture (density and degree) allows the indication of a topic strength and the classification of it as natural or artificial. The later created by the spammers on OSNs.}
}
@article{barkman1969phytosociology,
	title        = {Phytosociology and ecology of cryptogamic epiphytes (including a taxonomic survey and description of their vegetation units in Europe).},
	author       = {Barkman, Jan Johannes and others},
	year         = {1969},
	journal      = {Phytosociology and ecology of cryptogamic epiphytes (including a taxonomic survey and description of their vegetation units in Europe).},
	publisher    = {Van Gorcum  \& Comp. NV}
}
@book{barocas2023fairness,
	title        = {Fairness and machine learning: Limitations and opportunities},
	author       = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year         = {2023},
	publisher    = {MIT press}
}
@article{baroni2009wacky,
	title        = {The WaCky wide web: a collection of very large linguistically processed web-crawled corpora},
	author       = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
	year         = {2009},
	month        = {Sep},
	day          = {01},
	journal      = {Language Resources and Evaluation},
	volume       = {43},
	number       = {3},
	pages        = {209--226},
	doi          = {10.1007/s10579-009-9081-4},
	issn         = {1574-0218},
	url          = {https://doi.org/10.1007/s10579-009-9081-4},
	abstract     = {This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.}
}
@book{barrett2018satisfiability,
	title        = {Satisfiability modulo theories},
	author       = {Barrett, Clark and Tinelli, Cesare},
	year         = {2018},
	publisher    = {Springer}
}
@article{barry2018alcohol,
	title        = {Alcohol Advertising on Twitter—A Topic Model},
	author       = {Adam E. Barry and Danny Valdez and Alisa A. Padon and Alex M. Russell},
	year         = {2018},
	journal      = {American Journal of Health Education},
	publisher    = {Routledge},
	volume       = {49},
	number       = {4},
	pages        = {256--263},
	doi          = {10.1080/19325037.2018.1473180},
	url          = {https://doi.org/10.1080/19325037.2018.1473180},
	eprint       = {https://doi.org/10.1080/19325037.2018.1473180}
}
@article{bassett2019maximum,
	title        = {Maximum a posteriori estimators as a limit of Bayes estimators},
	author       = {Bassett, Robert and Deride, Julio},
	year         = {2019},
	month        = {Mar},
	day          = {01},
	journal      = {Mathematical Programming},
	volume       = {174},
	number       = {1},
	pages        = {129--144},
	doi          = {10.1007/s10107-018-1241-0},
	issn         = {1436-4646},
	url          = {https://doi.org/10.1007/s10107-018-1241-0},
	abstract     = {Maximum a posteriori and Bayes estimators are two common methods of point estimation in Bayesian statistics. It is commonly accepted that maximum a posteriori estimators are a limiting case of Bayes estimators with 0--1 loss. In this paper, we provide a counterexample which shows that in general this claim is false. We then correct the claim that by providing a level-set condition for posterior densities such that the result holds. Since both estimators are defined in terms of optimization problems, the tools of variational analysis find a natural application to Bayesian point estimation.}
}
@inproceedings{bastani2016measuring,
	title        = {Measuring Neural Net Robustness with Constraints},
	author       = {Bastani, Osbert and Ioannou, Yani and Lampropoulos, Leonidas and Vytiniotis, Dimitrios and Nori, Aditya and Criminisi, Antonio},
	year         = {2016},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {29},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2016/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@article{bastani2019probabilistic,
	title        = {Probabilistic verification of fairness properties via concentration},
	author       = {Bastani, Osbert and Zhang, Xin and Solar-Lezama, Armando},
	year         = {2019},
	month        = {oct},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {3},
	number       = {OOPSLA},
	doi          = {10.1145/3360544},
	url          = {https://doi.org/10.1145/3360544},
	issue_date   = {October 2019},
	abstract     = {As machine learning systems are increasingly used to make real world legal and financial decisions, it is of paramount importance that we develop algorithms to verify that these systems do not discriminate against minorities. We design a scalable algorithm for verifying fairness specifications. Our algorithm obtains strong correctness guarantees based on adaptive concentration inequalities; such inequalities enable our algorithm to adaptively take samples until it has enough data to make a decision. We implement our algorithm in a tool called VeriFair, and show that it scales to large machine learning models, including a deep recurrent neural network that is more than five orders of magnitude larger than the largest previously-verified neural network. While our technique only gives probabilistic guarantees due to the use of random samples, we show that we can choose the probability of error to be extremely small.},
	articleno    = {118},
	numpages     = {27},
	keywords     = {fairness, machine learning, probabilistic verification}
}
@inproceedings{behjati2019universal,
	title        = {Universal Adversarial Attacks on Text Classifiers},
	author       = {Behjati, Melika and Moosavi-Dezfooli, Seyed-Mohsen and Baghshah, Mahdieh Soleymani and Frossard, Pascal},
	year         = {2019},
	booktitle    = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	volume       = {},
	number       = {},
	pages        = {7345--7349},
	doi          = {10.1109/ICASSP.2019.8682430}
}
@inproceedings{beimel2013private,
	title        = {Private Learning and Sanitization: Pure vs. Approximate Differential Privacy},
	author       = {Beimel, Amos and Nissim, Kobbi and Stemmer, Uri},
	year         = {2013},
	booktitle    = {Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {363--378},
	isbn         = {978-3-642-40328-6},
	editor       = {Raghavendra, Prasad and Raskhodnikova, Sofya and Jansen, Klaus and Rolim, Jos{\'e} D. P.},
	abstract     = {We compare the sample complexity of private learning and sanitization tasks under pure$\epsilon$-differential privacy [Dwork, McSherry, Nissim, and Smith TCC 2006] and approximate ($\epsilon$,$\delta$)-differential privacy [Dwork, Kenthapadi, McSherry, Mironov, and Naor EUROCRYPT 2006]. We show that the sample complexity of these tasks under approximate differential privacy can be significantly lower than that under pure differential privacy.}
}
@inproceedings{beller2014modern,
	title        = {Modern code reviews in open-source projects: Which problems do they fix?},
	author       = {Beller, Moritz and Bacchelli, Alberto and Zaidman, Andy and Juergens, Elmar},
	year         = {2014},
	booktitle    = {Proceedings of the 11th working conference on mining software repositories},
	pages        = {202--211}
}
@inproceedings{bender2021dangers,
	title        = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
	author       = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year         = {2021},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Virtual Event, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '21},
	pages        = {610–623},
	doi          = {10.1145/3442188.3445922},
	isbn         = {9781450383097},
	url          = {https://doi.org/10.1145/3442188.3445922},
	abstract     = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	numpages     = {14}
}
@article{benedetti2019computing,
	title        = {Computing inter-document similarity with Context Semantic Analysis},
	author       = {Fabio Benedetti and Domenico Beneventano and Sonia Bergamaschi and Giovanni Simonini},
	year         = {2019},
	journal      = {Information Systems},
	volume       = {80},
	pages        = {136--147},
	doi          = {https://doi.org/10.1016/j.is.2018.02.009},
	issn         = {0306-4379},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306437917301503},
	keywords     = {Knowledge base, Knowledge graph, Inter-document similarity, Similarity measures, Information Retrieval},
	abstract     = {We propose a novel knowledge-based technique for inter-document similarity computation, called Context Semantic Analysis (CSA). Several specialized approaches built on top of specific knowledge base (e.g. Wikipedia) exist in literature, but CSA differs from them because it is designed to be portable to any RDF knowledge base. In fact, our technique relies on a generic RDF knowledge base (e.g. DBpedia and Wikidata) to extract from it a Semantic Context Vector, a novel model for representing the context of a document, which is exploited by CSA to compute inter-document similarity effectively. Moreover, we show how CSA can be effectively applied in the Information Retrieval domain. Experimental results show that: (i) for the general task of inter-document similarity, CSA outperforms baselines built on top of traditional methods, and achieves a performance similar to the ones built on top of specific knowledge bases; (ii) for Information Retrieval tasks, enriching documents with context (i.e., employing the Semantic Context Vector model) improves the results quality of the state-of-the-art technique that employs such similar semantic enrichment.}
}
@article{bentley1975multidimensional,
	title        = {Multidimensional binary search trees used for associative searching},
	author       = {Bentley, Jon Louis},
	year         = {1975},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = {18},
	number       = {9},
	pages        = {509--517}
}
@inproceedings{benussi2022individual,
	title        = {Individual Fairness Guarantees for Neural Networks},
	author       = {Benussi, Elias and Patane', Andrea and Wicker, Matthew and Laurenti, Luca and Kwiatkowska, Marta},
	year         = {2022},
	month        = {7},
	booktitle    = {Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, {IJCAI-22}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {651--658},
	doi          = {10.24963/ijcai.2022/92},
	url          = {https://doi.org/10.24963/ijcai.2022/92},
	note         = {Main Track},
	editor       = {Lud De Raedt}
}
@article{berisha2015empirically,
	title        = {Empirically Estimable Classification Bounds Based on a Nonparametric Divergence Measure},
	author       = {Berisha, Visar and Wisler, Alan and Hero, Alfred O. and Spanias, Andreas},
	year         = {2016},
	journal      = {IEEE Transactions on Signal Processing},
	volume       = {64},
	number       = {3},
	pages        = {580--591},
	doi          = {10.1109/TSP.2015.2477805}
}
@inproceedings{berlioz2015applying,
	title        = {Applying differential privacy to matrix factorization},
	author       = {Berlioz, Arnaud and Friedman, Arik and Kaafar, Mohamed Ali and Boreli, Roksana and Berkovsky, Shlomo},
	year         = {2015},
	booktitle    = {Proceedings of the 9th ACM Conference on Recommender Systems},
	pages        = {107--114}
}
@article{bertossi2018datalog,
	title        = {Datalog: Bag Semantics via Set Semantics},
	author       = {Leopoldo E. Bertossi and Georg Gottlob and Reinhard Pichler},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1803.06445},
	url          = {http://arxiv.org/abs/1803.06445},
	eprinttype   = {arXiv},
	eprint       = {1803.06445},
	timestamp    = {Mon, 13 Aug 2018 16:46:24 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06445.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{bertsekas2003goldstein,
	title        = {On the Goldstein-Levitin-Polyak gradient projection method},
	author       = {Bertsekas, Dimitri P},
	year         = {2003},
	journal      = {IEEE Transactions on automatic control},
	publisher    = {IEEE},
	volume       = {21},
	number       = {2},
	pages        = {174--184}
}
@inproceedings{bevendorff2023smauc,
	title        = {SMAuC - The Scientific Multi-Authorship Corpus},
	author       = {Bevendorff, Janek and Sauer, Philipp and Gienapp, Lukas and Kircheis, Wolfgang and Körner, Erik and Stein, Benno and Potthast, Martin},
	year         = {2023},
	booktitle    = {2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	volume       = {},
	number       = {},
	pages        = {25--29},
	doi          = {10.1109/JCDL57899.2023.00013},
	keywords     = {Humanities;Annotations;Writing;Libraries;Authorship Analysis;Multi-Authorship}
}
@inproceedings{beyer1999nearest,
	title        = {When Is "Nearest Neighbor" Meaningful?},
	author       = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
	year         = {1999},
	booktitle    = {Database Theory --- ICDT'99},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {217--235},
	isbn         = {978-3-540-49257-3},
	editor       = {Beeri, Catriel and Buneman, Peter},
	abstract     = {We explore the effect of dimensionality on the "nearest neighbor" problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.}
}
@article{bhat2020deep,
	title        = {Deep LDA : A new way to topic model},
	author       = {Muzafar Rasool Bhat and Majid A Kundroo and Tanveer A Tarray and Basant Agarwal},
	year         = {2020},
	journal      = {Journal of Information and Optimization Sciences},
	publisher    = {Taylor  \& Francis},
	volume       = {41},
	number       = {3},
	pages        = {823--834},
	doi          = {10.1080/02522667.2019.1616911},
	url          = {https://doi.org/10.1080/02522667.2019.1616911},
	eprint       = {https://doi.org/10.1080/02522667.2019.1616911}
}
@inproceedings{bhattacharya2019survey,
	title        = {A survey on: Facial emotion recognition invariant to pose, illumination and age},
	author       = {Bhattacharya, Saswati and Gupta, Mousumi},
	year         = {2019},
	booktitle    = {2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP)},
	pages        = {1--6},
	organization = {IEEE}
}
@inproceedings{bird2006mining,
	title        = {Mining email social networks},
	author       = {Bird, Christian and Gourley, Alex and Devanbu, Prem and Gertz, Michael and Swaminathan, Anand},
	year         = {2006},
	booktitle    = {Proceedings of the 2006 international workshop on Mining software repositories},
	pages        = {137--143}
}
@article{bird2009natural,
	title        = {Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit - O'Reilly Media, Beijing, 2009, {ISBN} 978-0-596-51649-9},
	author       = {Wiebke Wagner},
	year         = {2010},
	journal      = {Lang. Resour. Evaluation},
	volume       = {44},
	number       = {4},
	pages        = {421--424},
	doi          = {10.1007/s10579-010-9124-x},
	url          = {https://doi.org/10.1007/s10579-010-9124-x},
	timestamp    = {Thu, 05 Mar 2020 12:05:16 +0100},
	biburl       = {https://dblp.org/rec/journals/lre/Wagner10.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{bishop2006pattern,
	title        = {Pattern recognition and machine learning},
	author       = {Bishop, Christopher M and Nasrabadi, Nasser M},
	year         = {2006},
	publisher    = {Springer},
	volume       = {4},
	isbn         = {9780387310732},
	url          = {https://link.springer.com/book/9780387310732},
	timestamp    = {Fri, 17 Jul 2020 16:12:42 +0200},
	biburl       = {https://dblp.org/rec/books/lib/Bishop07.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{biswas2023fairify,
	title        = {Fairify: Fairness Verification of Neural Networks},
	author       = {Biswas, Sumon and Rajan, Hridesh},
	year         = {2023},
	booktitle    = {2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
	volume       = {},
	number       = {},
	pages        = {1546--1558},
	doi          = {10.1109/ICSE48619.2023.00134},
	keywords     = {Computational modeling;Scalability;Neurons;Artificial neural networks;Production;Machine learning;Biological neural networks;fairness;verification;machine learning}
}
@article{bizer2009dbpedia,
	title        = {DBpedia - A crystallization point for the Web of Data},
	author       = {Christian Bizer and Jens Lehmann and Georgi Kobilarov and Sören Auer and Christian Becker and Richard Cyganiak and Sebastian Hellmann},
	year         = {2009},
	journal      = {Journal of Web Semantics},
	volume       = {7},
	number       = {3},
	pages        = {154--165},
	doi          = {https://doi.org/10.1016/j.websem.2009.07.002},
	issn         = {1570-8268},
	url          = {https://www.sciencedirect.com/science/article/pii/S1570826809000225},
	note         = {The Web of Data},
	keywords     = {Web of Data, Linked Data, Knowledge extraction, Wikipedia, RDF},
	abstract     = {The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.}
}
@article{blank2020pymoo,
	title        = {Pymoo: Multi-Objective Optimization in Python},
	author       = {Blank, Julian and Deb, Kalyanmoy},
	year         = {2020},
	journal      = {IEEE Access},
	volume       = {8},
	number       = {},
	pages        = {89497--89509},
	doi          = {10.1109/ACCESS.2020.2990567}
}
@inproceedings{blei2001latent,
	title        = {Latent Dirichlet Allocation},
	author       = {Blei, David and Ng, Andrew and Jordan, Michael},
	year         = {2001},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = {14},
	url          = {https://proceedings.neurips.cc/paper/2001/file/296472c9542ad4d4788d543508116cbc-Paper.pdf},
	editor       = {T. Dietterich and S. Becker and Z. Ghahramani}
}
@article{blei2003latent,
	title        = {Latent Dirichlet Allocation},
	author       = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	year         = {2003},
	journal      = {J. Mach. Learn. Res.},
	volume       = {3},
	pages        = {993--1022},
	url          = {http://jmlr.org/papers/v3/blei03a.html},
	timestamp    = {Wed, 10 Jul 2019 15:28:30 +0200},
	biburl       = {https://dblp.org/rec/journals/jmlr/BleiNJ03.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{blevins_2010,
	title        = {Topic modeling Martha Ballard's diary: Cameron Blevins},
	author       = {Blevins, Cameron},
	year         = {2010},
	month        = {Apr},
	journal      = {Cameron Blevins |},
	url          = {http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/}
}
@book{blitzstein2015introduction,
	title        = {Introduction to probability},
	author       = {Blitzstein, Joseph K and Hwang, Jessica},
	year         = {2015},
	publisher    = {Crc Press Boca Raton, FL}
}
@book{blitzstein2019introduction,
	title        = {Introduction to probability},
	author       = {Blitzstein, Joseph K and Hwang, Jessica},
	year         = {2019},
	publisher    = {Crc Press}
}
@article{blondel2008fast,
	title        = {Fast unfolding of communities in large networks},
	author       = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
	year         = {2008},
	month        = {oct},
	journal      = {Journal of Statistical Mechanics: Theory and Experiment},
	publisher    = {{IOP} Publishing},
	volume       = {2008},
	number       = {10},
	pages        = {P10008},
	doi          = {10.1088/1742-5468/2008/10/p10008},
	url          = {https://doi.org/10.1088/1742-5468/2008/10/p10008},
	abstract     = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.}
}
@article{blough2001perception,
	title        = {The perception of similarity},
	author       = {Blough, Donald S},
	year         = {2001},
	journal      = {Avian visual cognition},
	volume       = {6},
	pages        = {23--25},
	url          = {https://pigeon.psy.tufts.edu/avc/dblough/default.htm}
}
@inproceedings{bollegala2007measuring,
	title        = {Measuring Semantic Similarity between Words Using Web Search Engines},
	author       = {Bollegala, Danushka and Matsuo, Yutaka and Ishizuka, Mitsuru},
	year         = {2007},
	booktitle    = {Proceedings of the 16th International Conference on World Wide Web},
	location     = {Banff, Alberta, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '07},
	pages        = {757--766},
	doi          = {10.1145/1242572.1242675},
	isbn         = {9781595936547},
	url          = {https://doi.org/10.1145/1242572.1242675},
	numpages     = {10}
}
@article{borner2003visualizing,
	title        = {Visualizing knowledge domains},
	author       = {B{\"o}rner, Katy and Chen, Chaomei and Boyack, Kevin W},
	year         = {2003},
	journal      = {Annual review of information science and technology},
	volume       = {37},
	number       = {1},
	pages        = {179--255}
}
@book{bracewell1986fourier,
	title        = {The Fourier transform and its applications},
	author       = {Bracewell, Ronald Newbold and Bracewell, Ronald N},
	year         = {1986},
	publisher    = {McGraw-Hill New York},
	volume       = {31999}
}
@inproceedings{brand2002incremental,
	title        = {Incremental Singular Value Decomposition of Uncertain Data with Missing Values},
	author       = {Brand, Matthew},
	year         = {2002},
	booktitle    = {Computer Vision --- ECCV 2002},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {707--720},
	isbn         = {978-3-540-47969-7},
	editor       = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
	abstract     = {We introduce an incremental singular value decomposition (svd) of incomplete data. The svd is developed as data arrives, and can handle arbitrary missing/untrusted values, correlated uncertainty across rows or columns of the measurement matrix, and user priors. Since incomplete data does not uniquely specify an svd, the procedure selects one having minimal rank. For a dense p {\texttimes} q matrix of low rank r, the incremental method has time complexity O(pqr) and space complexity O((p + q)r)---better than highly optimized batch algorithms such as matlab's svd(). In cases of missing data, it produces factorings of lower rank and residual than batch svd algorithms applied to standard missing-data imputations. We show applications in computer vision and audio feature extraction. In computer vision, we use the incremental svd to develop an efficient and unusually robust subspace-estimating flow-based tracker, and to handle occlusions/missing points in structure-from-motion factorizations.}
}
@article{breiman1996bagging,
	title        = {Bagging predictors},
	author       = {Breiman, Leo},
	year         = {1996},
	month        = {Aug},
	day          = {01},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = {24},
	number       = {2},
	pages        = {123--140},
	doi          = {10.1007/BF00058655},
	issn         = {1573-0565},
	url          = {https://doi.org/10.1007/BF00058655},
	abstract     = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.}
}
@inproceedings{brendel2017decision,
	title        = {Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models},
	author       = {Wieland Brendel and Jonas Rauber and Matthias Bethge},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SyZI0GWCZ},
	timestamp    = {Thu, 25 Jul 2019 14:25:46 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/BrendelRB18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{brilhante2015planning,
	title        = {On planning sightseeing tours with TripBuilder},
	author       = {Igo Ramalho Brilhante and Jose Antonio Macedo and Franco Maria Nardini and Raffaele Perego and Chiara Renso},
	year         = {2015},
	journal      = {Information Processing  \& Management},
	volume       = {51},
	number       = {2},
	pages        = {1--15},
	doi          = {https://doi.org/10.1016/j.ipm.2014.10.003},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457314000922},
	keywords     = {Recommender systems, Trajectory mining, Sightseeing tours},
	abstract     = {We propose TripBuilder, an unsupervised framework for planning personalized sightseeing tours in cities. We collect categorized Points of Interests (PoIs) from Wikipedia and albums of geo-referenced photos from Flickr. By considering the photos as traces revealing the behaviors of tourists during their sightseeing tours, we extract from photo albums spatio-temporal information about the itineraries made by tourists, and we match these itineraries to the Points of Interest (PoIs) of the city. The task of recommending a personalized sightseeing tour is modeled as an instance of the Generalized Maximum Coverage (GMC) problem, where a measure of personal interest for the user given her preferences and visiting time-budget is maximized. The set of actual trajectories resulting from the GMC solution is scheduled on the tourist's agenda by exploiting a particular instance of the Traveling Salesman Problem (TSP). Experimental results on three different cities show that our approach is effective, efficient and outperforms competitive baselines.}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = {2020},
	journal      = {Advances in neural information processing systems},
	volume       = {33},
	pages        = {1877--1901}
}
@article{brown2023detecting,
	title        = {Detecting shortcut learning for fair medical AI using shortcut testing},
	author       = {Brown, Alexander and Tomasev, Nenad and Freyberg, Jan and Liu, Yuan and Karthikesalingam, Alan and Schrouff, Jessica},
	year         = {2023},
	month        = {Jul},
	day          = {18},
	journal      = {Nature Communications},
	volume       = {14},
	number       = {1},
	pages        = {4314},
	doi          = {10.1038/s41467-023-39902-7},
	issn         = {2041-1723},
	url          = {https://doi.org/10.1038/s41467-023-39902-7},
	abstract     = {Machine learning (ML) holds great promise for improving healthcare, but it is critical to ensure that its use will not propagate or amplify health disparities. An important step is to characterize the (un)fairness of ML models---their tendency to perform differently across subgroups of the population---and to understand its underlying mechanisms. One potential driver of algorithmic unfairness, shortcut learning, arises when ML models base predictions on improper correlations in the training data. Diagnosing this phenomenon is difficult as sensitive attributes may be causally linked with disease. Using multitask learning, we propose a method to directly test for the presence of shortcut learning in clinical ML systems and demonstrate its application to clinical tasks in radiology and dermatology. Finally, our approach reveals instances when shortcutting is not responsible for unfairness, highlighting the need for a holistic approach to fairness mitigation in medical AI.}
}
@inproceedings{bui2021generative,
	title        = {Generative Pre-training for Paraphrase Generation by Representing and Predicting Spans in Exemplars},
	author       = {Bui, Tien-Cuong and Le, Van-Duc and To, Hai-Thien and Cha, Sang Kyun},
	year         = {2021},
	booktitle    = {2021 IEEE International Conference on Big Data and Smart Computing (BigComp)},
	pages        = {83--90},
	organization = {IEEE}
}
@online{burke2022lotteries,
	title        = {Lotteries are the fairest route to prejudice-free hiring},
	author       = {Burke, Neil W.},
	year         = {2022},
	journal      = {Times Higher Education},
	url          = {https://www.timeshighereducation.com/blog/lotteries-are-fairest-route-prejudice-free-hiring},
	note         = {Accessed: 2024-09-15}
}
@article{burrows2015eras,
	title        = {The Eras and Trends of Automatic Short Answer Grading},
	author       = {Steven Burrows and Iryna Gurevych and Benno Stein},
	year         = {2015},
	journal      = {Int. J. Artif. Intell. Educ.},
	volume       = {25},
	number       = {1},
	pages        = {60--117},
	doi          = {10.1007/s40593-014-0026-8},
	url          = {https://doi.org/10.1007/s40593-014-0026-8},
	timestamp    = {Tue, 29 Sep 2020 10:56:29 +0200},
	biburl       = {https://dblp.org/rec/journals/aiedu/BurrowsGS15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{busa2021pitfalls,
	title        = {On the Pitfalls of Label Differential Privacy},
	author       = {Andres Munoz medina and Robert Istvan Busa-Fekete and Umar Syed and Sergei Vassilvitskii},
	year         = {2021},
	booktitle    = {NeurIPS 2021 Workshop LatinX in AI},
	url          = {https://openreview.net/forum?id=2sWidqliCDG}
}
@inproceedings{busa2023label,
	title        = {Label differential privacy and private training data release},
	author       = {Busa-Fekete, Robert Istvan and Munoz Medina, Andres and Syed, Umar and Vassilvitskii, Sergei},
	year         = {2023},
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {202},
	pages        = {3233--3251},
	url          = {https://proceedings.mlr.press/v202/busa-fekete23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/busa-fekete23a/busa-fekete23a.pdf},
	abstract     = {We study differentially private mechanisms for sharing training data in machine learning settings. Our goal is to enable learning of an accurate predictive model while protecting the privacy of each user’s label. Previous work established privacy guarantees that assumed the features are public and given exogenously, a setting known as label differential privacy. In some scenarios, this can be a strong assumption that removes the interplay between features and labels from the privacy analysis. We relax this approach and instead assume the features are drawn from a distribution that depends on the private labels. We first show that simply adding noise to the label, as in previous work, can lead to an arbitrarily weak privacy guarantee, and also present methods for estimating this privacy loss from data. We then present a new mechanism that replaces some training examples with synthetically generated data, and show that our mechanism has a much better privacy-utility tradeoff if the synthetic data is ‘realistic’, in a certain quantifiable sense. Finally, we empirically validate our theoretical analysis.}
}
@article{camacho2016nasari,
	title        = {Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities},
	author       = {José Camacho-Collados and Mohammad Taher Pilehvar and Roberto Navigli},
	year         = {2016},
	journal      = {Artificial Intelligence},
	volume       = {240},
	pages        = {36--64},
	doi          = {https://doi.org/10.1016/j.artint.2016.07.005},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370216300820},
	keywords     = {Semantic representation, Lexical semantics, Word Sense Disambiguation, Semantic similarity, Sense clustering, Domain labeling},
	abstract     = {Owing to the need for a deep understanding of linguistic items, semantic representation is considered to be one of the fundamental components of several applications in Natural Language Processing and Artificial Intelligence. As a result, semantic representation has been one of the prominent research areas in lexical semantics over the past decades. However, due mainly to the lack of large sense-annotated corpora, most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses. In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively. Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/. As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.}
}
@article{camacho2018word,
	title        = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning},
	author       = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
	year         = {2018},
	month        = {sep},
	journal      = {J. Artif. Int. Res.},
	publisher    = {AI Access Foundation},
	address      = {El Segundo, CA, USA},
	volume       = {63},
	number       = {1},
	pages        = {743--788},
	doi          = {10.1613/jair.1.11259},
	issn         = {1076-9757},
	url          = {https://doi.org/10.1613/jair.1.11259},
	issue_date   = {September 2018},
	abstract     = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
	numpages     = {46}
}
@article{cancedda2003word,
	title        = {Word Sequence Kernels},
	author       = {Cancedda, Nicola and Gaussier, Eric and Goutte, Cyril and Renders, Jean Michel},
	year         = {2003},
	month        = {mar},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = {3},
	number       = {null},
	pages        = {1059--1082},
	issn         = {1532-4435},
	issue_date   = {3/1/2003},
	abstract     = {We address the problem of categorising documents using kernel-based methods such as Support Vector Machines. Since the work of Joachims (1998), there is ample experimental evidence that SVM using the standard word frequencies as features yield state-of-the-art performance on a number of benchmark problems. Recently, Lodhi et al. (2002) proposed the use of string kernels, a novel way of computing document similarity based of matching non-consecutive subsequences of characters. In this article, we propose the use of this technique with sequences of words rather than characters. This approach has several advantages, in particular it is more efficient computationally and it ties in closely with standard linguistic pre-processing techniques. We present some extensions to sequence kernels dealing with symbol-dependent and match-dependent decay factors, and present empirical evaluations of these extensions on the Reuters-21578 datasets.},
	numpages     = {24}
}
@article{canonne2022short,
	title        = {A short note on an inequality between KL and TV},
	author       = {Canonne, Cl{\'e}ment L},
	year         = {2022},
	journal      = {arXiv preprint arXiv:2202.07198}
}
@inproceedings{cao2015inferring,
	title        = {Inferring crowd-sourced venues for tweets},
	author       = {Cao, Bokai and Chen, Francine and Joshi, Dhiraj and Yu, Philip S.},
	year         = {2015},
	booktitle    = {2015 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {639--648},
	doi          = {10.1109/BigData.2015.7363808}
}
@inproceedings{cao2017joint,
	title        = {Joint copying and restricted generation for paraphrase},
	author       = {Cao, Ziqiang and Luo, Chuwei and Li, Wenjie and Li, Sujian},
	year         = {2017},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {31},
	number       = {1}
}
@inproceedings{cao2017mitigating,
	title        = {Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification},
	author       = {Xiaoyu Cao and Neil Zhenqiang Gong},
	year         = {2017},
	booktitle    = {Proceedings of the 33rd Annual Computer Security Applications Conference, Orlando, FL, USA, December 4-8, 2017},
	publisher    = {{ACM}},
	pages        = {278--287},
	doi          = {10.1145/3134600.3134606},
	url          = {https://doi.org/10.1145/3134600.3134606},
	timestamp    = {Tue, 06 Nov 2018 16:59:23 +0100},
	biburl       = {https://dblp.org/rec/conf/acsac/CaoG17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{carlini2017adversarial,
	title        = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = {2017},
	booktitle    = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {3--14},
	isbn         = {9781450352024},
	url          = {https://doi.org/10.1145/3128572.3140444},
	abstract     = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	numpages     = {12}
}
@inproceedings{carlini2017towards,
	title        = {Towards Evaluating the Robustness of Neural Networks},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = {2017},
	booktitle    = {2017 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {39--57},
	doi          = {10.1109/SP.2017.49}
}
@article{carlini2019evaluating,
	title        = {On Evaluating Adversarial Robustness},
	author       = {Nicholas Carlini and Anish Athalye and Nicolas Papernot and Wieland Brendel and Jonas Rauber and Dimitris Tsipras and Ian J. Goodfellow and Aleksander Madry and Alexey Kurakin},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1902.06705},
	url          = {http://arxiv.org/abs/1902.06705},
	eprinttype   = {arXiv},
	eprint       = {1902.06705},
	timestamp    = {Tue, 21 May 2019 18:03:36 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1902-06705.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{carlson2003building,
	title        = {Building a discourse-tagged corpus in the framework of rhetorical structure theory},
	author       = {Carlson, Lynn and Marcu, Daniel and Okurowski, Mary Ellen},
	year         = {2003},
	booktitle    = {Current and new directions in discourse and dialogue},
	publisher    = {Springer},
	pages        = {85--112}
}
@inproceedings{caron2021unsupervised,
	title        = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
	author       = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year         = {2020},
	booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)}
}
@inproceedings{caruana2006empirical,
	title        = {An empirical comparison of supervised learning algorithms},
	author       = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	year         = {2006},
	booktitle    = {Proceedings of the 23rd international conference on Machine learning},
	pages        = {161--168}
}
@article{caton2024fairness,
	title        = {Fairness in Machine Learning: A Survey},
	author       = {Caton, Simon and Haas, Christian},
	year         = {2024},
	month        = {apr},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {56},
	number       = {7},
	doi          = {10.1145/3616865},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3616865},
	issue_date   = {July 2024},
	abstract     = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.},
	articleno    = {166},
	numpages     = {38},
	keywords     = {Fairness, accountability, transparency, machine learning}
}
@article{cer2018universal,
	title        = {Universal Sentence Encoder},
	author       = {Daniel Cer and Yinfei Yang and Sheng{-}yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St. John and Noah Constant and Mario Guajardo{-}Cespedes and Steve Yuan and Chris Tar and Yun{-}Hsuan Sung and Brian Strope and Ray Kurzweil},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1803.11175},
	url          = {http://arxiv.org/abs/1803.11175},
	eprinttype   = {arXiv},
	eprint       = {1803.11175},
	timestamp    = {Mon, 13 Aug 2018 16:46:40 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-11175.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{chakraborty2018adversarial,
	title        = {A survey on adversarial attacks and defences},
	author       = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year         = {2021},
	journal      = {CAAI Transactions on Intelligence Technology},
	volume       = {6},
	number       = {1},
	pages        = {25--45},
	doi          = {https://doi.org/10.1049/cit2.12028},
	url          = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12028},
	keywords     = {security of data, deep learning (artificial intelligence)},
	eprint       = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12028},
	abstract     = {Abstract Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human-level performance. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.}
}
@article{chandrasekaran2021evolution,
	title        = {Evolution of Semantic Similarity—A Survey},
	author       = {Chandrasekaran, Dhivya and Mago, Vijay},
	year         = {2021},
	month        = {feb},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {54},
	number       = {2},
	doi          = {10.1145/3440755},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3440755},
	issue_date   = {March 2022},
	abstract     = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
	articleno    = {41},
	numpages     = {37},
	keywords     = {word embeddings, supervised and unsupervised methods, linguistics, knowledge-based methods, corpus-based methods, Semantic similarity}
}
@article{chang2010training,
	title        = {Training and testing low-degree polynomial data mappings via linear svm.},
	author       = {Chang, Yin-Wen and Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and others},
	year         = {2010},
	journal      = {Journal of Machine Learning Research},
	volume       = {11},
	number       = {4}
}
@article{chaudhuri2011differentially,
	title        = {Differentially Private Empirical Risk Minimization},
	author       = {Kamalika Chaudhuri and Claire Monteleoni and Anand D. Sarwate},
	year         = {2011},
	journal      = {Journal of Machine Learning Research},
	volume       = {12},
	number       = {29},
	pages        = {1069--1109},
	url          = {http://jmlr.org/papers/v12/chaudhuri11a.html}
}
@inproceedings{chaudhuri2011sample,
	title        = {Sample Complexity Bounds for Differentially Private Learning},
	author       = {Chaudhuri, Kamalika and Hsu, Daniel},
	year         = {2011},
	month        = {09--11 Jun},
	booktitle    = {Proceedings of the 24th Annual Conference on Learning Theory},
	publisher    = {PMLR},
	address      = {Budapest, Hungary},
	series       = {Proceedings of Machine Learning Research},
	volume       = {19},
	pages        = {155--186},
	url          = {https://proceedings.mlr.press/v19/chaudhuri11a.html},
	editor       = {Kakade, Sham M. and von Luxburg, Ulrike},
	pdf          = {http://proceedings.mlr.press/v19/chaudhuri11a/chaudhuri11a.pdf},
	abstract     = {This work studies the problem of privacy-preserving classification – namely, learning a classifier from sensitive data while preserving the privacy of individuals in the training set. In particular, the learning algorithm is required in this problem to guarantee differential privacy, a very strong notion of privacy that has gained significant attention in recent years. A natural question to ask is: what is the sample requirement of a learning algorithm that guarantees a certain level of privacy and accuracy? We address this question in the context of learning with infinite hypothesis classes when the data is drawn from a continuous distribution. We first show that even for very simple hypothesis classes, any algorithmth at uses a finite number of examples and guarantees differential privacy must fail to return an accurate classifier for at least some unlabeled data distributions. This result is unlike the case with either finite hypothesis classes or discrete data domains, in which distribution-free private learning is possible, as previously shown by Kasiviswanathan et al. (2008). We then consider two approaches to differentially private learning that get around this lower bound. The first approach is to use prior knowledge about the unlabeled data distribution in the form of a reference distribution $\mathcal{U}$ chosen independently of the sensitive data. Given such a reference $\mathcal{U}$, we provide an upper bound on the sample requirement that depends (among other things) on a measure of closeness between $\mathcal{U}$ and the unlabeled data distribution. Our upper bound applies to the non-realizable as well as the realizable case. The second approach is to relax the privacy requirement, by requiring only label-privacy –namely, that the only labels (and not the unlabeled parts of the examples)be considered sensitive information. An upper bound on the sample requirement of learning with label privacy was shown by Chaudhuri et al. (2006); in this work, we show a lower bound.}
}
@article{chebyshev1867valeurs,
	title        = {Des valeurs moyennes},
	author       = {Chebyshev, Pafnutii Lvovich},
	year         = {1867},
	journal      = {J. Math. Pures Appl},
	volume       = {12},
	number       = {2},
	pages        = {177--184}
}
@inproceedings{chekuri2023integrated,
	title        = {Integrated Digital Library System for Long Documents and their Elements},
	author       = {Chekuri, Satvik and Chandrasekar, Prashant and Banerjee, Bipasha and Park, Sung Hee and Masrourisaadat, Nila and Ahuja, Aman and Ingram, William A. and Fox, Edward A.},
	year         = {2023},
	booktitle    = {2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	volume       = {},
	number       = {},
	pages        = {13--24},
	doi          = {10.1109/JCDL57899.2023.00012},
	keywords     = {Deep learning;Visualization;Analytical models;Education;Information retrieval;Libraries;Natural language processing;Document representation;Retrieval tasks and goals;Digital Library;Information System;Information Retrieval;Deep Learning;NLP}
}
@inproceedings{chen2013emerging,
	title        = {Emerging Topic Detection for Organizations from Microblogs},
	author       = {Chen, Yan and Amiri, Hadi and Li, Zhoujun and Chua, Tat-Seng},
	year         = {2013},
	booktitle    = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Dublin, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '13},
	pages        = {43--52},
	doi          = {10.1145/2484028.2484057},
	isbn         = {9781450320344},
	url          = {https://doi.org/10.1145/2484028.2484057},
	abstract     = {Microblog services have emerged as an essential way to strengthen the communications among individuals and organizations. These services promote timely and active discussions and comments towards products, markets as well as public events, and have attracted a lot of attentions from organizations. In particular, emerging topics are of immediate concerns to organizations since they signal current concerns of, and feedback by their users. Two challenges must be tackled for effective emerging topic detection. One is the problem of real-time relevant data collection and the other is the ability to model the emerging characteristics of detected topics and identify them before they become hot topics. To tackle these challenges, we first design a novel scheme to crawl the relevant messages related to the designated organization by monitoring multi-aspects of microblog content, including users, the evolving keywords and their temporal sequence. We then develop an incremental clustering framework to detect new topics, and employ a range of content and temporal features to help in promptly detecting hot emerging topics. Extensive evaluations on a representative real-world dataset based on Twitter data demonstrate that our scheme is able to characterize emerging topics well and detect them before they become hot topics.},
	numpages     = {10},
	keywords     = {emerging topic detection, organization monitoring, brand monitoring, microblog service}
}
@inproceedings{chen2020simple,
	title        = {A simple framework for contrastive learning of visual representations},
	author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year         = {2020},
	booktitle    = {International conference on machine learning},
	pages        = {1597--1607},
	organization = {PmLR}
}
@article{chen2020survey,
	title        = {A survey on adversarial examples in deep learning},
	author       = {Chen, Kai and Zhu, Haoqi and Yan, Leiming and Wang, Jinwei},
	year         = {2020},
	journal      = {Journal on Big Data},
	publisher    = {Tech Science Press},
	volume       = {2},
	number       = {2},
	pages        = {71}
}
@article{chen2023evaluating,
	title        = {Evaluating Classification Model Against Bayes Error Rate},
	author       = {Chen, Qingqiang and Cao, Fuyuan and Xing, Ying and Liang, Jiye},
	year         = {2023},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {45},
	number       = {8},
	pages        = {9639--9653},
	doi          = {10.1109/TPAMI.2023.3240194}
}
@inproceedings{chen2024progressive,
	title        = {Progressive poisoned data isolation for training-time backdoor defense},
	author       = {Chen, Yiming and Wu, Haiwei and Zhou, Jiantao},
	year         = {2024},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {38},
	number       = {10},
	pages        = {11425--11433}
}
@article{chernick2002saw,
	title        = {The Saw-Toothed Behavior of Power Versus Sample Size and Software Solutions},
	author       = {Michael R Chernick and Christine Y Liu},
	year         = {2002},
	journal      = {The American Statistician},
	publisher    = {Taylor \& Francis},
	volume       = {56},
	number       = {2},
	pages        = {149--155},
	doi          = {10.1198/000313002317572835},
	url          = {https://doi.org/10.1198/000313002317572835},
	eprint       = {https://doi.org/10.1198/000313002317572835}
}
@article{chernoff1959sequential,
	title        = {Sequential Design of Experiments},
	author       = {Herman Chernoff},
	year         = {1959},
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = {30},
	number       = {3},
	pages        = {755--770},
	issn         = {00034851},
	url          = {http://www.jstor.org/stable/2237415},
	urldate      = {2023-05-10}
}
@inproceedings{chiang2020certified,
	title        = {Certified Defenses for Adversarial Patches},
	author       = {Ping{-}yeh Chiang and Renkun Ni and Ahmed Abdelkader and Chen Zhu and Christoph Studer and Tom Goldstein},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HyeaSkrYPH},
	timestamp    = {Mon, 02 May 2022 17:13:05 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/ChiangNAZSG20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{chicco2021siamese,
	title        = {Siamese Neural Networks: An Overview},
	author       = {Chicco, Davide},
	year         = {2021},
	booktitle    = {Artificial Neural Networks},
	publisher    = {Springer US},
	address      = {New York, NY},
	pages        = {73--94},
	doi          = {10.1007/978-1-0716-0826-5_3},
	isbn         = {978-1-0716-0826-5},
	url          = {https://doi.org/10.1007/978-1-0716-0826-5_3},
	abstract     = {Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman's rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.}
}
@article{chien2008data,
	title        = {Data mining to improve personnel selection and enhance human capital: A case study in high-technology industry},
	author       = {Chen-Fu Chien and Li-Fei Chen},
	year         = {2008},
	journal      = {Expert Systems with Applications},
	volume       = {34},
	number       = {1},
	pages        = {280--290},
	doi          = {https://doi.org/10.1016/j.eswa.2006.09.003},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417406002776},
	keywords     = {Personnel selection, Human capital, Data mining, Decision tree, Semiconductor industry},
	abstract     = {The quality of human capital is crucial for high-tech companies to maintain competitive advantages in knowledge economy era. However, high-technology companies suffering from high turnover rates often find it hard to recruit the right talents. In addition to conventional human resource management approaches, there is an urgent need to develop effective personnel selection mechanism to find the talents who are the most suitable to their own organizations. This study aims to fill the gap by developing a data mining framework based on decision tree and association rules to generate useful rules for personnel selection. The results can provide decision rules relating personnel information with work performance and retention. An empirical study was conducted in a semiconductor company to support their hiring decision for indirect labors including engineers and managers with different job functions. The results demonstrated the practical viability of this approach. Moreover, based on discussions among domain experts and data miner, specific recruitment and human resource management strategies were created from the results.}
}
@inproceedings{cho2019efficacy,
	title        = {On the Efficacy of Knowledge Distillation},
	author       = {Cho, Jang Hyun and Hariharan, Bharath},
	year         = {2019},
	booktitle    = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	volume       = {},
	number       = {},
	pages        = {4793--4801},
	doi          = {10.1109/ICCV.2019.00489},
	keywords     = {Training;Neural networks;Standards;Computer architecture;Knowledge engineering;Computational modeling;Probability distribution}
}
@inproceedings{chopra2005learning,
	title        = {Learning a similarity metric discriminatively, with application to face verification},
	author       = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	year         = {2005},
	booktitle    = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
	volume       = {1},
	number       = {},
	pages        = {539--546 vol. 1},
	doi          = {10.1109/CVPR.2005.202}
}
@article{chowdhery2022palm,
	title        = {PaLM: Scaling Language Modeling with Pathways},
	author       = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur{-}Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier{-}Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2204.02311},
	doi          = {10.48550/arXiv.2204.02311},
	url          = {https://doi.org/10.48550/arXiv.2204.02311},
	eprinttype   = {arXiv},
	eprint       = {2204.02311},
	timestamp    = {Thu, 07 Apr 2022 08:26:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2204-02311.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{chu2019meansum,
	title        = {{M}ean{S}um: A Neural Model for Unsupervised Multi-Document Abstractive Summarization},
	author       = {Chu, Eric and Liu, Peter},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {1223--1232},
	url          = {https://proceedings.mlr.press/v97/chu19b.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/chu19b/chu19b.pdf},
	abstract     = {Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outperforms a strong extractive baseline.}
}
@article{chudova2003analysis,
	title        = {Analysis of Pattern Discovery in Sequences Using a Bayes Error Framework},
	author       = {Chudova, Darya and Smyth, Padhraic},
	year         = {2003},
	month        = {Jul},
	day          = {01},
	journal      = {Data Mining and Knowledge Discovery},
	volume       = {7},
	number       = {3},
	pages        = {273--299},
	doi          = {10.1023/A:1024032204965},
	issn         = {1573-756X},
	url          = {https://doi.org/10.1023/A:1024032204965},
	abstract     = {In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. There are a number of fundamental aspects of this data mining problem that can make discovery ``easy'' or ``hard''---we characterize the difficulty of this problem using an analysis based on the Bayes error rate under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology.}
}
@article{cilibrasi2007google,
	title        = {The Google Similarity Distance},
	author       = {Cilibrasi, Rudi L. and Vitanyi, Paul M.B.},
	year         = {2007},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {19},
	number       = {3},
	pages        = {370--383},
	doi          = {10.1109/TKDE.2007.48}
}
@article{clarke2008global,
	title        = {Global Inference for Sentence Compression: An Integer Linear Programming Approach},
	author       = {James Clarke and Mirella Lapata},
	year         = {2008},
	journal      = {J. Artif. Intell. Res.},
	volume       = {31},
	pages        = {399--429},
	doi          = {10.1613/jair.2433},
	url          = {https://doi.org/10.1613/jair.2433},
	timestamp    = {Mon, 21 Jan 2019 15:01:18 +0100},
	biburl       = {https://dblp.org/rec/journals/jair/ClarkeL08.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{code,
	title        = {{Unlearnable Examples (GitHub repository)}},
	author       = {Anonymous},
	year         = {2025},
	note         = {Accessed: 2025‑08‑01},
	howpublished = {\url{https://github.com/soumission-anonyme/unlearnable}}
}
@inproceedings{cohan2016revisiting,
	title        = {Revisiting Summarization Evaluation for Scientific Articles},
	author       = {Arman Cohan and Nazli Goharian},
	year         = {2016},
	booktitle    = {Proceedings of the Tenth International Conference on Language Resources and Evaluation {LREC} 2016, Portoro{\v{z}}, Slovenia, May 23-28, 2016},
	publisher    = {European Language Resources Association {(ELRA)}},
	url          = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/1144.html},
	editor       = {Nicoletta Calzolari and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and H{\'{e}}l{\`{e}}ne Mazo and Asunci{\'{o}}n Moreno and Jan Odijk and Stelios Piperidis},
	timestamp    = {Mon, 19 Aug 2019 15:22:28 +0200},
	biburl       = {https://dblp.org/rec/conf/lrec/CohanG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cohen2019certified,
	title        = {Certified Adversarial Robustness via Randomized Smoothing},
	author       = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {1310--1320},
	url          = {https://proceedings.mlr.press/v97/cohen19c.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf},
	organization = {PMLR}
}
@inproceedings{collins2001convolution,
	title        = {Convolution Kernels for Natural Language},
	author       = {Collins, Michael and Duffy, Nigel},
	year         = {2001},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = {14},
	url          = {https://proceedings.neurips.cc/paper/2001/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf},
	editor       = {T. Dietterich and S. Becker and Z. Ghahramani}
}
@article{colombo2021infolm,
	title        = {InfoLM: {A} New Metric to Evaluate Summarization {\&} Data2Text Generation},
	author       = {Pierre Colombo and Chlo{\'{e}} Clavel and Pablo Piantanida},
	year         = {2021},
	journal      = {CoRR},
	volume       = {abs/2112.01589},
	url          = {https://arxiv.org/abs/2112.01589},
	eprinttype   = {arXiv},
	eprint       = {2112.01589},
	timestamp    = {Fri, 21 Jan 2022 20:43:46 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2112-01589.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{colwell1982spearman,
	title        = {Spearman versus Kendall},
	author       = {Colwell, D. J. and Gillett, J. R.},
	year         = {1982},
	journal      = {The Mathematical Gazette},
	publisher    = {Cambridge University Press},
	volume       = {66},
	number       = {438},
	pages        = {307--309},
	doi          = {10.2307/3615525}
}
@article{cooper1988organizing,
	title        = {Organizing knowledge syntheses: A taxonomy of literature reviews},
	author       = {Cooper, Harris M.},
	year         = {1988},
	month        = {Mar},
	day          = {01},
	journal      = {Knowledge in Society},
	volume       = {1},
	number       = {1},
	pages        = {104},
	doi          = {10.1007/BF03177550},
	issn         = {0897-1986},
	url          = {https://doi.org/10.1007/BF03177550},
	abstract     = {A taxonomy of literature reviews in education and psychology is presented. The taxonomy categorizes reviews according to: (a) focus; (b) goal; (c) perspective; (d) coverage; (e) organization; and (f) audience. The seven winners of the American Educational Research Association's Research Review Award are used to illustrate the taxonomy's categories. Data on the reliability of taxonomy codings when applied by readers is presented. Results of a survey of review authors provides baseline data on how frequently different types of reviews appear in the education and psychology literature. How the taxonomy might help in judging the quality of literature reviews is discussed, along with more general standards for evaluating reviews.}
}
@techreport{cooper1996using,
	title        = {Using the framework},
	author       = {Cooper, Robin and Crouch, Dick and Van Eijck, Jan and Fox, Chris and Van Genabith, Johan and Jaspars, Jan and Kamp, Hans and Milward, David and Pinkal, Manfred and Poesio, Massimo and others},
	year         = {1996},
	institution  = {Technical Report LRE 62-051 D-16, The FraCaS Consortium}
}
@article{corbett2023measure,
	title        = {The Measure and Mismeasure of Fairness},
	author       = {Sam Corbett-Davies and Johann D. Gaebler and Hamed Nilforoshan and Ravi Shroff and Sharad Goel},
	year         = {2023},
	journal      = {Journal of Machine Learning Research},
	volume       = {24},
	number       = {312},
	pages        = {1--117},
	url          = {http://jmlr.org/papers/v24/22-1511.html}
}
@book{cormen2022introduction,
	title        = {Introduction to Algorithms, 3rd Edition},
	author       = {Thomas H. Cormen and Charles E. Leiserson and Ronald L. Rivest and Clifford Stein},
	year         = {2009},
	publisher    = {{MIT} Press},
	isbn         = {978-0-262-03384-8},
	url          = {http://mitpress.mit.edu/books/introduction-algorithms},
	timestamp    = {Mon, 17 Aug 2020 11:36:12 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0023376.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{cover1967nearest,
	title        = {Nearest neighbor pattern classification},
	author       = {Cover, Thomas and Hart, Peter},
	year         = {1967},
	journal      = {IEEE transactions on information theory},
	publisher    = {IEEE},
	volume       = {13},
	number       = {1},
	pages        = {21--27}
}
@inproceedings{croce2020minimally,
	title        = {Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack},
	author       = {Croce, Francesco and Hein, Matthias},
	year         = {2020},
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {119},
	pages        = {2196--2205},
	url          = {https://proceedings.mlr.press/v119/croce20a.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/croce20a/croce20a.pdf},
	abstract     = {The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.}
}
@inproceedings{croce2020reliable,
	title        = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	author       = {Croce, Francesco and Hein, Matthias},
	year         = {2020},
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {119},
	pages        = {2206--2216},
	url          = {https://proceedings.mlr.press/v119/croce20b.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/croce20b/croce20b.pdf},
	abstract     = {The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	organization = {PMLR}
}
@article{croce2020scaling,
	title        = {Scaling up the Randomized Gradient-Free Adversarial Attack Reveals Overestimation of Robustness Using Established Attacks},
	author       = {Croce, Francesco and Rauber, Jonas and Hein, Matthias},
	year         = {2020},
	month        = {Apr},
	day          = {01},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer},
	volume       = {128},
	number       = {4},
	pages        = {1028--1046},
	doi          = {10.1007/s11263-019-01213-0},
	issn         = {1573-1405},
	url          = {https://doi.org/10.1007/s11263-019-01213-0},
	abstract     = {Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks (Croce and Hein in GCPR, 2018), in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.}
}
@inproceedings{cronen2002quantifying,
	title        = {Quantifying query ambiguity},
	author       = {Cronen-Townsend, Steve and Croft, W Bruce and others},
	year         = {2002},
	booktitle    = {Proceedings of HLT},
	volume       = {2},
	pages        = {94--98}
}
@misc{cs231n,
	title        = {CS231n Convolutional Neural Networks for Visual Recognition},
	journal      = {CS231N convolutional neural networks for visual recognition},
	url          = {https://cs231n.github.io/convolutional-networks/}
}
@misc{ctvnews2012danone,
	title        = {Danone to settle lawsuit over activia yogurt, Danactive Health claims},
	author       = {CTVNews},
	year         = {2012},
	month        = {Sep},
	journal      = {CTVNews},
	publisher    = {The Canadian Press},
	url          = {https://www.ctvnews.ca/health/health-headlines/danone-to-settle-lawsuit-over-activia-yogurt-danactive-health-claims-1.971371?cache=\%3FclipId\%3D375756}
}
@inproceedings{cummings2019compatibility,
	title        = {On the Compatibility of Privacy and Fairness},
	author       = {Cummings, Rachel and Gupta, Varun and Kimpara, Dhamma and Morgenstern, Jamie},
	year         = {2019},
	booktitle    = {Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization},
	location     = {Larnaca, Cyprus},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {UMAP'19 Adjunct},
	pages        = {309–315},
	doi          = {10.1145/3314183.3323847},
	isbn         = {9781450367110},
	url          = {https://doi.org/10.1145/3314183.3323847},
	abstract     = {In this work, we investigate whether privacy and fairness can be simultaneously achieved by a single classifier in several different models. Some of the earliest work on fairness in algorithm design defined fairness as a guarantee of similar outputs for "similar'' input data, a notion with tight technical connections to differential privacy. We study whether tensions exist between differential privacy and statistical notions of fairness, namely Equality of False Positives and Equality of False Negatives (EFP/EFN). We show that even under full distributional access, there are cases where the constraint of differential privacy precludes exact EFP/EFN. We then turn to ask whether one can learn a differentially private classifier which approximately satisfies EFP/EFN, and show the existence of a PAC learner which is private and approximately fair with high probability. We conclude by giving an efficient algorithm for classification that maintains utility and satisfies both privacy and approximate fairness with high probability.},
	numpages     = {7},
	keywords     = {learning theory, fairness, differential privacy}
}
@article{dagan_dolan_magnini_roth_2010,
	title        = {Recognizing textual entailment: Rational, evaluation and approaches - Erratum},
	author       = {DAGAN, IDO and DOLAN, BILL and MAGNINI, BERNARDO and ROTH, DAN},
	year         = {2010},
	journal      = {Natural Language Engineering},
	publisher    = {Cambridge University Press},
	volume       = {16},
	number       = {1},
	pages        = {105--105},
	doi          = {10.1017/S1351324909990234}
}
@article{dagan1999contextual,
	title        = {Contextual word similarity},
	author       = {Dagan, Ido},
	year         = {1999},
	journal      = {Handbook of Natural Language Processing},
	publisher    = {Marcel Dekker, inc.},
	pages        = {Chapter--19}
}
@inproceedings{dagan2005pascal,
	title        = {The PASCAL Recognising Textual Entailment Challenge},
	author       = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
	year         = {2006},
	booktitle    = {Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {177--190},
	isbn         = {978-3-540-33428-6},
	editor       = {Qui{\~{n}}onero-Candela, Joaquin and Dagan, Ido and Magnini, Bernardo and d'Alch{\'e}-Buc, Florence},
	abstract     = {This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.}
}
@article{dahal2019topic,
	title        = {Topic modeling and sentiment analysis of global climate change tweets},
	author       = {Dahal, Biraj and Kumar, Sathish A. P. and Li, Zhenlong},
	year         = {2019},
	month        = {Jun},
	day          = {10},
	journal      = {Social Network Analysis and Mining},
	volume       = {9},
	number       = {1},
	pages        = {24},
	doi          = {10.1007/s13278-019-0568-8},
	issn         = {1869-5469},
	url          = {https://doi.org/10.1007/s13278-019-0568-8},
	abstract     = {Social media websites can be used as a data source for mining public opinion on a variety of subjects including climate change. Twitter, in particular, allows for the evaluation of public opinion across both time and space because geotagged tweets include timestamps and geographic coordinates (latitude/longitude). In this study, a large dataset of geotagged tweets containing certain keywords relating to climate change is analyzed using volume analysis and text mining techniques such as topic modeling and sentiment analysis. Latent Dirichlet allocation was applied for topic modeling to infer the different topics of discussion, and Valence Aware Dictionary and sEntiment Reasoner was applied for sentiment analysis to determine the overall feelings and attitudes found in the dataset. These techniques are used to compare and contrast the nature of climate change discussion between different countries and over time. Sentiment analysis shows that the overall discussion is negative, especially when users are reacting to political or extreme weather events. Topic modeling shows that the different topics of discussion on climate change are diverse, but some topics are more prevalent than others. In particular, the discussion of climate change in the USA is less focused on policy-related topics than other countries.}
}
@article{dai2015document,
	title        = {Document Embedding with Paragraph Vectors},
	author       = {Andrew M. Dai and Christopher Olah and Quoc V. Le},
	year         = {2015},
	journal      = {CoRR},
	volume       = {abs/1507.07998},
	url          = {http://arxiv.org/abs/1507.07998},
	eprinttype   = {arXiv},
	eprint       = {1507.07998},
	timestamp    = {Mon, 13 Aug 2018 16:47:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/DaiOL15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dai2017social,
	title        = {From social media to public health surveillance: Word embedding based clustering method for twitter classification},
	author       = {Dai, Xiangfeng and Bikdash, Marwan and Meyer, Bradley},
	year         = {2017},
	booktitle    = {SoutheastCon 2017},
	pages        = {1--7},
	doi          = {10.1109/SECON.2017.7925400}
}
@article{das1999preference,
	title        = {A preference ordering among various Pareto optimal alternatives},
	author       = {Das, I.},
	year         = {1999},
	month        = {Aug},
	day          = {01},
	journal      = {Structural optimization},
	volume       = {18},
	number       = {1},
	pages        = {30--35},
	doi          = {10.1007/BF01210689},
	issn         = {1615-1488},
	url          = {https://doi.org/10.1007/BF01210689},
	abstract     = {It is often necessary to choose a Pareto optimal point from a set of many. This paper introduces the concept of order of efficiency, which provides a notion that is stronger than Pareto optimality and allows us to set up a preference ordering amongst various alternatives that are Pareto optimal. This approach does not resort to setting up a ranking on the basis of an arbitrary ``criterion of merit'' obtained by combining the multiple decision criteria into one scalar index. Examples are cited and it is argued that using the procedure described in this paper, it is possible to rule out Pareto alternatives with ``extreme components'' and retain alternatives ``in the middle'' of the Pareto set without the help of plots or other visualization aids. This makes the approach applicable for cases where the number of criteria is very high and visualization is intractable.}
}
@inproceedings{dasu2024neufair,
	title        = {NeuFair: Neural Network Fairness Repair with Dropout},
	author       = {Dasu, Vishnu Asutosh and Kumar, Ashish and Tizpaz-Niari, Saeid and Tan, Gang},
	year         = {2024},
	booktitle    = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
	location     = {Vienna, Austria},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ISSTA 2024},
	pages        = {1541–1553},
	doi          = {10.1145/3650212.3680380},
	isbn         = {9798400706127},
	url          = {https://doi.org/10.1145/3650212.3680380},
	abstract     = {This paper investigates neuron dropout as a post-processing bias mitigation method for deep neural networks (DNNs). Neural-driven software solutions are increasingly applied in socially critical domains with significant fairness implications. While DNNs are exceptional at learning statistical patterns from data, they may encode and amplify historical biases. Existing bias mitigation algorithms often require modifying the input dataset or the learning algorithms. We posit that prevalent dropout methods may be an effective and less intrusive approach to improve fairness of pre-trained DNNs during inference. However, finding the ideal set of neurons to drop is a combinatorial problem.                 We propose NeuFair, a family of post-processing randomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts during inference. Our randomized search is guided by an objective to minimize discrimination while maintaining the model’s utility. We show that NeuFair is efficient and effective in improving fairness (up to 69\%) with minimal or no model performance degradation. We provide intuitive explanations of these phenomena and carefully examine the influence of various hyperparameters of NeuFair on the results. Finally, we empirically and conceptually compare NeuFair to different state-of-the-art bias mitigators.},
	numpages     = {13},
	keywords     = {AI Ethics, Bias Mitigation, Machine Learning}
}
@article{DBLP:journals/corr/abs-2407-21783,
	title        = {The Llama 3 Herd of Models},
	author       = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al{-}Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur{\'{e}}lien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Rozi{\`{e}}re and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia{-}Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr{\'{e}}goire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and et al.},
	year         = {2024},
	journal      = {CoRR},
	volume       = {abs/2407.21783},
	doi          = {10.48550/ARXIV.2407.21783},
	url          = {https://doi.org/10.48550/arXiv.2407.21783},
	eprinttype   = {arXiv},
	eprint       = {2407.21783},
	timestamp    = {Mon, 26 Aug 2024 08:08:35 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2407-21783.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@techreport{de2008stanford,
	title        = {Stanford typed dependencies manual},
	author       = {De Marneffe, Marie-Catherine and Manning, Christopher D},
	year         = {2008},
	institution  = {Technical report, Stanford University}
}
@inproceedings{de2009cross,
	title        = {Cross-Language Linking of News Stories on the Web Using Interlingual Topic Modelling},
	author       = {De Smet, Wim and Moens, Marie-Francine},
	year         = {2009},
	booktitle    = {Proceedings of the 2nd ACM Workshop on Social Web Search and Mining},
	location     = {Hong Kong, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SWSM '09},
	pages        = {57--64},
	doi          = {10.1145/1651437.1651447},
	isbn         = {9781605588063},
	url          = {https://doi.org/10.1145/1651437.1651447},
	abstract     = {We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News.},
	numpages     = {8},
	keywords     = {latent dirichlet allocation, event detection}
}
@article{deaton2019transformers,
	title        = {Transformers and pointer-generator networks for abstractive summarization},
	author       = {Deaton, Jon and Jacobs, Austin and Kenealy, Kathleen and See, Abigail}
}
@inproceedings{deb2007self,
	title        = {Self-adaptive simulated binary crossover for real-parameter optimization},
	author       = {Kalyanmoy Deb and Karthik Sindhya and Tatsuya Okabe},
	year         = {2007},
	booktitle    = {Genetic and Evolutionary Computation Conference, {GECCO} 2007, Proceedings, London, England, UK, July 7-11, 2007},
	publisher    = {{ACM}},
	pages        = {1187--1194},
	doi          = {10.1145/1276958.1277190},
	url          = {https://doi.org/10.1145/1276958.1277190},
	editor       = {Hod Lipson},
	timestamp    = {Tue, 06 Nov 2018 11:06:41 +0100},
	biburl       = {https://dblp.org/rec/conf/gecco/DebSO07.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{deerwester1990indexing,
	title        = {Indexing by latent semantic analysis},
	author       = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year         = {1990},
	journal      = {Journal of the American Society for Information Science},
	volume       = {41},
	number       = {6},
	pages        = {391--407},
	doi          = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	url          = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9},
	eprint       = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9},
	abstract     = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley  \& Sons, Inc.}
}
@book{degroot2012probability,
	title        = {Probability and statistics},
	author       = {DeGroot, Morris H and Schervish, Mark J},
	year         = {2012},
	publisher    = {Pearson Education}
}
@inproceedings{dehnert2017storm,
	title        = {A storm is coming: A modern probabilistic model checker},
	author       = {Dehnert, Christian and Junges, Sebastian and Katoen, Joost-Pieter and Volk, Matthias},
	year         = {2017},
	booktitle    = {Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part II 30},
	pages        = {592--600},
	organization = {Springer}
}
@phdthesis{demartines1994analyse,
	title        = {Analyse de donn{\'e}es par r{\'e}seaux de neurones auto-organis{\'e}s},
	author       = {Demartines, Pierre},
	year         = {1994},
	school       = {Grenoble INPG}
}
@inproceedings{demontis2019adversarial,
	title        = {Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks},
	author       = {Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio},
	year         = {2019},
	booktitle    = {Proceedings of the 28th USENIX Conference on Security Symposium},
	location     = {Santa Clara, CA, USA},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {SEC'19},
	pages        = {321--338},
	isbn         = {9781939133069},
	abstract     = {Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transferability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferability: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack's transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.},
	numpages     = {18}
}
@inproceedings{deng2009imagenet,
	title        = {ImageNet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	year         = {2009},
	booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
	volume       = {},
	number       = {},
	pages        = {248--255},
	doi          = {10.1109/CVPR.2009.5206848},
	keywords     = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine}
}
@inproceedings{deng2019arcface,
	title        = {ArcFace: Additive Angular Margin Loss for Deep Face Recognition},
	author       = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	year         = {2019},
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {4685--4694},
	doi          = {10.1109/CVPR.2019.00482}
}
@inproceedings{deng2019retinaface,
	title        = {RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild},
	author       = {Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
	year         = {2020},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{devroye1996bayes,
	title        = {The Bayes Error},
	author       = {Devroye, Luc and Gy{\"o}rfi, L{\'a}szl{\'o} and Lugosi, G{\'a}bor and Devroye, Luc and Gy{\"o}rfi, L{\'a}szl{\'o} and Lugosi, G{\'a}bor},
	year         = {1996},
	journal      = {A Probabilistic Theory of Pattern Recognition},
	publisher    = {Springer},
	pages        = {9--20}
}
@incollection{devroye2006nonuniform,
	title        = {Chapter 4 Nonuniform Random Variate Generation},
	author       = {Luc Devroye},
	year         = {2006},
	booktitle    = {Simulation},
	publisher    = {Elsevier},
	series       = {Handbooks in Operations Research and Management Science},
	volume       = {13},
	pages        = {83--121},
	doi          = {https://doi.org/10.1016/S0927-0507(06)13004-2},
	issn         = {0927-0507},
	url          = {https://www.sciencedirect.com/science/article/pii/S0927050706130042},
	editor       = {Shane G. Henderson and Barry L. Nelson},
	abstract     = {This chapter provides a survey of the main methods in nonuniform random variate generation, and highlights recent research on the subject. Classical paradigms such as inversion, rejection, guide tables, and transformations are reviewed. We provide information on the expected time complexity of various algorithms, before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods.}
}
@book{deza2009encyclopedia,
	title        = {Encyclopedia of distances},
	author       = {Deza, Elena and Deza, Michel Marie and Deza, Michel Marie and Deza, Elena},
	year         = {2009},
	publisher    = {Springer}
}
@article{dhillon2000technical,
	title        = {Technical Opinion: Information System Security Management in the New Millennium},
	author       = {Dhillon, Gurpreet and Backhouse, James},
	year         = {2000},
	month        = {jul},
	journal      = {Commun. ACM},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {43},
	number       = {7},
	pages        = {125–128},
	doi          = {10.1145/341852.341877},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/341852.341877},
	issue_date   = {July 2000},
	numpages     = {4}
}
@inproceedings{di2020topfilter,
	title        = {TopFilter: An Approach to Recommend Relevant GitHub Topics},
	author       = {Di Rocco, Juri and Di Ruscio, Davide and Di Sipio, Claudio and Nguyen, Phuong and Rubei, Riccardo},
	year         = {2020},
	booktitle    = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
	location     = {Bari, Italy},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ESEM '20},
	doi          = {10.1145/3382494.3410690},
	isbn         = {9781450375801},
	url          = {https://doi.org/10.1145/3382494.3410690},
	abstract     = {Background: In the context of software development, GitHub has been at the forefront of platforms to store, analyze and maintain a large number of software repositories. Topics have been introduced by GitHub as an effective method to annotate stored repositories. However, labeling GitHub repositories should be carefully conducted to avoid adverse effects on project popularity and reachability. Aims: We present TopFilter, a novel approach to assist open source software developers in selecting suitable topics for GitHub repositories being created. Method: We built a project-topic matrix and applied a syntactic-based similarity function to recommend missing topics by representing repositories and related topics in a graph. The ten-fold cross-validation methodology has been used to assess the performance of TopFilter by considering different metrics, i.e., success rate, precision, recall, and catalog coverage. Result: The results show that TopFilter recommends good topics depending on different factors, i.e., collaborative filtering settings, considered datasets, and pre-processing activities. Moreover, TopFilter can be combined with a state-of-the-art topic recommender system (i.e., MNB network) to improve the overall prediction performance. Conclusion: Our results confirm that collaborative filtering techniques can successfully be used to provide relevant topics for GitHub repositories. Moreover, TopFilter can gain a significant boost in prediction performances by employing the outcomes obtained by the MNB network as its initial set of topics.},
	articleno    = {21},
	numpages     = {11},
	keywords     = {Collaborative filtering, Recommender systems, GitHub topics recommendation}
}
@article{dietterich1995overfitting,
	title        = {Overfitting and undercomputing in machine learning},
	author       = {Dietterich, Tom},
	year         = {1995},
	month        = sep,
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {27},
	number       = {3},
	pages        = {326–327},
	doi          = {10.1145/212094.212114},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/212094.212114},
	issue_date   = {Sept. 1995},
	numpages     = {2}
}
@inproceedings{ding2021repvgg,
	title        = {RepVGG: Making VGG-Style ConvNets Great Again},
	author       = {Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
	year         = {2021},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {13733--13742}
}
@inproceedings{dinu2009ordinal,
	title        = {Ordinal measures in authorship identification},
	author       = {Dinu, Liviu P and Popescu, Marius},
	year         = {2009},
	booktitle    = {Proc. SEPLN},
	volume       = {9},
	pages        = {62--66}
}
@inproceedings{doddington2002automatic,
	title        = {Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics},
	author       = {Doddington, George},
	year         = {2002},
	booktitle    = {Proceedings of the Second International Conference on Human Language Technology Research},
	location     = {San Diego, California},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {HLT '02},
	pages        = {138--145},
	abstract     = {Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R\&amp;D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an "evaluation understudy", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research.},
	numpages     = {8}
}
@inproceedings{doherty2023individual,
	title        = {Individual Fairness in Bayesian Neural Networks},
	author       = {Alice Doherty and Matthew Robert Wicker and Luca Laurenti and Andrea Patane},
	year         = {2023},
	booktitle    = {Fifth Symposium on Advances in Approximate Bayesian Inference},
	url          = {https://openreview.net/forum?id=Ea0fNpgMJh}
}
@inproceedings{dolan2005automatically,
	title        = {Automatically constructing a corpus of sentential paraphrases},
	author       = {Dolan, William B and Brockett, Chris},
	year         = {2005},
	booktitle    = {Proceedings of the Third International Workshop on Paraphrasing (IWP2005)}
}
@article{dolan2005microsoft,
	title        = {Microsoft research paraphrase corpus},
	author       = {Dolan, Bill and Brockett, Chris and Quirk, Chris},
	year         = {2005},
	journal      = {Retrieved March},
	volume       = {29},
	number       = {2008},
	pages        = {63},
	url          = {http://research.microsoft.com/research/nlp/msr_paraphrase.htm}
}
@inproceedings{dong2018boosting,
	title        = {Boosting Adversarial Attacks With Momentum},
	author       = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	year         = {2018},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{dong2019efficient,
	title        = {Efficient decision-based black-box adversarial attacks on face recognition},
	author       = {Dong, Yinpeng and Su, Hang and Wu, Baoyuan and Li, Zhifeng and Liu, Wei and Zhang, Tong and Zhu, Jun},
	year         = {2019},
	booktitle    = {proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {7714--7722}
}
@inproceedings{dong2019evading,
	title        = {Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks},
	author       = {Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
	year         = {2019},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{dong2019unified,
	title        = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
	author       = {Li Dong and Nan Yang and Wenhui Wang and Furu Wei and Xiaodong Liu and Yu Wang and Jianfeng Gao and Ming Zhou and Hsiao{-}Wuen Hon},
	year         = {2019},
	booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
	pages        = {13042--13054},
	url          = {https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html},
	editor       = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	timestamp    = {Thu, 21 Jan 2021 15:15:20 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/00040WWLWGZH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{donkers2017sequential,
	title        = {Sequential user-based recurrent neural network recommendations},
	author       = {Donkers, Tim and Loepp, Benedikt and Ziegler, J{\"u}rgen},
	year         = {2017},
	booktitle    = {Proceedings of the eleventh ACM conference on recommender systems},
	pages        = {152--160}
}
@inproceedings{dosovitskiy2020image,
	title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year         = {2021},
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=YicbFdNTTy},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{du2023sanitizing,
	title        = {Sanitizing Sentence Embeddings (and Labels) for Local Differential Privacy},
	author       = {Du, Minxin and Yue, Xiang and Chow, Sherman S. M. and Sun, Huan},
	year         = {2023},
	booktitle    = {Proceedings of the ACM Web Conference 2023},
	location     = {Austin, TX, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '23},
	pages        = {2349–2359},
	doi          = {10.1145/3543507.3583512},
	isbn         = {9781450394161},
	url          = {https://doi.org/10.1145/3543507.3583512},
	abstract     = {Differentially private (DP) learning, notably DP stochastic gradient descent (DP-SGD), has limited applicability in fine-tuning gigantic pre-trained language models (LMs) for natural language processing tasks. The culprit is the perturbation of gradients (as gigantic as entire models), leading to significant efficiency and accuracy drops. We show how to achieve metric-based local DP (LDP) by sanitizing (high-dimensional) sentence embedding, extracted by LMs and much smaller than gradients. For potential utility improvement, we impose a consistency constraint on the sanitization. We explore&nbsp;two approaches: One is brand new and can directly output consistent noisy embeddings; the other is an upgradation with post-processing. To further mitigate “the curse of dimensionality,” we introduce two trainable linear maps for mediating dimensions without hurting privacy or utility. Our protection can effectively defend against privacy threats on embeddings. It also naturally extends to inference. Our experiments1 show that we reach the non-private accuracy under properly configured parameters, e.g., 0.92 for SST-2 with a privacy budget ϵ = 10 and the reduced dimension as 16. We also sanitize the label for LDP (with another small privacy budget) with limited accuracy losses to fully protect every sequence-label pair.},
	numpages     = {11},
	keywords     = {Local Differential Privacy, Natural Language Processing, Pre-trained Language Models, Privacy-preserving NLP, Sentence Embeddings}
}
@article{duan2024efficient,
	title        = {Efficient Training of Large Language Models on Distributed Infrastructures: A Survey},
	author       = {Duan, Jiangfei and Zhang, Shuo and Wang, Zerui and Jiang, Lijuan and Qu, Wenwen and Hu, Qinghao and Wang, Guoteng and Weng, Qizhen and Yan, Hang and Zhang, Xingcheng and others},
	year         = {2024},
	journal      = {arXiv preprint arXiv:2407.20018}
}
@inproceedings{dutta2023bibliography,
	title        = {Bibliography Counselor: A Citation Recommendation Tool},
	author       = {Dutta, Debanjan and Pal, Dipasree and Roy, Dwaipayan and Mitra, Mandar},
	year         = {2023},
	booktitle    = {2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	volume       = {},
	number       = {},
	pages        = {260--262},
	doi          = {10.1109/JCDL57899.2023.00051},
	keywords     = {Bibliographies;Source coding;Libraries;Task analysis;Surges;Recommender systems;Software development management;Demo;Digital Library;Bibliography;Citation;Recommendation Tool}
}
@article{duvcinskas2011expected,
	title        = {Expected Bayes Error Rate in Supervised Classification of Spatial Gaussian Data},
	author       = {Du\v{c}inskas, Kundefinedstutis and Stabingienundefined, Lijana},
	year         = {2011},
	month        = {jul},
	journal      = {Informatica},
	publisher    = {IOS Press},
	address      = {NLD},
	volume       = {22},
	number       = {3},
	pages        = {371–381},
	issn         = {0868-4952},
	issue_date   = {July 2011},
	abstract     = {In the usual statistical approach of spatial classification, it is assumed that the feature observations are independent conditionally on class labels (conditional independence). Discarding this popular assumption, we consider the problem of statistical classification by using multivariate stationary Gaussian Random Field (GRF) for modeling the conditional distribution given class labels of feature observations. The classes are specified by multivariate regression model for means and by common factorized covariance function. In the two-class case and for the class labels modeled by Random Field (RF) based on 0–1 divergence, the formula of the Expected Bayes Error Rate (EBER) is derived. The effect of training sample size on the EBER and the influence of statistical parameters to the values of EBER are numerically evaluated in the case when the spatial framework of data is the subset of the 2-dimensional rectangular lattice with unit spacing.},
	numpages     = {11},
	keywords     = {Divergence, Gaussian Random Fields, Spatial Correlation, Bayes Discriminant Function}
}
@article{duvcinskas2015actual,
	title        = {Actual Error Rates in Classification of the T-Distributed Random Field Observation Based on Plug-in Linear Discriminant Function},
	author       = {Du\v{c}inskas, Kundefinedstutis and Zikarienundefined, Eglundefined},
	year         = {2015},
	month        = {jan},
	journal      = {Informatica},
	publisher    = {IOS Press},
	address      = {NLD},
	volume       = {26},
	number       = {4},
	pages        = {557–568},
	issn         = {0868-4952},
	issue_date   = {2015},
	abstract     = {In current paper a problem of classification of T-distributed random field observation into one of two populations specified by common scaling function is considered. The ML and LS estimators of the mean parameters are plugged into the linear discriminant function. The closed form expressions for the Bayes error rate and the actual error rate associated with the aforementioned discriminant functions are derived. This is the extension of one for the Gaussian case. The actual error rates are used to evaluate and compare the performance of the plug-in discriminant function by means of Monte Carlo study.},
	numpages     = {12},
	keywords     = {actual error rate, T-distributed random field, Bayes rule, spatial correlation, scaling function}
}
@article{duvsek2020evaluating,
	title        = {Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge},
	author       = {Ondřej Dušek and Jekaterina Novikova and Verena Rieser},
	year         = {2020},
	journal      = {Computer Speech  \& Language},
	volume       = {59},
	pages        = {123--156},
	doi          = {https://doi.org/10.1016/j.csl.2019.06.009},
	issn         = {0885-2308},
	url          = {https://www.sciencedirect.com/science/article/pii/S0885230819300919},
	abstract     = {This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures - with the majority implementing sequence-to-sequence models (seq2seq) - as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness - with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.}
}
@inproceedings{dwork2006calibrating,
	title        = {Calibrating Noise to Sensitivity in Private Data Analysis},
	author       = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	year         = {2006},
	booktitle    = {Theory of Cryptography},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {265--284},
	isbn         = {978-3-540-32732-5},
	editor       = {Halevi, Shai and Rabin, Tal},
	abstract     = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.}
}
@inproceedings{dwork2012fairness,
	title        = {Fairness through awareness},
	author       = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year         = {2012},
	booktitle    = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	location     = {Cambridge, Massachusetts},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ITCS '12},
	pages        = {214–226},
	doi          = {10.1145/2090236.2090255},
	isbn         = {9781450311151},
	url          = {https://doi.org/10.1145/2090236.2090255},
	abstract     = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	numpages     = {13}
}
@article{dwork2014algorithmic,
	title        = {The Algorithmic Foundations of Differential Privacy},
	author       = {Cynthia Dwork and Aaron Roth},
	year         = {2014},
	journal      = {Foundations and Trends® in Theoretical Computer Science},
	volume       = {9},
	number       = {3–4},
	pages        = {211--407},
	doi          = {10.1561/0400000042},
	issn         = {1551-305X},
	url          = {http://dx.doi.org/10.1561/0400000042}
}
@inproceedings{ehlers2017formal,
	title        = {Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks},
	author       = {Ehlers, R{\"u}diger},
	year         = {2017},
	booktitle    = {Automated Technology for Verification and Analysis},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {269--286},
	isbn         = {978-3-319-68167-2},
	editor       = {D'Souza, Deepak and Narayan Kumar, K.},
	abstract     = {We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.}
}
@inproceedings{elboher2020abstraction,
	title        = {An Abstraction-Based Framework for Neural Network Verification},
	author       = {Elboher, Yizhak Yisrael and Gottschlich, Justin and Katz, Guy},
	year         = {2020},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {43--65},
	isbn         = {978-3-030-53288-8},
	editor       = {Lahiri, Shuvendu K. and Wang, Chao},
	abstract     = {Deep neural networks are increasingly being used as controllers for safety-critical systems. Because neural networks are opaque, certifying their correctness is a significant challenge. To address this issue, several neural network verification approaches have recently been proposed. However, these approaches afford limited scalability, and applying them to large networks can be challenging. In this paper, we propose a framework that can enhance neural network verification techniques by using over-approximation to reduce the size of the network---thus making it more amenable to verification. We perform the approximation such that if the property holds for the smaller (abstract) network, it holds for the original as well. The over-approximation may be too coarse, in which case the underlying verification tool might return a spurious counterexample. Under such conditions, we perform counterexample-guided refinement to adjust the approximation, and then repeat the process. Our approach is orthogonal to, and can be integrated with, many existing verification techniques. For evaluation purposes, we integrate it with the recently proposed Marabou framework, and observe a significant improvement in Marabou's performance. Our experiments demonstrate the great potential of our approach for verifying larger neural networks.}
}
@article{engstrom2017rotation,
	title        = {A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations},
	author       = {Logan Engstrom and Dimitris Tsipras and Ludwig Schmidt and Aleksander Madry},
	year         = {2017},
	journal      = {CoRR},
	volume       = {abs/1712.02779},
	url          = {http://arxiv.org/abs/1712.02779},
	eprinttype   = {arXiv},
	eprint       = {1712.02779},
	timestamp    = {Mon, 13 Aug 2018 16:48:14 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1712-02779.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{enwiki:1091909435,
	title        = {Handedness --- {Wikipedia}{,} The Free Encyclopedia},
	author       = {{Wikipedia}},
	url          = {https://en.wikipedia.org/w/index.php?title=Handedness\&oldid=1091909435},
	note         = {[Online; accessed 19-June-2022]}
}
@inproceedings{esfandiari2022label,
	title        = {Label differential privacy via clustering},
	author       = {Esfandiari, Hossein and Mirrokni, Vahab and Syed, Umar and Vassilvitskii, Sergei},
	year         = {2022},
	month        = {28--30 Mar},
	booktitle    = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {151},
	pages        = {7055--7075},
	url          = {https://proceedings.mlr.press/v151/esfandiari22a.html},
	editor       = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
	pdf          = {https://proceedings.mlr.press/v151/esfandiari22a/esfandiari22a.pdf},
	abstract     = {We present new mechanisms for label differential privacy, a relaxation of differentially private machine learning that only protects the privacy of the labels in the training set. Our mechanisms cluster the examples in the training set using their (non-private) feature vectors, randomly re-sample each label from examples in the same cluster, and output a training set with noisy labels as well as a modified version of the true loss function. We prove that when the clusters are both large and high-quality, the model that minimizes the modified loss on the noisy training set converges to small excess risk at a rate that is comparable to the rate for non-private learning. We also describe a learning problem in which large clusters are necessary to achieve both strong privacy and either good precision or good recall. Our experiments show that randomizing the labels within each cluster significantly improves the privacy vs. accuracy trade-off compared to applying uniform randomized response to the labels, and also compared to learning a model via DP-SGD.}
}
@article{estes1955statistical,
	title        = {Statistical theory of spontaneous recovery and regression.},
	author       = {Estes, William K},
	year         = {1955},
	journal      = {Psychological review},
	volume       = {62},
	number       = {3},
	url          = {https://doi.org/10.1037/h0048509}
}
@misc{ets_2022,
	title        = {Sample essay responses and Rater Commentary for the argument task},
	author       = {ETS},
	year         = {2022},
	journal      = {ETS},
	url          = {https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/sample_responses}
}
@inproceedings{evtimov2018robust,
	title        = {Robust physical-world attacks on deep learning visual classification},
	author       = {Evtimov, Ivan and Eykholt, Kevin and Fernandes, Earlence and Kohno, Tadayoshi and Li, Bo and Prakash, Atul and Rahmati, Amir and Song, Dawn},
	year         = {2018},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {1625--1634}
}
@article{faber2014sample,
	title        = {How sample size influences research outcomes},
	author       = {Faber, Jorge and Fonseca, Lilian Martins},
	year         = {2014},
	month        = jul,
	journal      = {Dental Press J Orthod},
	address      = {Brazil},
	volume       = {19},
	number       = {4},
	pages        = {27--29},
	abstract     = {Sample size calculation is part of the early stages of conducting an epidemiological, clinical or laboratory study. In preparing a scientific paper, there are ethical and methodological indications for its use. Two investigations conducted with the same methodology and achieving equivalent results, but different only in terms of sample size, may point the researcher in different directions when it comes to making clinical decisions. Therefore, ideally, samples should not be small and, contrary to what one might think, should not be excessive. The aim of this paper is to discuss in clinical language the main implications of the sample size when interpreting a study.},
	keywords     = {Clinical trial; Methodology; Sample calculation; Sample size; Scientific evidence},
	language     = {en}
}
@article{fact2019report,
	title        = {Report on the Facebook Third Party Fact Checking programme},
	author       = {Fact, Full},
	year         = {2019},
	month        = {Jul},
	journal      = {London, Full Fact},
	url          = {https://fullfact.org/media/uploads/tpfc-q1q2-2019.pdf}
}
@inproceedings{fader2014open,
	title        = {Open question answering over curated and extracted knowledge bases},
	author       = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
	year         = {2014},
	booktitle    = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {1156--1165}
}
@inproceedings{fang2012mining,
	title        = {Mining Contrastive Opinions on Political Texts Using Cross-Perspective Topic Model},
	author       = {Fang, Yi and Si, Luo and Somasundaram, Naveen and Yu, Zhengtao},
	year         = {2012},
	booktitle    = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
	location     = {Seattle, Washington, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WSDM '12},
	pages        = {63--72},
	doi          = {10.1145/2124295.2124306},
	isbn         = {9781450307475},
	url          = {https://doi.org/10.1145/2124295.2124306},
	abstract     = {This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model.},
	numpages     = {10},
	keywords     = {topic modeling, contrastive opinions, opinion mining, opinion retrieval}
}
@inproceedings{farrand2020neither,
	title        = {Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy},
	author       = {Farrand, Tom and Mireshghallah, Fatemehsadat and Singh, Sahib and Trask, Andrew},
	year         = {2020},
	booktitle    = {Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {PPMLP'20},
	pages        = {15–19},
	doi          = {10.1145/3411501.3419419},
	isbn         = {9781450380881},
	url          = {https://doi.org/10.1145/3411501.3419419},
	abstract     = {Deployment of deep learning in different fields and industries is growing day by day due to its performance, which relies on the availability of data and compute. Data is often crowd-sourced and contains sensitive information about its contributors, which leaks into models that are trained on it. To achieve rigorous privacy guarantees, differentially private training mechanisms are used. However, it has recently been shown that differential privacy can exacerbate existing biases in the data and have disparate impacts on the accuracy of different subgroups of data. In this paper, we aim to study these effects within differentially private deep learning. Specifically, we aim to study how different levels of imbalance in the data affect the accuracy and the fairness of the decisions made by the model, given different levels of privacy. We demonstrate that even small imbalances and loose privacy guarantees can cause disparate impacts.},
	numpages     = {5},
	keywords     = {bias, data imbalance, deep learning, differential privacy, fairness}
}
@inproceedings{feldman2015certifying,
	title        = {Certifying and Removing Disparate Impact},
	author       = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year         = {2015},
	booktitle    = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Sydney, NSW, Australia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '15},
	pages        = {259–268},
	doi          = {10.1145/2783258.2783311},
	isbn         = {9781450336642},
	url          = {https://doi.org/10.1145/2783258.2783311},
	abstract     = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
	numpages     = {10},
	keywords     = {machine learning, fairness, disparate impact}
}
@inbook{fellbaum2010wordnet,
	title        = {WordNet},
	author       = {Fellbaum, Christiane},
	year         = {2010},
	booktitle    = {Theory and Applications of Ontology: Computer Applications},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {231--243},
	doi          = {10.1007/978-90-481-8847-5_10},
	isbn         = {978-90-481-8847-5},
	url          = {https://doi.org/10.1007/978-90-481-8847-5_10},
	editor       = {Poli, Roberto and Healy, Michael and Kameas, Achilles},
	abstract     = {WordNet is a large electronic lexical database for English (Miller 1995, Fellbaum 1998a). It originated in 1986 at Princeton University where it continues to be developed and maintained. George A. Miller, a psycholinguist, was inspired by experiments in Artificial Intelligence that tried to understand human semantic memory (e.g., Collins and Quillian 1969). Given the fact that speakers possess knowledge about tens of thousands of words and the concepts expressed by these words, it seemed reasonable to assume efficient and economic storage and access mechanisms for words and concepts. The Collins and Quillian model proposed a hierarchical structure of concepts, where more specific concepts inherit information from their superordinate, more general concepts; only knowledge particular to more specific concepts needs to be stored with such concepts. Thus, it took subjects longer to confirm a statement like "canaries have feathers" than the statement "birds have feathers" since, presumably, the property "has-feathers" is stored with the concept bird and not redundantly with the concept for each kind of bird.}
}
@misc{feller1958introduction,
	title        = {An introduction to probability theory and its applications},
	author       = {Feller, William and Morse, Philip M},
	year         = {1958},
	publisher    = {American Institute of Physics}
}
@inproceedings{filighera2020fooling,
	title        = {Fooling Automatic Short Answer Grading Systems},
	author       = {Filighera, Anna and Steuer, Tim and Rensing, Christoph},
	year         = {2020},
	booktitle    = {Artificial Intelligence in Education},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {177--190},
	isbn         = {978-3-030-52237-7},
	editor       = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Mill{\'a}n, Eva},
	abstract     = {With the rising success of adversarial attacks on many NLP tasks, systems which actually operate in an adversarial scenario need to be reevaluated. For this purpose, we pose the following research question: How difficult is it to fool automatic short answer grading systems? In particular, we investigate the robustness of the state of the art automatic short answer grading system proposed by Sung et al. towards cheating in the form of universal adversarial trigger employment. These are short token sequences that can be prepended to students' answers in an exam to artificially improve their automatically assigned grade. Such triggers are especially critical as they can easily be used by anyone once they are found. In our experiments, we discovered triggers which allow students to pass exams with passing thresholds of {\$}{\$}50{\backslash}{\%}{\$}{\$}without answering a single question correctly. Furthermore, we show that such triggers generalize across models and datasets in this scenario, nullifying the defense strategy of keeping grading models or data secret.}
}
@article{filighera2022cheating,
	title        = {Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs},
	author       = {Anna Filighera and Sebastian Ochs and Tim Steuer and Thomas Tregel},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2201.08318},
	url          = {https://arxiv.org/abs/2201.08318},
	eprinttype   = {arXiv},
	eprint       = {2201.08318},
	timestamp    = {Tue, 01 Feb 2022 14:59:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2201-08318.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{finlayson2019adversarial,
	title        = {Adversarial attacks on medical machine learning},
	author       = {Samuel G. Finlayson  and John D. Bowers  and Joichi Ito  and Jonathan L. Zittrain  and Andrew L. Beam  and Isaac S. Kohane},
	year         = {2019},
	journal      = {Science},
	volume       = {363},
	number       = {6433},
	pages        = {1287--1289},
	doi          = {10.1126/science.aaw4399},
	url          = {https://www.science.org/doi/abs/10.1126/science.aaw4399},
	eprint       = {https://www.science.org/doi/pdf/10.1126/science.aaw4399},
	abstract     = {Emerging vulnerabilities demand new conversations With public and academic attention increasingly focused on the new role of machine learning in the health information economy, an unusual and no-longer-esoteric category of vulnerabilities in machine-learning systems could prove important. These vulnerabilities allow a small, carefully designed change in how inputs are presented to a system to completely alter its output, causing it to confidently arrive at manifestly wrong conclusions. These advanced techniques to subvert otherwise-reliable machine-learning systems—so-called adversarial attacks—have, to date, been of interest primarily to computer science researchers (1). However, the landscape of often-competing interests within health care, and billions of dollars at stake in systems' outputs, implies considerable problems. We outline motivations that various players in the health care system may have to use adversarial attacks and begin a discussion of what to do about them. Far from discouraging continued innovation with medical machine learning, we call for active engagement of medical, technical, legal, and ethical experts in pursuit of efficient, broadly available, and effective health care that machine learning will enable.}
}
@article{floridi2020gpt,
	title        = {GPT-3: Its Nature, Scope, Limits, and Consequences},
	author       = {Floridi, Luciano and Chiriatti, Massimo},
	year         = {2020},
	month        = {Dec},
	day          = {01},
	journal      = {Minds and Machines},
	volume       = {30},
	number       = {4},
	pages        = {681--694},
	doi          = {10.1007/s11023-020-09548-1},
	issn         = {1572-8641},
	url          = {https://doi.org/10.1007/s11023-020-09548-1},
	abstract     = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.}
}
@inproceedings{forgues2014bootstrapping,
	title        = {Bootstrapping dialog systems with word embeddings},
	author       = {Forgues, Gabriel and Pineau, Joelle and Larchev{\^e}que, Jean-Marie and Tremblay, R{\'e}al},
	year         = {2014},
	booktitle    = {Nips, modern machine learning and natural language processing workshop},
	volume       = {2},
	pages        = {168}
}
@article{fortunato2010community,
	title        = {Community detection in graphs},
	author       = {Santo Fortunato},
	year         = {2010},
	journal      = {Physics Reports},
	volume       = {486},
	number       = {3},
	pages        = {75--174},
	doi          = {https://doi.org/10.1016/j.physrep.2009.11.002},
	issn         = {0370-1573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	keywords     = {Graphs, Clusters, Statistical physics},
	abstract     = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.}
}
@article{fowl2021adversarial,
	title        = {Adversarial examples make strong poisons},
	author       = {Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom},
	year         = {2021},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {34},
	pages        = {30339--30351}
}
@article{frasch2011bayes,
	title        = {A Bayes-true data generator for evaluation of supervised and unsupervised learning methods},
	author       = {Janick V. Frasch and Aleksander Lodwich and Faisal Shafait and Thomas M. Breuel},
	year         = {2011},
	journal      = {Pattern Recognition Letters},
	volume       = {32},
	number       = {11},
	pages        = {1523--1531},
	doi          = {https://doi.org/10.1016/j.patrec.2011.04.010},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865511001103},
	keywords     = {Synthetic data generation, Benchmarking, Experimental proofs},
	abstract     = {Benchmarking pattern recognition, machine learning and data mining methods commonly relies on real-world data sets. However, there are some disadvantages in using real-world data. On one hand collecting real-world data can become difficult or impossible for various reasons, on the other hand real-world variables are hard to control, even in the problem domain; in the feature domain, where most statistical learning methods operate, exercising control is even more difficult and hence rarely attempted. This is at odds with the scientific experimentation guidelines mandating the use of as directly controllable and as directly observable variables as possible. Because of this, synthetic data possesses certain advantages over real-world data sets. In this paper we propose a method that produces synthetic data with guaranteed global and class-specific statistical properties. This method is based on overlapping class densities placed on the corners of a regular k-simplex. This generator can be used for algorithm testing and fair performance evaluation of statistical learning methods. Because of the strong properties of this generator researchers can reproduce each others experiments by knowing the parameters used, instead of transmitting large data sets.}
}
@article{fraser2011bayes,
	title        = {{Is Bayes Posterior just Quick and Dirty Confidence?}},
	author       = {D. A. S. Fraser},
	year         = {2011},
	journal      = {Statistical Science},
	publisher    = {Institute of Mathematical Statistics},
	volume       = {26},
	number       = {3},
	pages        = {299 -- 316},
	doi          = {10.1214/11-STS352},
	url          = {https://doi.org/10.1214/11-STS352},
	keywords     = {Bayes, Bayes error rate, Confidence, default prior, evaluating a prior, nonlinear parameter, Posterior, prior}
}
@inproceedings{fredrikson2014privacy,
	title        = {Privacy in pharmacogenetics: an end-to-end case study of personalized warfarin dosing},
	author       = {Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
	year         = {2014},
	booktitle    = {Proceedings of the 23rd USENIX Conference on Security Symposium},
	location     = {San Diego, CA},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {SEC'14},
	pages        = {17–32},
	isbn         = {9781931971157},
	abstract     = {We initiate the study of privacy in pharmacogenetics, wherein machine learning models are used to guide medical treatments based on a patient's genotype and background. Performing an in-depth case study on privacy in personalized warfarin dosing, we show that suggested models carry privacy risks, in particular because attackers can perform what we call model inversion: an attacker, given the model and some demographic information about a patient, can predict the patient's genetic markers.As differential privacy (DP) is an oft-proposed solution for medical settings such as this, we evaluate its effectiveness for building private versions of pharmacogenetic models. We show that DP mechanisms prevent our model inversion attacks when the privacy budget is carefully selected. We go on to analyze the impact on utility by performing simulated clinical trials with DP dosing models. We find that for privacy budgets effective at preventing attacks, patients would be exposed to increased risk of stroke, bleeding events, and mortality. We conclude that current DP mechanisms do not simultaneously improve genomic privacy while retaining desirable clinical efficacy, highlighting the need for new mechanisms that should be evaluated in situ using the general methodology introduced by our work.},
	numpages     = {16}
}
@inproceedings{fried2014analyzing,
	title        = {Analyzing the language of food on social media},
	author       = {Fried, Daniel and Surdeanu, Mihai and Kobourov, Stephen and Hingle, Melanie and Bell, Dane},
	year         = {2014},
	booktitle    = {2014 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {778--783},
	doi          = {10.1109/BigData.2014.7004305}
}
@misc{ftc2010dannon,
	title        = {Dannon agrees to drop exaggerated health claims for activia yogurt and danactive dairy drink},
	author       = {the Premerger Notification Office and DPIP and CTO},
	year         = {2010},
	month        = {Dec},
	journal      = {Federal Trade Commission},
	url          = {https://www.ftc.gov/news-events/news/press-releases/2010/12/dannon-agrees-drop-exaggerated-health-claims-activia-yogurt-danactive-dairy-drink}
}
@inproceedings{fu2016using,
	title        = {Using LSTM and GRU neural network methods for traffic flow prediction},
	author       = {Fu, Rui and Zhang, Zuo and Li, Li},
	year         = {2016},
	booktitle    = {2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
	volume       = {},
	number       = {},
	pages        = {324--328},
	doi          = {10.1109/YAC.2016.7804912}
}
@article{fu2022robust,
	title        = {Robust unlearnable examples: Protecting data against adversarial learning},
	author       = {Fu, Shaopeng and He, Fengxiang and Liu, Yang and Shen, Li and Tao, Dacheng},
	year         = {2022},
	journal      = {arXiv preprint arXiv:2203.14533}
}
@article{fukunaga1975k,
	title        = {k-nearest-neighbor Bayes-risk estimation},
	author       = {Fukunaga, K. and Hostetler, L.},
	year         = {1975},
	journal      = {IEEE Transactions on Information Theory},
	volume       = {21},
	number       = {3},
	pages        = {285--293},
	doi          = {10.1109/TIT.1975.1055373}
}
@book{fukunaga1990introduction,
	title        = {Introduction to Statistical Pattern Recognition (2nd Ed.)},
	author       = {Fukunaga, Keinosuke},
	year         = {1990},
	publisher    = {Academic Press Professional, Inc.},
	address      = {USA},
	isbn         = {0122698517}
}
@inproceedings{gabrilovich2007computing,
	title        = {Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic Analysis},
	author       = {Gabrilovich, Evgeniy and Markovitch, Shaul},
	year         = {2007},
	booktitle    = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
	location     = {Hyderabad, India},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {IJCAI'07},
	pages        = {1606--1611},
	abstract     = {Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.},
	numpages     = {6}
}
@article{gage1994new,
	title        = {A new algorithm for data compression},
	author       = {Gage, Philip},
	year         = {1994},
	journal      = {The C Users Journal},
	publisher    = {R \& D Publications, Inc. Lawrence, KS, USA},
	volume       = {12},
	number       = {2},
	pages        = {23--38}
}
@inproceedings{galhotra2017fairness,
	title        = {Fairness testing: testing software for discrimination},
	author       = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
	year         = {2017},
	booktitle    = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
	location     = {Paderborn, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ESEC/FSE 2017},
	pages        = {498–510},
	doi          = {10.1145/3106237.3106277},
	isbn         = {9781450351058},
	url          = {https://doi.org/10.1145/3106237.3106277},
	abstract     = {This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98\% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.},
	numpages     = {13},
	keywords     = {testing, software bias, fairness testing, Discrimination testing}
}
@article{ganin2016domain,
	title        = {Domain-Adversarial Training of Neural Networks},
	author       = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran{\c{c}}ois Laviolette and Mario March and Victor Lempitsky},
	year         = {2016},
	journal      = {Journal of Machine Learning Research},
	volume       = {17},
	number       = {59},
	pages        = {1--35},
	url          = {http://jmlr.org/papers/v17/15-239.html}
}
@article{gao2015wordnet,
	title        = {A WordNet-based semantic similarity measurement combining edge-counting and information content theory},
	author       = {Jian-Bo Gao and Bao-Wen Zhang and Xiao-Hua Chen},
	year         = {2015},
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = {39},
	pages        = {80--88},
	doi          = {https://doi.org/10.1016/j.engappai.2014.11.009},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197614002814},
	keywords     = {Semantic similarity, WordNet, Edge-counting, Information content},
	abstract     = {Semantic similarity measuring between words can be applied to many applications, such as Artificial Intelligence, Information Processing, Medical Care and Linguistics. In this paper, we present a new approach for semantic similarity measuring which is based on edge-counting and information content theory. Specifically, the proposed measure nonlinearly transforms the weighted shortest path length between the compared concepts to achieve the semantic similarity results, and the relation between parameters and the correlation value is discussed in detail. Experimental results show that the proposed approach not only achieves high correlation value against human ratings but also has better distribution characteristics of the correlation coefficient compared with several related works in the literature. In addition, the proposed method is computationally efficient due to the simplified ways of weighting the shortest path length between the concept pairs.}
}
@article{gao2019incorporating,
	title        = {Incorporating word embeddings into topic modeling of short text},
	author       = {Gao, Wang and Peng, Min and Wang, Hua and Zhang, Yanchun and Xie, Qianqian and Tian, Gang},
	year         = {2019},
	month        = {Nov},
	day          = {01},
	journal      = {Knowledge and Information Systems},
	volume       = {61},
	number       = {2},
	pages        = {1123--1145},
	doi          = {10.1007/s10115-018-1314-7},
	issn         = {0219-3116},
	url          = {https://doi.org/10.1007/s10115-018-1314-7},
	abstract     = {Short texts have become the prevalent format of information on the Internet. Inferring the topics of this type of messages becomes a critical and challenging task for many applications. Due to the length of short texts, conventional topic models (e.g., latent Dirichlet allocation and its variants) suffer from the severe data sparsity problem which makes topic modeling of short texts difficult and unreliable. Recently, word embeddings have been proved effective to capture semantic and syntactic information about words, which can be used to induce similarity measures and semantic correlations among words. Enlightened by this, in this paper, we design a novel model for short text topic modeling, referred as Conditional Random Field regularized Topic Model (CRFTM). CRFTM not only develops a generalized solution to alleviate the sparsity problem by aggregating short texts into pseudo-documents, but also leverages a Conditional Random Field regularized model that encourages semantically related words to share the same topic assignment. Experimental results on two real-world datasets show that our method can extract more coherent topics, and significantly outperform state-of-the-art baselines on several evaluation metrics.}
}
@inproceedings{gao2020fuzz,
	title        = {Fuzz testing based data augmentation to improve robustness of deep neural networks},
	author       = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
	year         = {2020},
	booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
	location     = {Seoul, South Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '20},
	pages        = {1147–1158},
	doi          = {10.1145/3377811.3380415},
	isbn         = {9781450371216},
	url          = {https://doi.org/10.1145/3377811.3380415},
	abstract     = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9\% and 5.5\% on average. Further, Sensei-SA can reduce the average DNN training time by 25\%, while still improving robust accuracy.},
	numpages     = {12},
	keywords     = {robustness, genetic algorithm, data augmentation, DNN}
}
@article{garber1988bounds,
	title        = {Bounds on the Bayes classification error based on pairwise risk functions},
	author       = {Garber, F.D. and Djouadi, A.},
	year         = {1988},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {10},
	number       = {2},
	pages        = {281--288},
	doi          = {10.1109/34.3891}
}
@book{gardenfors2004conceptual,
	title        = {Conceptual spaces - the geometry of thought},
	author       = {Peter G{\"{a}}rdenfors},
	year         = {2000},
	publisher    = {{MIT} Press},
	isbn         = {978-0-262-07199-4},
	timestamp    = {Mon, 18 Apr 2011 17:48:55 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0006106.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{garfield1955citation,
	title        = {Citation Indexes for Science},
	author       = {Eugene Garfield},
	year         = {1955},
	journal      = {Science},
	volume       = {122},
	number       = {3159},
	pages        = {108--111},
	doi          = {10.1126/science.122.3159.108},
	url          = {https://www.science.org/doi/abs/10.1126/science.122.3159.108},
	eprint       = {https://www.science.org/doi/pdf/10.1126/science.122.3159.108}
}
@inproceedings{ge2018deep,
	title        = {Deep Metric Learning with Hierarchical Triplet Loss},
	author       = {Ge, Weifeng},
	year         = {2018},
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@inproceedings{gehr2018ai2,
	title        = {AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
	author       = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
	year         = {2018},
	booktitle    = {2018 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {3--18},
	doi          = {10.1109/SP.2018.00058}
}
@inproceedings{geirhos2018imagenet,
	title        = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	author       = {Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Bygh9j09KX},
	timestamp    = {Thu, 25 Jul 2019 13:03:15 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/GeirhosRMBWB19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{geirhos2018imagenettrained,
	title        = {ImageNet-trained {CNN}s are biased towards texture; increasing shape bias improves accuracy and robustness.},
	author       = {Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
	year         = {2019},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Bygh9j09KX}
}
@article{geirhos2020shortcut,
	title        = {Shortcut learning in deep neural networks},
	author       = {Robert Geirhos and J{\"{o}}rn{-}Henrik Jacobsen and Claudio Michaelis and Richard S. Zemel and Wieland Brendel and Matthias Bethge and Felix A. Wichmann},
	year         = {2020},
	journal      = {Nat. Mach. Intell.},
	volume       = {2},
	number       = {11},
	pages        = {665--673},
	doi          = {10.1038/s42256-020-00257-z},
	url          = {https://doi.org/10.1038/s42256-020-00257-z},
	timestamp    = {Wed, 15 Dec 2021 10:26:49 +0100},
	biburl       = {https://dblp.org/rec/journals/natmi/GeirhosJMZBBW20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{george2021real,
	title        = {Real-time spatio-temporal event detection on geotagged social media},
	author       = {George, Yasmeen and Karunasekera, Shanika and Harwood, Aaron and Lim, Kwan Hui},
	year         = {2021},
	month        = {Jun},
	day          = {24},
	journal      = {Journal of Big Data},
	volume       = {8},
	number       = {1},
	pages        = {91},
	doi          = {10.1186/s40537-021-00482-2},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-021-00482-2},
	abstract     = {A key challenge in mining social media data streams is to identify events which are actively discussed by a group of people in a specific local or global area. Such events are useful for early warning for accident, protest, election or breaking news. However, neither the list of events nor the resolution of both event time and space is fixed or known beforehand. In this work, we propose an online spatio-temporal event detection system using social media that is able to detect events at different time and space resolutions. First, to address the challenge related to the unknown spatial resolution of events, a quad-tree method is exploited in order to split the geographical space into multiscale regions based on the density of social media data. Then, a statistical unsupervised approach is performed that involves Poisson distribution and a smoothing method for highlighting regions with unexpected density of social posts. Further, event duration is precisely estimated by merging events happening in the same region at consecutive time intervals. A post processing stage is introduced to filter out events that are spam, fake or wrong. Finally, we incorporate simple semantics by using social media entities to assess the integrity, and accuracy of detected events. The proposed method is evaluated using different social media datasets: Twitter and Flickr for different cities: Melbourne, London, Paris and New York. To verify the effectiveness of the proposed method, we compare our results with two baseline algorithms based on fixed split of geographical space and clustering method. For performance evaluation, we manually compute recall and precision. We also propose a new quality measure named strength index, which automatically measures how accurate the reported event is.}
}
@article{gesi2022code,
	title        = {Code Smells in Machine Learning Systems},
	author       = {Jiri Gesi and Siqi Liu and Jiawei Li and Iftekhar Ahmed and Nachiappan Nagappan and David Lo and Eduardo Santana de Almeida and Pavneet Singh Kochhar and Lingfeng Bao},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2203.00803},
	doi          = {10.48550/arXiv.2203.00803},
	url          = {https://doi.org/10.48550/arXiv.2203.00803},
	eprinttype   = {arXiv},
	eprint       = {2203.00803},
	timestamp    = {Wed, 16 Mar 2022 16:39:52 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2203-00803.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ghazi2021deep,
	title        = {Deep Learning with Label Differential Privacy},
	author       = {Ghazi, Badih and Golowich, Noah and Kumar, Ravi and Manurangsi, Pasin and Zhang, Chiyuan},
	year         = {2021},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {34},
	pages        = {27131--27145},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e3a54649aeec04cf1c13907bc6c5c8aa-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@inproceedings{ghazi2022regression,
	title        = {Regression with Label Differential Privacy},
	author       = {Badih Ghazi and Pritish Kamath and Ravi Kumar and Ethan Leeman and Pasin Manurangsi and Avinash Varadarajan and Chiyuan Zhang},
	year         = {2023},
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=h9O0wsmL-cT}
}
@inproceedings{ghazilabeldp,
	title        = {Label{DP}-Pro: Learning with Label Differential Privacy via Projections},
	author       = {Badih Ghazi and Yangsibo Huang and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Chiyuan Zhang},
	year         = {2024},
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=JnYaF3vv3G}
}
@inproceedings{gilmer2018adversarial,
	title        = {Adversarial Spheres},
	author       = {Justin Gilmer and Luke Metz and Fartash Faghri and Samuel S. Schoenholz and Maithra Raghu and Martin Wattenberg and Ian J. Goodfellow},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SkthlLkPf},
	timestamp    = {Thu, 04 Apr 2019 13:20:09 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/GilmerMFSRWG18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gilmer2018motivating,
	title        = {Motivating the Rules of the Game for Adversarial Example Research},
	author       = {Justin Gilmer and Ryan P. Adams and Ian J. Goodfellow and David G. Andersen and George E. Dahl},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1807.06732},
	url          = {http://arxiv.org/abs/1807.06732},
	eprinttype   = {arXiv},
	eprint       = {1807.06732},
	timestamp    = {Wed, 07 Oct 2020 15:24:17 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1807-06732.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gilpin2018explaining,
	title        = {Explaining explanations: An overview of interpretability of machine learning},
	author       = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	year         = {2018},
	booktitle    = {2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
	pages        = {80--89},
	organization = {IEEE}
}
@misc{gina,
	title        = {Public Law 110-233: Genetic Information Nondiscrimination Act of 2008},
	year         = {2008},
	url          = {https://www.govinfo.gov/content/pkg/PLAW-110publ233/html/PLAW-110publ233.htm},
	note         = {Act of Congress, United States}
}
@inproceedings{goodfellow2014explaining,
	title        = {Explaining and Harnessing Adversarial Examples},
	author       = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
	year         = {2015},
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6572},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:38 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{goodfellow2016deep,
	title        = {Deep learning},
	author       = {GOODFELLOW, Ian and BENGIO, Yoshua and COURVILLE, Aaron},
	year         = {2016},
	publisher    = {MIT Press},
	place        = {Cambridge; Massachusetts; London}
}
@article{goodman2020advbox,
	title        = {Advbox: a toolbox to generate adversarial examples that fool neural networks},
	author       = {Dou Goodman and Xin Hao and Yang Wang and Yuesheng Wu and Junfeng Xiong and Huan Zhang},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2001.05574},
	url          = {https://arxiv.org/abs/2001.05574},
	eprinttype   = {arXiv},
	eprint       = {2001.05574},
	timestamp    = {Fri, 17 Jan 2020 14:07:30 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2001-05574.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{gordon2014probabilistic,
	title        = {Probabilistic programming},
	author       = {Gordon, Andrew D and Henzinger, Thomas A and Nori, Aditya V and Rajamani, Sriram K},
	year         = {2014},
	booktitle    = {Future of software engineering proceedings},
	pages        = {167--181}
}
@article{goswami2018unravelling,
	title        = {Unravelling Robustness of Deep Learning Based Face Recognition Against Adversarial Attacks},
	author       = {Goswami, Gaurav and Ratha, Nalini and Agarwal, Akshay and Singh, Richa and Vatsa, Mayank},
	year         = {2018},
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {32},
	number       = {1},
	doi          = {10.1609/aaai.v32i1.12341},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/12341},
	abstractnote = {\&lt;p\&gt; Deep neural network (DNN) architecture based models have high expressive power and learning capacity. However, they are essentially a black box method since it is not easy to mathematically formulate the functions that are learned within its many layers of representation. Realizing this, many researchers have started to design methods to exploit the drawbacks of deep learning based algorithms questioning their robustness and exposing their singularities. In this paper, we attempt to unravel three aspects related to the robustness of DNNs for face recognition: (i) assessing the impact of deep architectures for face recognition in terms of vulnerabilities to attacks inspired by commonly observed distortions in the real world that are well handled by shallow learning methods along with learning based adversaries; (ii) detecting the singularities by characterizing abnormal filter response behavior in the hidden layers of deep networks; and (iii) making corrections to the processing pipeline to alleviate the problem. Our experimental evaluation using multiple open-source DNN-based face recognition networks, including OpenFace and VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates that the performance of deep learning based face recognition algorithms can suffer greatly in the presence of such distortions. The proposed method is also compared with existing detection algorithms and the results show that it is able to detect the attacks with very high accuracy by suitably designing a classifier using the response of the hidden layers in the network. Finally, we present several effective countermeasures to mitigate the impact of adversarial attacks and improve the overall robustness of DNN-based face recognition. \&lt;/p\&gt;}
}
@article{gou2021knowledge,
	title        = {Knowledge Distillation: A Survey},
	author       = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	year         = {2021},
	month        = {Jun},
	day          = {01},
	journal      = {International Journal of Computer Vision},
	volume       = {129},
	number       = {6},
	pages        = {1789--1819},
	doi          = {10.1007/s11263-021-01453-z},
	issn         = {1573-1405},
	url          = {https://doi.org/10.1007/s11263-021-01453-z},
	abstract     = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher--student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.}
}
@inproceedings{gowal2019scalable,
	title        = {Scalable verified training for provably robust image classification},
	author       = {Gowal, Sven and Dvijotham, Krishnamurthy Dj and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
	year         = {2019},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {4842--4851}
}
@article{gower1971general,
	title        = {A General Coefficient of Similarity and Some of Its Properties},
	author       = {J. C. Gower},
	year         = {1971},
	journal      = {Biometrics},
	publisher    = {International Biometric Society},
	volume       = {27},
	number       = {4},
	pages        = {857--871},
	issn         = {0006341X, 15410420},
	url          = {http://www.jstor.org/stable/2528823},
	urldate      = {2024-09-20},
	abstract     = {A general coefficient measuring the similarity between two sampling units is defined. The matrix of similarities between all pairs of sample units is shown to be positive semidefinite (except possibly when there are missing values). This is important for the multidimensional Euclidean representation of the sample and also establishes some inequalities amongst the similarities relating three individuals. The definition is extended to cope with a hierarchy of characters.}
}
@article{graham2017can,
	title        = {Can machine translation systems be evaluated by the crowd alone},
	author       = {GRAHAM, YVETTE and BALDWIN, TIMOTHY and MOFFAT, ALISTAIR and ZOBEL, JUSTIN},
	year         = {2017},
	journal      = {Natural Language Engineering},
	publisher    = {Cambridge University Press},
	volume       = {23},
	number       = {1},
	pages        = {3--30},
	doi          = {10.1017/S1351324915000339}
}
@inproceedings{graves2014towards,
	title        = {Towards End-To-End Speech Recognition with Recurrent Neural Networks},
	author       = {Graves, Alex and Jaitly, Navdeep},
	year         = {2014},
	month        = {22--24 Jun},
	booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Bejing, China},
	series       = {Proceedings of Machine Learning Research},
	volume       = {32},
	number       = {2},
	pages        = {1764--1772},
	url          = {https://proceedings.mlr.press/v32/graves14.html},
	editor       = {Xing, Eric P. and Jebara, Tony},
	pdf          = {http://proceedings.mlr.press/v32/graves14.pdf}
}
@inproceedings{grgic2016case,
	title        = {The case for process fairness in learning: Feature selection for fair decision making},
	author       = {Grgic-Hlaca, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	year         = {2016},
	booktitle    = {NIPS symposium on machine learning and the law},
	volume       = {1},
	number       = {2},
	pages        = {11},
	organization = {Barcelona, Spain}
}
@article{grimmer2010bayesian,
	title        = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases},
	author       = {Grimmer, Justin},
	year         = {2010},
	journal      = {Political Analysis},
	publisher    = {Cambridge University Press},
	volume       = {18},
	number       = {1},
	pages        = {1--35},
	doi          = {10.1093/pan/mpp034}
}
@inproceedings{grosse2017adversarial,
	title        = {Adversarial Examples for Malware Detection},
	author       = {Grosse, Kathrin and Papernot, Nicolas and Manoharan, Praveen and Backes, Michael and McDaniel, Patrick},
	year         = {2017},
	booktitle    = {Computer Security -- ESORICS 2017},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {62--79},
	isbn         = {978-3-319-66399-9},
	editor       = {Foley, Simon N. and Gollmann, Dieter and Snekkenes, Einar},
	abstract     = {Machine learning models are known to lack robustness against inputs crafted by an adversary. Such adversarial examples can, for instance, be derived from regular inputs by introducing minor---yet carefully selected---perturbations.},
	organization = {Springer}
}
@article{gu2022privacy,
	title        = {Privacy, accuracy, and model fairness trade-offs in federated learning},
	author       = {Gu, Xiuting and Tianqing, Zhu and Li, Jie and Zhang, Tao and Ren, Wei and Choo, Kim-Kwang Raymond},
	year         = {2022},
	month        = {nov},
	journal      = {Comput. Secur.},
	publisher    = {Elsevier Advanced Technology Publications},
	address      = {GBR},
	volume       = {122},
	number       = {C},
	doi          = {10.1016/j.cose.2022.102907},
	issn         = {0167-4048},
	url          = {https://doi.org/10.1016/j.cose.2022.102907},
	issue_date   = {Nov 2022},
	numpages     = {10},
	keywords     = {Machine learning, Privacy preservation, Fairness, Discrimination, Differential privacy, Federated learning}
}
@inproceedings{gu2024minillm,
	title        = {MiniLLM: Knowledge Distillation of Large Language Models},
	author       = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
	year         = {2024},
	booktitle    = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=5h0qf7IBZZ},
	timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/Gu0WH24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{guarino1995ontologies,
	title        = {Ontologies and knowledge bases},
	author       = {Guarino, Nicola and Giaretta, Pierdaniele},
	year         = {1995},
	journal      = {Towards very large knowledge bases},
	publisher    = {IOS press Clifton, VA, USA},
	pages        = {1--2}
}
@inproceedings{guerin2021certifying,
	title        = {Certifying Emergency Landing for Safe Urban UAV},
	author       = {Guerin, Joris and Delmas, Kevin and Guiochet, Jérémie},
	year         = {2021},
	booktitle    = {2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
	volume       = {},
	number       = {},
	pages        = {55--62},
	doi          = {10.1109/DSN-W52860.2021.00020},
	keywords     = {Training;Runtime;Uncertainty;Roads;Urban areas;Semantics;Unmanned aerial vehicles;UAV Emergency Landing;Certification;Run-time Monitoring;Semantic Segmentation;Safety}
}
@misc{guide51,
	title        = {Iso/iec guide 51: Safety aspects-guidelines for their inclusion in standards. Geneva, Switzerland},
	author       = {ISO},
	year         = {2014}
}
@inproceedings{guo2017calibration,
	title        = {On calibration of modern neural networks},
	author       = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	year         = {2017},
	booktitle    = {International conference on machine learning},
	pages        = {1321--1330},
	organization = {PMLR}
}
@inproceedings{guo2021logbert,
	title        = {Logbert: Log anomaly detection via bert},
	author       = {Guo, Haixuan and Yuan, Shuhan and Wu, Xintao},
	year         = {2021},
	booktitle    = {2021 international joint conference on neural networks (IJCNN)},
	pages        = {1--8},
	organization = {IEEE}
}
@article{gupta2021individual,
	title        = {Individual Fairness in Hindsight},
	author       = {Swati Gupta and Vijay Kamble},
	year         = {2021},
	journal      = {Journal of Machine Learning Research},
	volume       = {22},
	number       = {144},
	pages        = {1--35},
	url          = {http://jmlr.org/papers/v22/19-658.html}
}
@article{gusfield1997algorithms,
	title        = {Algorithms on Stings, Trees, and Sequences: Computer Science and Computational Biology},
	author       = {Gusfield, Dan},
	year         = {1997},
	month        = {dec},
	journal      = {SIGACT News},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {28},
	number       = {4},
	pages        = {41--60},
	doi          = {10.1145/270563.571472},
	issn         = {0163-5700},
	url          = {https://doi.org/10.1145/270563.571472},
	issue_date   = {Dec. 1997},
	numpages     = {20}
}
@article{guyon2003introduction,
	title        = {An introduction to variable and feature selection},
	author       = {Guyon, Isabelle and Elisseeff, Andr{\'e}},
	year         = {2003},
	journal      = {Journal of machine learning research},
	volume       = {3},
	number       = {Mar},
	pages        = {1157--1182}
}
@inproceedings{haim2006second,
	title        = {The second pascal recognising textual entailment challenge},
	author       = {Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
	year         = {2006},
	booktitle    = {Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
	volume       = {7}
}
@inproceedings{halder2021transformer,
	title        = {Transformer-Based Multi-task Learning for Queuing Time Aware Next POI Recommendation},
	author       = {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and Zhang, Xiuzhen},
	year         = {2021},
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {510--523},
	isbn         = {978-3-030-75765-6},
	editor       = {Karlapalem, Kamal and Cheng, Hong and Ramakrishnan, Naren and Agrawal, R. K. and Reddy, P. Krishna and Srivastava, Jaideep and Chakraborty, Tanmoy},
	abstract     = {Next point-of-interest (POI) recommendation is an important and challenging problem due to different contextual information and wide variety in human mobility patterns. Most of the prior studies incorporated user travel spatiotemporal andsequential patterns to recommend next POIs. However, few of these previous approaches considered the queuing time at POIs and its influence on user's mobility. The queuing time plays a significant role in affecting user mobility behaviour, e.g., having to queue a long time to enter a POI might reduce visitor's enjoyment. Recently, attention based recurrent neural networks-based approaches show promising performance in next POI recommendation but they are limited to single head attention which can have difficulty finding the appropriate complex connections between users, previous travel history and POI information. In this research, we present a problem of queuing time aware next POI recommendation and demonstrate how it is non-trivial to both recommend a next POI and simultaneously predict its queuing time. To solve this problem, we propose a multi-task, multi head attention transformer model called TLR-M. The model recommends next POIs to the target users and predicts queuing time to access the POIs simultaneously. By utilizing multi-head attention, the TLR-M model can integrate long range dependencies between any two POI visit efficiently and evaluate their contribution to select next POIs and to predict queuing time. Extensive experiments on eight real datasets show that the proposed model outperforms than the state-of-the-art baseline approaches in terms of precision, recall and F1 score evaluation metrics. The model also predicts and minimizes the queuing time effectively.}
}
@article{hall1987smoothing,
	title        = {On Smoothing Sparse Multinomial Data},
	author       = {Hall, Peter and Titterington, D. M.},
	year         = {1987},
	journal      = {Australian Journal of Statistics},
	volume       = {29},
	number       = {1},
	pages        = {19--37},
	doi          = {https://doi.org/10.1111/j.1467-842X.1987.tb00717.x},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-842X.1987.tb00717.x},
	keywords     = {Cross-validation, kernel estimators, multinomial smoothing, optimal smoothing, sparse multinomial},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-842X.1987.tb00717.x},
	abstract     = {Summary Asymptotic theory is developed for the problem of smoothing sparse multinomial data, with emphasis on the criterion of mean summed square error of estimators of the probability mass function. If the data are not too sparse, in a well-defined sense, then the optimal rate of convergence is that achieved by the unsmoothed cell proportions. Otherwise, this rate can be improved upon by smoothing. Explicit results, including formulae for the optimal smoothing parameter, are presented for a kernel-type estimator. Also for this case, a cross-validatory choice procedure is shown to be asymptotically optimal.}
}
@article{han2014unsupervised,
	title        = {Unsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation},
	author       = {Han, Aaron L.-F. and Wong, Derek F. and Chao, Lidia S. and He, Liangye and Lu, Yi},
	year         = {2014},
	month        = {Apr},
	day          = {28},
	journal      = {The Scientific World Journal},
	publisher    = {Hindawi Publishing Corporation},
	volume       = {2014},
	pages        = {760301},
	doi          = {10.1155/2014/760301},
	issn         = {2356-6140},
	url          = {https://doi.org/10.1155/2014/760301},
	abstract     = {With the rapid development of machine translation (MT), the MT evaluation becomes very important to timely tell us whether the MT system makes any progress. The conventional MT evaluation methods tend to calculate the similarity between hypothesis translations offered by automatic translation systems and reference translations offered by professional translators. There are several weaknesses in existing evaluation metrics. Firstly, the designed incomprehensive factors result in language-bias problem, which means they perform well on some special language pairs but weak on other language pairs. Secondly, they tend to use no linguistic features or too many linguistic features, of which no usage of linguistic feature draws a lot of criticism from the linguists and too many linguistic features make the model weak in repeatability. Thirdly, the employed reference translations are very expensive and sometimes not available in the practice. In this paper, the authors propose an unsupervised MT evaluation metric using universal part-of-speech tagset without relying on reference translations. The authors also explore the performances of the designed metric on traditional supervised evaluation tasks. Both the supervised and unsupervised experiments show that the designed methods yield higher correlation scores with human judgments.}
}
@inproceedings{han2015learning,
	title        = {Learning both Weights and Connections for Efficient Neural Network},
	author       = {Song Han and Jeff Pool and John Tran and William J. Dally},
	year         = {2015},
	journal      = {Advances in neural information processing systems},
	booktitle    = {Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
	volume       = {28},
	pages        = {1135--1143},
	url          = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
	editor       = {Corinna Cortes and Neil D. Lawrence and Daniel D. Lee and Masashi Sugiyama and Roman Garnett},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/HanPTD15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{han2016machine,
	title        = {Machine translation evaluation resources and methods: A survey},
	author       = {Han, Lifeng},
	year         = {2016},
	journal      = {arXiv preprint arXiv:1605.04515}
}
@inproceedings{han2020ghostnet,
	title        = {GhostNet: More Features From Cheap Operations},
	author       = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	year         = {2020},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{han2021deeprec,
	title        = {DeepRec: On-device deep learning for privacy-preserving sequential recommendation in mobile commerce},
	author       = {Han, Jialiang and Ma, Yun and Mei, Qiaozhu and Liu, Xuanzhe},
	year         = {2021},
	booktitle    = {Proceedings of the Web Conference 2021},
	pages        = {900--911}
}
@inproceedings{han2021rethinking,
	title        = {Rethinking Channel Dimensions for Efficient Model Design},
	author       = {Han, Dongyoon and Yun, Sangdoo and Heo, Byeongho and Yoo, YoungJoon},
	year         = {2021},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {732--741}
}
@inproceedings{hanneke2024revisiting,
	title        = {Revisiting Agnostic PAC Learning},
	author       = {Hanneke, Steve and Larsen, Kasper Green and Zhivotovskiy, Nikita},
	year         = {2024},
	booktitle    = {2024 IEEE 65th Annual Symposium on Foundations of Computer Science (FOCS)},
	volume       = {},
	number       = {},
	pages        = {1968--1982},
	doi          = {10.1109/FOCS61266.2024.00118},
	keywords     = {Training;Computer science;Risk minimization;Supervised learning;Training data;Picture archiving and communication systems;Classification algorithms;learning theory;pac learning;agnostic;sample complexity;vc-dimension}
}
@inproceedings{hardt2016equality,
	title        = {Equality of Opportunity in Supervised Learning},
	author       = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	year         = {2016},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {29},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@article{harper2015movielens,
	title        = {The movielens datasets: History and context},
	author       = {Harper, F Maxwell and Konstan, Joseph A},
	year         = {2015},
	journal      = {Acm transactions on interactive intelligent systems (tiis)},
	publisher    = {Acm New York, NY, USA},
	volume       = {5},
	number       = {4},
	pages        = {1--19}
}
@book{hart2018doing,
	title        = {Doing a Literature Review: Releasing the Research Imagination},
	author       = {Chris Hart},
	year         = {2018},
	publisher    = {SAGE Publications Ltd},
	address      = {London},
	series       = {SAGE Study Skills Series},
	pages        = {352},
	isbn         = {9781526423146},
	url          = {http://digital.casalini.it/9781526423146},
	casaliniid   = {5019541}
}
@book{hastie2009elements,
	title        = {The Elements of Statistical Learning},
	author       = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year         = {2009},
	publisher    = {Springer},
	url          = {https://hastie.su.domains/Papers/ESLII.pdf},
	keywords     = {book.ml,ebook},
	edition      = {2}
}
@inproceedings{he2016deep,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = {2016},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{he2016fusing,
	title        = {Fusing similarity models with markov chains for sparse sequential recommendation},
	author       = {He, Ruining and McAuley, Julian},
	year         = {2016},
	booktitle    = {2016 IEEE 16th international conference on data mining (ICDM)},
	pages        = {191--200},
	organization = {IEEE}
}
@inproceedings{he2017neural,
	title        = {Neural collaborative filtering},
	author       = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
	year         = {2017},
	booktitle    = {Proceedings of the 26th international conference on world wide web},
	pages        = {173--182}
}
@inproceedings{he2017translation,
	title        = {Translation-based recommendation},
	author       = {He, Ruining and Kang, Wang-Cheng and McAuley, Julian},
	year         = {2017},
	booktitle    = {Proceedings of the eleventh ACM conference on recommender systems},
	pages        = {161--169}
}
@article{he2023sharpness,
	title        = {Sharpness-aware data poisoning attack},
	author       = {He, Pengfei and Xu, Han and Ren, Jie and Cui, Yingqian and Liu, Hui and Aggarwal, Charu C and Tang, Jiliang},
	year         = {2023},
	journal      = {arXiv preprint arXiv:2305.14851}
}
@misc{health_prize,
	title        = {{Heritage Health Prize}},
	author       = {{Kaggle}},
	year         = {{n.d.}},
	url          = {https://www.kaggle.com/c/hhp},
	note         = {Accessed: {DATE}}
}
@article{heckner2008tagging,
	title        = {Tagging tagging. Analysing user keywords in scientific bibliography management systems},
	author       = {Heckner, Markus and M{\"u}hlbacher, Susanne and Wolff, Christian},
	year         = {2008}
}
@article{hegde2020unsupervised,
	title        = {Unsupervised Paraphrase Generation using Pre-trained Language Models},
	author       = {Chaitra V. Hegde and Shrikumar Patil},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2006.05477},
	url          = {https://arxiv.org/abs/2006.05477},
	eprinttype   = {arXiv},
	eprint       = {2006.05477},
	timestamp    = {Sat, 13 Jun 2020 18:28:13 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2006-05477.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hein2017formal,
	title        = {Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation},
	author       = {Hein, Matthias and Andriushchenko, Maksym},
	year         = {2017},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {30},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{hendrycks2019benchmarking,
	title        = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
	author       = {Dan Hendrycks and Thomas G. Dietterich},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1903.12261},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HJz6tiCqYm},
	timestamp    = {Thu, 25 Jul 2019 14:25:46 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/HendrycksD19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hendrycks2021natural,
	title        = {Natural adversarial examples},
	author       = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	year         = {2021},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {15262--15271}
}
@inproceedings{hermann2015teaching,
	title        = {Teaching Machines to Read and Comprehend},
	author       = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	year         = {2015},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {28},
	url          = {https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
	editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@article{hevner2004design,
	title        = {Design Science in Information Systems Research},
	author       = {Alan R. Hevner and Salvatore T. March and Jinsoo Park and Sudha Ram},
	year         = {2004},
	journal      = {MIS Quarterly},
	publisher    = {Management Information Systems Research Center, University of Minnesota},
	volume       = {28},
	number       = {1},
	pages        = {75--105},
	issn         = {02767783},
	url          = {http://www.jstor.org/stable/25148625},
	urldate      = {2022-08-28},
	abstract     = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.}
}
@article{hickey1993stylistics,
	title        = {Stylistics, Pragmatics and Pragmastylistics},
	author       = {Leo Hickey},
	year         = {1993},
	journal      = {Revue belge de philologie et d'histoire},
	publisher    = {Pers{\'e}e-Portail des revues scientifiques en SHS},
	volume       = {71},
	number       = {3},
	pages        = {573--586},
	doi          = {10.3406/rbph.1993.3890},
	url          = {https://www.persee.fr/doc/rbph_0035-0818_1993_num_71_3_3890}
}
@article{hidasi2015session,
	title        = {Session-based recommendations with recurrent neural networks},
	author       = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
	year         = {2015},
	journal      = {arXiv preprint arXiv:1511.06939}
}
@inproceedings{hidasi2018recurrent,
	title        = {Recurrent neural networks with top-k gains for session-based recommendations},
	author       = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
	year         = {2018},
	booktitle    = {Proceedings of the 27th ACM international conference on information and knowledge management},
	pages        = {843--852}
}
@article{hindle2016naturalness,
	title        = {On the naturalness of software},
	author       = {Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
	year         = {2016},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = {59},
	number       = {5},
	pages        = {122--131}
}
@inproceedings{hirst2003paraphrasing,
	title        = {Paraphrasing paraphrased},
	author       = {Hirst, Graeme},
	year         = {2003},
	booktitle    = {Keynote address for The Second International Workshop on Paraphrasing: Paraphrase acquisition and Applications}
}
@article{hjorland2015classical,
	title        = {Classical databases and knowledge organization: A case for boolean retrieval and human decision-making during searches},
	author       = {Hj{\o}rland, Birger},
	year         = {2015},
	journal      = {Journal of the Association for Information Science and Technology},
	publisher    = {Wiley Online Library},
	volume       = {66},
	number       = {8},
	pages        = {1559--1575}
}
@inproceedings{ho2023sbtrec,
	title        = {SBTREC - A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis},
	author       = {Ho, Ngai Lam and Lee, Roy Ka-Wei and Lim, Kwan Hui},
	year         = {2023},
	booktitle    = {2023 IEEE International Conference on Big Data (BigData)},
	volume       = {},
	number       = {},
	pages        = {5790--5798},
	doi          = {10.1109/BigData59044.2023.10386486},
	keywords     = {Training;Sentiment analysis;Analytical models;Urban areas;Logic gates;Prediction algorithms;Transformers;Recommendation Systems;Neural Networks;Word Embedding;Self-Attention;Transformer}
}
@article{hodonu2024rise,
	title        = {The rise of artificial intelligence in libraries: the ethical and equitable methodologies, and prospects for empowering library users},
	author       = {Hodonu-Wusu, James Oluwaseyi},
	year         = {2024},
	month        = {Feb},
	day          = {19},
	journal      = {AI and Ethics},
	doi          = {10.1007/s43681-024-00432-7},
	issn         = {2730-5961},
	url          = {https://doi.org/10.1007/s43681-024-00432-7},
	abstract     = {Libraries have always been a repository of knowledge, a place where people from all walks of life can come to learn and grow. In today's fast-paced world, libraries must keep up with the changing times in order to remain relevant. Artificial intelligence (AI) is one of the most promising technologies that can help libraries achieve this goal. AI can help libraries automate processes, provide personalized services, and improve user experiences. However, with great power comes great responsibility, and AI is no exception. Libraries have an ethical and equitable promise to their users, and AI must be deployed in a way that upholds these promises. This study explores the ethical and equitable use of AI in libraries, how it can empower users, and what librarians need to consider when implementing AI. The result of the reviewed articles showed that 1499 out of 170,262 papers have been identified as describing AI in libraries through ethical and equitable methodologies and prospect for empowering library users. The future studies can focus on other professional terms, such as Trustworthy AI, Fairness in AI, Explainable AI, and Human-in-the-loop, and how this can impact libraries, and other professionals.}
}
@article{hodosh2013framing,
	title        = {Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics},
	author       = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
	year         = {2013},
	month        = {may},
	journal      = {J. Artif. Int. Res.},
	publisher    = {AI Access Foundation},
	address      = {El Segundo, CA, USA},
	volume       = {47},
	number       = {1},
	pages        = {853--899},
	issn         = {1076-9757},
	issue_date   = {May 2013},
	abstract     = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
	numpages     = {47}
}
@article{hoffart2013yago2,
	title        = {YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia},
	author       = {Johannes Hoffart and Fabian M. Suchanek and Klaus Berberich and Gerhard Weikum},
	year         = {2013},
	journal      = {Artificial Intelligence},
	volume       = {194},
	pages        = {28--61},
	doi          = {https://doi.org/10.1016/j.artint.2012.06.001},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370212000719},
	note         = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
	keywords     = {Ontologies, Knowledge bases, Spatio-temporal facts, Information extraction},
	abstract     = {We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an accuracy of 95\% of the facts in YAGO2. In this paper, we present the extraction methodology, the integration of the spatio-temporal dimension, and our knowledge representation SPOTL, an extension of the original SPO-triple model to time and space.}
}
@inproceedings{hoffer2015deep,
	title        = {Deep Metric Learning Using Triplet Network},
	author       = {Hoffer, Elad and Ailon, Nir},
	year         = {2015},
	booktitle    = {Similarity-Based Pattern Recognition},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {84--92},
	isbn         = {978-3-319-24261-3},
	editor       = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
	abstract     = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.}
}
@inproceedings{hofmann1999probabilistic,
	title        = {Probabilistic Latent Semantic Analysis},
	author       = {Thomas Hofmann},
	year         = {1999},
	booktitle    = {{UAI} '99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, Stockholm, Sweden, July 30 - August 1, 1999},
	publisher    = {Morgan Kaufmann},
	pages        = {289--296},
	url          = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=179\&proceeding\_id=15},
	editor       = {Kathryn B. Laskey and Henri Prade},
	timestamp    = {Wed, 03 Feb 2021 11:09:31 +0100},
	biburl       = {https://dblp.org/rec/conf/uai/Hofmann99.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{holland1992genetic,
	title        = {Genetic algorithms},
	author       = {John H. Holland},
	year         = {2012},
	journal      = {Scholarpedia},
	volume       = {7},
	number       = {12},
	pages        = {1482},
	doi          = {10.4249/scholarpedia.1482},
	url          = {https://doi.org/10.4249/scholarpedia.1482},
	timestamp    = {Thu, 23 May 2019 15:09:50 +0200},
	biburl       = {https://dblp.org/rec/journals/scholarpedia/Holland12.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hong2010empirical,
	title        = {Empirical Study of Topic Modeling in Twitter},
	author       = {Hong, Liangjie and Davison, Brian D.},
	year         = {2010},
	booktitle    = {Proceedings of the First Workshop on Social Media Analytics},
	location     = {Washington D.C., District of Columbia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOMA '10},
	pages        = {80--88},
	doi          = {10.1145/1964858.1964870},
	isbn         = {9781450302173},
	url          = {https://doi.org/10.1145/1964858.1964870},
	abstract     = {Social networks such as Facebook, LinkedIn, and Twitter have been a crucial source of information for a wide spectrum of users. In Twitter, popular information that is deemed important by the community propagates through the network. Studying the characteristics of content in the messages becomes important for a number of tasks, such as breaking news detection, personalized message recommendation, friends recommendation, sentiment analysis and others. While many researchers wish to use standard text mining tools to understand messages on Twitter, the restricted length of those messages prevents them from being employed to their full potential.We address the problem of using standard topic models in micro-blogging environments by studying how the models can be trained on the dataset. We propose several schemes to train a standard topic model and compare their quality and effectiveness through a set of carefully designed experiments from both qualitative and quantitative perspectives. We show that by training a topic model on aggregated messages we can obtain a higher quality of learned model which results in significantly better performance in two real-world classification problems. We also discuss how the state-of-the-art Author-Topic model fails to model hierarchical relationships between entities in Social Media.},
	numpages     = {9},
	keywords     = {topic models, Twitter, social media}
}
@inproceedings{hu2004mining,
	title        = {Mining and Summarizing Customer Reviews},
	author       = {Hu, Minqing and Liu, Bing},
	year         = {2004},
	booktitle    = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Seattle, WA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '04},
	pages        = {168–177},
	doi          = {10.1145/1014052.1014073},
	isbn         = {1581138881},
	url          = {https://doi.org/10.1145/1014052.1014073},
	abstract     = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
	numpages     = {10},
	keywords     = {summarization, text mining, sentiment classification, reviews}
}
@article{hu2012manipulation,
	title        = {Manipulation of online reviews: An analysis of ratings, readability, and sentiments},
	author       = {Nan Hu and Indranil Bose and Noi Sian Koh and Ling Liu},
	year         = {2012},
	journal      = {Decision Support Systems},
	volume       = {52},
	number       = {3},
	pages        = {674--684},
	doi          = {https://doi.org/10.1016/j.dss.2011.11.002},
	issn         = {0167-9236},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167923611002065},
	keywords     = {Manipulation, Online reviews, Ratings, Readability, Runs test, Sentiments, Text mining},
	abstract     = {As consumers become increasingly reliant on online reviews to make purchase decisions, the sales of the product becomes dependent on the word of mouth (WOM) that it generates. As a result, there can be attempts by firms to manipulate online reviews of products to increase their sales. Despite the suspicion on the existence of such manipulation, the amount of such manipulation is unknown, and deciding which reviews to believe in is largely based on the reader's discretion and intuition. Therefore, the success of the manipulation of reviews by firms in generating sales of products is unknown. In this paper, we propose a simple statistical method to detect online reviews manipulation, and assess how consumers respond to products with manipulated reviews. In particular, the writing style of reviewers is examined, and the effectiveness of manipulation through ratings, sentiments, and readability is investigated. Our analysis examines textual information available in online reviews by combining sentiment mining techniques with readability assessments. We discover that around 10.3\% of the products are subject to online reviews manipulation. In spite of the deliberate use of sentiments and ratings in manipulated products, consumers are only able to detect manipulation taking place through ratings, but not through sentiments. The findings from this research ensue a note of caution for all consumers that rely on online reviews of books for making purchases, and encourage them to delve deep into the book reviews without being deceived by fraudulent manipulation.}
}
@inproceedings{hu2013spatial,
	title        = {Spatial Topic Modeling in Online Social Media for Location Recommendation},
	author       = {Hu, Bo and Ester, Martin},
	year         = {2013},
	booktitle    = {Proceedings of the 7th ACM Conference on Recommender Systems},
	location     = {Hong Kong, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {RecSys '13},
	pages        = {25--32},
	doi          = {10.1145/2507157.2507174},
	isbn         = {9781450324090},
	url          = {https://doi.org/10.1145/2507157.2507174},
	abstract     = {Mobile networks enable users to post on social media services (e.g., Twitter) from anywhere. The activities of mobile users involve three major entities: user, post, and location. The interaction of these entities is the key to answer questions such as who will post a message where and on what topic? In this paper, we address the problem of profiling mobile users by modeling their activities, i.e., we explore topic modeling considering the spatial and textual aspects of user posts, and predict future user locations. We propose the first ST (Spatial Topic) model to capture the correlation between users' movements and between user interests and the function of locations. We employ the sparse coding technique which greatly speeds up the learning process. We perform experiments on two real life data sets from Twitter and Yelp. Through comprehensive experiments, we demonstrate that our proposed model consistently improves the average precision@1,5,10,15,20 for location recommendation by at least 50\% (Twitter) and 300\% (Yelp) against existing state-of-the-art recommendation algorithms and geographical topic models.},
	numpages     = {8},
	keywords     = {mobile users, spatial topic model, location recommendation}
}
@inproceedings{hu2017diversifying,
	title        = {Diversifying personalized recommendation with user-session context.},
	author       = {Hu, Liang and Cao, Longbing and Wang, Shoujin and Xu, Guandong and Cao, Jian and Gu, Zhiping},
	year         = {2017},
	booktitle    = {IJCAI},
	pages        = {1858--1864}
}
@inproceedings{hu2020metric,
	title        = {Metric-Free Individual Fairness with Cooperative Contextual Bandits},
	author       = {Hu, Qian and Rangwala, Huzefa},
	year         = {2020},
	booktitle    = {2020 IEEE International Conference on Data Mining (ICDM)},
	volume       = {},
	number       = {},
	pages        = {182--191},
	doi          = {10.1109/ICDM50108.2020.00027},
	keywords     = {Measurement;Machine learning algorithms;Stochastic processes;Machine learning;Information retrieval;Data mining;Recommender systems;decision making;contextual bandits;Fair machine learning}
}
@inproceedings{hu2020tf,
	title        = {TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search},
	author       = {Hu, Yibo and Wu, Xiang and He, Ran},
	year         = {2020},
	booktitle    = {Computer Vision -- ECCV 2020},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {123--139},
	isbn         = {978-3-030-58555-6},
	editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	abstract     = {With the flourish of differentiable neural architecture search (NAS), automatically searching latency-constrained architectures gives a new perspective to reduce human labor and expertise. However, the searched architectures are usually suboptimal in accuracy and may have large jitters around the target latency. In this paper, we rethink three freedoms of differentiable NAS, i.e. operation-level, depth-level and width-level, and propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good classification accuracy and precise latency constraint. For the operation-level, we present a bi-sampling search algorithm to moderate the operation collapse. For the depth-level, we introduce a sink-connecting search space to ensure the mutual exclusion between skip and other candidate operations, as well as eliminate the architecture redundancy. For the width-level, we propose an elasticity-scaling strategy that achieves precise latency constraint in a progressively fine-grained manner. Experiments on ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched TF-NAS-A obtains 76.9{\%} top-1 accuracy, achieving state-of-the-art results with less latency. Code is available at https://github.com/AberHu/TF-NAS.}
}
@article{hu2024towards,
	title        = {Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach},
	author       = {Hu, Wentao and Fang, Hui},
	year         = {2024},
	journal      = {ACM Transactions on Knowledge Discovery from Data},
	publisher    = {ACM New York, NY},
	volume       = {18},
	number       = {5},
	pages        = {1--21}
}
@inproceedings{huang2008labeled,
	title        = {{Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments}},
	author       = {Huang, Gary B. and Mattar, Marwan and Berg, Tamara and Learned-Miller, Eric},
	year         = {2008},
	month        = {Oct},
	booktitle    = {{Workshop on Faces in 'Real-Life' Images: Detection, Alignment, and Recognition}},
	address      = {Marseille, France},
	url          = {https://hal.inria.fr/inria-00321923},
	organization = {{Erik Learned-Miller and Andras Ferencz and Fr{\'e}d{\'e}ric Jurie}},
	pdf          = {https://hal.inria.fr/inria-00321923/file/Huang_long_eccv2008-lfw.pdf},
	hal_id       = {inria-00321923},
	hal_version  = {v1}
}
@inproceedings{huang2017safety,
	title        = {Safety Verification of Deep Neural Networks},
	author       = {Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
	year         = {2017},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {3--29},
	isbn         = {978-3-319-63387-9},
	editor       = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	abstract     = {Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.}
}
@inproceedings{huang2017snapshot,
	title        = {Snapshot Ensembles: Train 1, Get {M} for Free},
	author       = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJYwwY9ll},
	timestamp    = {Fri, 18 Nov 2022 15:40:45 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/HuangLP0HW17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{huang2018improving,
	title        = {Improving sequential recommendation with knowledge-enhanced memory networks},
	author       = {Huang, Jin and Zhao, Wayne Xin and Dou, Hongjian and Wen, Ji-Rong and Chang, Edward Y},
	year         = {2018},
	booktitle    = {The 41st international ACM SIGIR conference on research \& development in information retrieval},
	pages        = {505--514}
}
@article{huang2020bridging,
	title        = {Bridging the Performance Gap between {FGSM} and {PGD} Adversarial Training},
	author       = {Tianjin Huang and Vlado Menkovski and Yulong Pei and Mykola Pechenizkiy},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2011.05157},
	url          = {https://arxiv.org/abs/2011.05157},
	eprinttype   = {arXiv},
	eprint       = {2011.05157},
	timestamp    = {Thu, 12 Nov 2020 15:14:56 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2011-05157.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{huang2020curricularface,
	title        = {CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition},
	author       = {Huang, Yuge and Wang, Yuhan and Tai, Ying and Liu, Xiaoming and Shen, Pengcheng and Li, Shaoxin and Li, Jilin and Huang, Feiyue},
	year         = {2020},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{huang2021unlearnable,
	title        = {Unlearnable Examples: Making Personal Data Unexploitable},
	author       = {Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey and Yisen Wang},
	year         = {2021},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=iAmZUo0DxC0}
}
@phdthesis{hummels1987nonparametric,
	title        = {Nonparametric estimation of the Bayes error},
	author       = {Hummels, Donald Michael},
	year         = {1987},
	url          = {https://docs.lib.purdue.edu/dissertations/AAI8814491/},
	school       = {Purdue University}
}
@inproceedings{huster2019limitations,
	title        = {Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples},
	author       = {Huster, Todd and Chiang, Cho-Yu Jason and Chadha, Ritu},
	year         = {2019},
	booktitle    = {ECML PKDD 2018 Workshops},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {16--29},
	isbn         = {978-3-030-13453-2},
	editor       = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and Garc{\'i}a-Mart{\'i}n, Eva and Gavald{\`a}, Ricard and Koprinska, Irena and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
	abstract     = {Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses.}
}
@misc{IEC61508,
	title        = {Functional safety of electrical/electronic/programmable electronic safety-related systems -- Part 7: Overview of techniques and measures},
	author       = {International Electrotechnical Commission},
	year         = {2010},
	note         = {IEC 61508-7:2010},
	howpublished = {Annex D, page 126}
}
@article{image_adv,
	title        = {Multiloss Adversarial Attacks for Multimodal Remote Sensing Image Classification},
	author       = {Qi Hu and Zhidong Shen and Zongyao Sha and Weijie Tan},
	year         = {2024},
	journal      = {{IEEE} Trans. Geosci. Remote. Sens.},
	volume       = {62},
	pages        = {1--13},
	doi          = {10.1109/TGRS.2024.3384927},
	url          = {https://doi.org/10.1109/TGRS.2024.3384927},
	timestamp    = {Sat, 04 May 2024 10:55:27 +0200},
	biburl       = {https://dblp.org/rec/journals/tgrs/HuSST24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ishida2022performance,
	title        = {Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification},
	author       = {Takashi Ishida and Ikko Yamane and Nontawat Charoenphakdee and Gang Niu and Masashi Sugiyama},
	year         = {2023},
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=FZdJQgy05rz}
}
@article{islam2008semantic,
	title        = {Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity},
	author       = {Islam, Aminul and Inkpen, Diana},
	year         = {2008},
	month        = {jul},
	journal      = {ACM Trans. Knowl. Discov. Data},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {2},
	number       = {2},
	doi          = {10.1145/1376815.1376819},
	issn         = {1556-4681},
	url          = {https://doi.org/10.1145/1376815.1376819},
	issue_date   = {July 2008},
	abstract     = {We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.},
	articleno    = {10},
	numpages     = {25},
	keywords     = {Semantic similarity of words, corpus-based measures, similarity of short texts}
}
@book{iverson1962programming,
	title        = {A Programming Language},
	author       = {Iverson, Kenneth E.},
	year         = {1962},
	publisher    = {John Wiley \&amp; Sons, Inc.},
	address      = {USA},
	isbn         = {0471430145},
	abstract     = {From the PrefaceApplied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or programs. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a programming language.Much of applied mathematics, particularly the more recent computer-related areas which cut across the older disciplines, suffers from the lack of an adequate programming language. It is the central thesis of this book that the descriptive and analytic power of an adequate programming language amply repays the considerable effort required for its mastery. This thesis is developed by first presenting the entire language and then applying it in later chapters to several major topics.The areas of application are chosen primarily for their intrinsic interest and lack of previous treatment, but they are also designed to illustrate the universality and other facets of the language. For example, the microprogramming of Chapter 2 illustrates the divisibility of the language, i.e., the ability to treat a restricted area using only a small portion of the complete language. Chapter 6 (Sorting) shows its capacity to compass a relatively complex and detailed topic in a short space. Chapter 7 (The Logical Calculus) emphasizes the formal manipulability of the language and its utility in theoretical work.The material was developed largely in a graduate course given for several years at Harvard and in a later course presented repeatedly at the IBM Systems Research Institute in New York. It should prove suitable for a two-semester course at the senior or graduate level. Although for certain audiences an initial presentation of the entire language may be appropriate, I have found it helpful to motivate the development by presenting the minimum notation required for a given topic, proceeding to its treatment (e.g., microprogramming), and then returning to further notation. The 130-odd problems not only provide the necessary finger exercises but also develop results of general interest.Chapter 1 or some part of it is prerequisite to each of the remaining "applications" chapters, but the applications chapters are virtually independent of one another. A complete appreciation of search techniques (Chapter 4) does, however, require a knowledge of methods of representation (Chapter 3). The cross references which do occur in the applications chapters are either nonessential or are specific to a given figure, table, or program. The entire language presented in Chapter 1 is summarized for reference at the end of the book.}
}
@misc{iyer2017qqp,
	title        = {First Quora Dataset Release: Question Pairs},
	author       = {Shankar Iyer and Nikhil Dandekar and Kornél Csernai},
	year         = {2017},
	url          = {https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}
}
@article{jacobi2016quantitative,
	title        = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	author       = {Carina Jacobi and Wouter van Atteveldt and Kasper Welbers},
	year         = {2016},
	journal      = {Digital Journalism},
	publisher    = {Routledge},
	volume       = {4},
	number       = {1},
	pages        = {89--106},
	doi          = {10.1080/21670811.2015.1093271},
	url          = {https://doi.org/10.1080/21670811.2015.1093271},
	eprint       = {https://doi.org/10.1080/21670811.2015.1093271}
}
@inproceedings{jagielski2019differentially,
	title        = {Differentially Private Fair Learning},
	author       = {Jagielski, Matthew and Kearns, Michael and Mao, Jieming and Oprea, Alina and Roth, Aaron and -Malvajerdi, Saeed Sharifi and Ullman, Jonathan},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {3000--3008},
	url          = {https://proceedings.mlr.press/v97/jagielski19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf},
	abstract     = {Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.}
}
@book{jain2011handbook,
	title        = {Handbook of Face Recognition, 2nd Edition},
	year         = {2011},
	publisher    = {Springer},
	doi          = {10.1007/978-0-85729-932-1},
	isbn         = {978-0-85729-931-4},
	url          = {https://doi.org/10.1007/978-0-85729-932-1},
	editor       = {Stan Z. Li and Anil K. Jain},
	timestamp    = {Fri, 27 Oct 2017 15:34:03 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0027896.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%misc{janecyk2020,
  author    = {Janecyk v. International Business Machines},
  title     = {Circuit Court of Cook County, Illinois, January, 2020, 2020-CH-833 (United States)},
  year      = {2020},
  month     = {jan},
  day       = {},
  note      = {Court Case},
  url       = {https://perkinscoie.com/insights/update/new-biometrics-lawsuits-signal-potential-legal-risks-ai},
}
@misc{janecyk2020,
	title        = {District Court, N.D. Illinois, Illinois, filed February 3, 2020, 1:20-cv-00783 (United States)},
	author       = {Janecyk v. International Business Machines Corporation},
	year         = {2020},
	month        = {feb},
	day          = {3},
	url          = {https://www.courthousenews.com/wp-content/uploads/2020/03/Conure.pdf},
	note         = {Court Case}
}
@article{jaro1989advances,
	title        = {Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida},
	author       = {Matthew A.   Jaro},
	year         = {1989},
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = {84},
	number       = {406},
	pages        = {414--420},
	doi          = {10.1080/01621459.1989.10478785},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478785},
	eprint       = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478785}
}
@article{jarvelin2002cumulated,
	title        = {Cumulated gain-based evaluation of IR techniques},
	author       = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
	year         = {2002},
	journal      = {ACM Transactions on Information Systems (TOIS)},
	publisher    = {ACM New York, NY, USA},
	volume       = {20},
	number       = {4},
	pages        = {422--446}
}
@article{jensen2011measuring,
	title        = {Measuring spatial dispersion: exact results on the variance of random spatial distributions},
	author       = {Jensen, Pablo and Michel, Julien},
	year         = {2011},
	month        = {Aug},
	day          = {01},
	journal      = {The Annals of Regional Science},
	volume       = {47},
	number       = {1},
	pages        = {81--110},
	doi          = {10.1007/s00168-009-0342-3},
	issn         = {1432-0592},
	url          = {https://doi.org/10.1007/s00168-009-0342-3},
	abstract     = {Measuring the spatial distribution of locations of many entities (trees, atoms, economic activities, etc.), and, more precisely, the deviations from purely random configurations, is a powerful method to unravel their underlying interactions. Several coefficients have been developed in the past to quantify the possible deviations. It is important to quantify the variances of the coefficients for random distributions, to ascertain the statistical significance of an empirical deviation. By lack of a proper analytical expression, the significance is usually obtained by simulating many random configurations by Monte Carlo simulations. In the present paper, we present an exact analytical expression for the variance of several spatial coefficients for random distributions, and we rigorously show that these distributions asymptotically follow a Normal law. These two results eliminate the need for cumbersome Monte Carlo simulations. They also allow to understand qualitatively the main factors that may change the variance: number of sites, spatial inhomogeneity, etc.}
}
@article{jeong2019social,
	title        = {Social media mining for product planning: A product opportunity mining approach based on topic modeling and sentiment analysis},
	author       = {Byeongki Jeong and Janghyeok Yoon and Jae-Min Lee},
	year         = {2019},
	journal      = {International Journal of Information Management},
	volume       = {48},
	pages        = {280--290},
	doi          = {https://doi.org/10.1016/j.ijinfomgt.2017.09.009},
	issn         = {0268-4012},
	url          = {https://www.sciencedirect.com/science/article/pii/S0268401217302955},
	keywords     = {Product opportunity, New product development, Social media mining, Opportunity algorithm, Topic modeling, Sentiment analysis},
	abstract     = {Social media data have recently attracted considerable attention as an emerging voice of the customer as it has rapidly become a channel for exchanging and storing customer-generated, large-scale, and unregulated voices about products. Although product planning studies using social media data have used systematic methods for product planning, their methods have limitations, such as the difficulty of identifying latent product features due to the use of only term-level analysis and insufficient consideration of opportunity potential analysis of the identified features. Therefore, an opportunity mining approach is proposed in this study to identify product opportunities based on topic modeling and sentiment analysis of social media data. For a multifunctional product, this approach can identify latent product topics discussed by product customers in social media using topic modeling, thereby quantifying the importance of each product topic. Next, the satisfaction level of each product topic is evaluated using sentiment analysis. Finally, the opportunity value and improvement direction of each product topic from a customer-centered view are identified by an opportunity algorithm based on product topics' importance and satisfaction. We expect that our approach for product planning will contribute to the systematic identification of product opportunities from large-scale customer-generated social media data and will be used as a real-time monitoring tool for changing customer needs analysis in rapidly evolving product environments.}
}
@inproceedings{jia2022certified,
	title        = {Certified robustness of nearest neighbors against data poisoning and backdoor attacks},
	author       = {Jia, Jinyuan and Liu, Yupei and Cao, Xiaoyu and Gong, Neil Zhenqiang},
	year         = {2022},
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = {36},
	number       = {9},
	pages        = {9575--9583}
}
@article{jiang2015feature,
	title        = {Feature-based approaches to semantic similarity assessment of concepts using Wikipedia},
	author       = {Yuncheng Jiang and Xiaopei Zhang and Yong Tang and Ruihua Nie},
	year         = {2015},
	journal      = {Information Processing  \& Management},
	volume       = {51},
	number       = {3},
	pages        = {215--234},
	doi          = {https://doi.org/10.1016/j.ipm.2015.01.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457315000023},
	keywords     = {Concept similarity, Semantic similarity, Semantic relatedness, Feature-based measures, Wikipedia},
	abstract     = {Semantic similarity assessment between concepts is an important task in many language related applications. In the past, several approaches to assess similarity by evaluating the knowledge modeled in an (or multiple) ontology (or ontologies) have been proposed. However, there are some limitations such as the facts of relying on predefined ontologies and fitting non-dynamic domains in the existing measures. Wikipedia provides a very large domain-independent encyclopedic repository and semantic network for computing semantic similarity of concepts with more coverage than usual ontologies. In this paper, we propose some novel feature based similarity assessment methods that are fully dependent on Wikipedia and can avoid most of the limitations and drawbacks introduced above. To implement similarity assessment based on feature by making use of Wikipedia, firstly a formal representation of Wikipedia concepts is presented. We then give a framework for feature based similarity based on the formal representation of Wikipedia concepts. Lastly, we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework. The evaluation, based on several widely used benchmarks and a benchmark developed in ourselves, sustains the intuitions with respect to human judgements. Overall, several methods proposed in this paper have good human correlation and constitute some effective ways of determining similarity between Wikipedia concepts.}
}
@inproceedings{jiang2015travel,
	title        = {Travel Recommendation via Author Topic Model Based Collaborative Filtering},
	author       = {Jiang, Shuhui and Qian, Xueming and Shen, Jialie and Mei, Tao},
	year         = {2015},
	booktitle    = {MultiMedia Modeling},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {392--402},
	isbn         = {978-3-319-14442-9},
	editor       = {He, Xiangjian and Luo, Suhuai and Tao, Dacheng and Xu, Changsheng and Yang, Jie and Hasan, Muhammad Abul},
	abstract     = {While automatic travel recommendation has attracted a lot of attentions, the existing approaches generally suffer from different kinds of weaknesses. For example, sparsity problem can significantly degrade the performance of traditional collaborative filtering (CF). If a user only visits very few locations, accurate similar user identification becomes very challenging due to lack of sufficient information. Motivated by this concern, we propose an Author Topic Collaborative Filtering (ATCF) method to facilitate comprehensive Points of Interest (POIs) recommendation for social media users. In our approach, the topics about user preference (e.g., cultural, cityscape, or landmark) are extracted from the textual description of photos by author topic model instead of from GPS (geo-tag). Consequently, unlike CF based approaches, even without GPS records, similar users could still be identified accurately according to the similarity of users' topic preferences. In addition, ATCF doesn't pre-define the category of travel topics. The category and user topic preference could be elicited simultaneously. Experiment results with a large test collection demonstrate various kinds of advantages of our approach.}
}
@article{jiang2017wikipedia,
	title        = {Wikipedia-based information content and semantic similarity computation},
	author       = {Yuncheng Jiang and Wen Bai and Xiaopei Zhang and Jiaojiao Hu},
	year         = {2017},
	journal      = {Information Processing  \& Management},
	volume       = {53},
	number       = {1},
	pages        = {248--265},
	doi          = {https://doi.org/10.1016/j.ipm.2016.09.001},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457316303934},
	keywords     = {Information content, Semantic similarity, Concept similarity, Wikipedia, Category structure},
	abstract     = {The Information Content (IC) of a concept is a fundamental dimension in computational linguistics. It enables a better understanding of concept's semantics. In the past, several approaches to compute IC of a concept have been proposed. However, there are some limitations such as the facts of relying on corpora availability, manual tagging, or predefined ontologies and fitting non-dynamic domains in the existing methods. Wikipedia provides a very large domain-independent encyclopedic repository and semantic network for computing IC of concepts with more coverage than usual ontologies. In this paper, we propose some novel methods to IC computation of a concept to solve the shortcomings of existing approaches. The presented methods focus on the IC computation of a concept (i.e., Wikipedia category) drawn from the Wikipedia category structure. We propose several new IC-based measures to compute the semantic similarity between concepts. The evaluation, based on several widely used benchmarks and a benchmark developed in ourselves, sustains the intuitions with respect to human judgments. Overall, some methods proposed in this paper have a good human correlation and constitute some effective ways of determining IC values for concepts and semantic similarity between concepts.}
}
@inproceedings{jin2022input,
	title        = {Input-agnostic Certified Group Fairness via {G}aussian Parameter Smoothing},
	author       = {Jin, Jiayin and Zhang, Zeru and Zhou, Yang and Wu, Lingfei},
	year         = {2022},
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {162},
	pages        = {10340--10361},
	url          = {https://proceedings.mlr.press/v162/jin22g.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/jin22g/jin22g.pdf},
	abstract     = {Only recently, researchers attempt to provide classification algorithms with provable group fairness guarantees. Most of these algorithms suffer from harassment caused by the requirement that the training and deployment data follow the same distribution. This paper proposes an input-agnostic certified group fairness algorithm, FairSmooth, for improving the fairness of classification models while maintaining the remarkable prediction accuracy. A Gaussian parameter smoothing method is developed to transform base classifiers into their smooth versions. An optimal individual smooth classifier is learnt for each group with only the data regarding the group and an overall smooth classifier for all groups is generated by averaging the parameters of all the individual smooth ones. By leveraging the theory of nonlinear functional analysis, the smooth classifiers are reformulated as output functions of a Nemytskii operator. Theoretical analysis is conducted to derive that the Nemytskii operator is smooth and induces a Frechet differentiable smooth manifold. We theoretically demonstrate that the smooth manifold has a global Lipschitz constant that is independent of the domain of the input data, which derives the input-agnostic certified group fairness.}
}
@book{jing1994association,
	title        = {An association thesaurus for information retrieval},
	author       = {Jing, Yufeng and Croft, W Bruce},
	year         = {1994},
	publisher    = {University of Massachusetts, Department of Computer Science}
}
@book{jm3,
	title        = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
	author       = {Daniel Jurafsky and James H. Martin},
	year         = {2025},
	url          = {https://web.stanford.edu/~jurafsky/slp3/},
	note         = {Online manuscript released January 12, 2025},
	edition      = {3rd}
}
@inproceedings{joachims2002optimizing,
	title        = {Optimizing search engines using clickthrough data},
	author       = {Joachims, Thorsten},
	year         = {2002},
	booktitle    = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {133--142}
}
@inproceedings{john2020verifying,
	title        = {Verifying Individual Fairness in Machine Learning Models},
	author       = {George John, Philips and Vijaykeerthy, Deepak and Saha, Diptikalyan},
	year         = {2020},
	month        = {03--06 Aug},
	booktitle    = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {124},
	pages        = {749--758},
	url          = {https://proceedings.mlr.press/v124/george-john20a.html},
	editor       = {Peters, Jonas and Sontag, David},
	pdf          = {http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf},
	abstract     = {We consider the problem of whether a given decision model, working with structured data, has individual fairness. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an appropriate metric) but are treated differently by the model (different class label, or large difference in output), and it is unbiased (or fair) if no such pair exists. Our objective is to construct verifiers for proving individual fairness of a given model, and we do so by considering appropriate relaxations of the problem. We construct verifiers which are sound but not complete for linear classifiers, and kernelized polynomial/radial basis function classifiers. We also report the experimental results of evaluating our proposed algorithms on publicly available datasets.}
}
@inbook{jones1972statistical,
	title        = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
	author       = {Sparck Jones, Karen},
	year         = {1988},
	booktitle    = {Document Retrieval Systems},
	publisher    = {Taylor Graham Publishing},
	address      = {GBR},
	pages        = {132--142},
	isbn         = {0947568212},
	numpages     = {11}
}
@inproceedings{joseph2016fairness,
	title        = {Fairness in Learning: Classic and Contextual Bandits},
	author       = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie H and Roth, Aaron},
	year         = {2016},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {29},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2016/file/eb163727917cbba1eea208541a643e74-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{jovanovic2023fare,
	title        = {{FARE}: Provably Fair Representation Learning with Practical Certificates},
	author       = {Jovanovi\'{c}, Nikola and Balunovic, Mislav and Dimitrov, Dimitar Iliev and Vechev, Martin},
	year         = {2023},
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {202},
	pages        = {15401--15420},
	url          = {https://proceedings.mlr.press/v202/jovanovic23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/jovanovic23a/jovanovic23a.pdf},
	abstract     = {Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. Recent regulatory directives stress the need for FRL methods that provide practical certificates, i.e., provable upper bounds on the unfairness of any downstream classifier trained on preprocessed data, which directly provides assurance in a practical scenario. Creating such FRL methods is an important challenge that remains unsolved. In this work, we address that challenge and introduce FARE (Fairness with Restricted Encoders), the first FRL method with practical fairness certificates. FARE is based on our key insight that restricting the representation space of the encoder enables the derivation of practical guarantees, while still permitting favorable accuracy-fairness tradeoffs for suitable instantiations, such as one we propose based on fair trees. To produce a practical certificate, we develop and apply a statistical procedure that computes a finite sample high-confidence upper bound on the unfairness of any downstream classifier trained on FARE embeddings. In our comprehensive experimental evaluation, we demonstrate that FARE produces practical certificates that are tight and often even comparable with purely empirical results obtained by prior methods, which establishes the practical value of our approach.}
}
@inproceedings{jurman2009canberra,
	title        = {Canberra distance on ranked lists},
	author       = {Jurman, Giuseppe and Riccadonna, Samantha and Visintainer, Roberto and Furlanello, Cesare},
	year         = {2009},
	booktitle    = {Proceedings of advances in ranking NIPS 09 workshop},
	pages        = {22--27},
	organization = {Citeseer}
}
@inproceedings{kabbur2013fism,
	title        = {Fism: factored item similarity models for top-n recommender systems},
	author       = {Kabbur, Santosh and Ning, Xia and Karypis, George},
	year         = {2013},
	booktitle    = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages        = {659--667}
}
@inproceedings{kamigaito2020syntactically,
	title        = {Syntactically Look-Ahead Attention Network for Sentence Compression},
	author       = {Hidetaka Kamigaito and Manabu Okumura},
	year         = {2020},
	booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
	publisher    = {{AAAI} Press},
	pages        = {8050--8057},
	url          = {https://aaai.org/ojs/index.php/AAAI/article/view/6315},
	timestamp    = {Tue, 02 Feb 2021 08:00:36 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/KamigaitoO20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inbook{kamp1978semantics,
	title        = {Semantics Versus Pragmatics},
	author       = {Kamp, Hans},
	year         = {1978},
	booktitle    = {Formal Semantics and Pragmatics for Natural Languages},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {255--287},
	doi          = {10.1007/978-94-009-9775-2_9},
	isbn         = {978-94-009-9775-2},
	url          = {https://doi.org/10.1007/978-94-009-9775-2_9},
	editor       = {Guenthner, F. and Schmidt, S. J.},
	abstract     = {Consider the sentences(1)You may take an apple,(2)You may take a pear, and(3)You may take an apple or take a pear.}
}
@inproceedings{kang2004product,
	title        = {Product approximation by minimizing the upper bound of Bayes error rate for Bayesian combination of classifiers},
	author       = {Hee-Joong Kang and Doermann, D.},
	year         = {2004},
	booktitle    = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
	volume       = {1},
	number       = {},
	pages        = {252--255 Vol.1},
	doi          = {10.1109/ICPR.2004.1334071}
}
@inproceedings{kang2018self,
	title        = {Self-attentive sequential recommendation},
	author       = {Kang, Wang-Cheng and McAuley, Julian},
	year         = {2018},
	booktitle    = {2018 IEEE international conference on data mining (ICDM)},
	pages        = {197--206},
	organization = {IEEE}
}
@inproceedings{kang2022certifying,
	title        = {Certifying Some Distributional Fairness with Subpopulation Decomposition},
	author       = {Kang, Mintong and Li, Linyi and Weber, Maurice and Liu, Yang and Zhang, Ce and Li, Bo},
	year         = {2022},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {35},
	pages        = {31045--31058},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c8e9a2beb84ab1a616edb89581c4b32a-Paper-Conference.pdf},
	editor       = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh}
}
@article{kannan2018adversarial,
	title        = {Adversarial Logit Pairing},
	author       = {Harini Kannan and Alexey Kurakin and Ian J. Goodfellow},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1803.06373},
	url          = {http://arxiv.org/abs/1803.06373},
	eprinttype   = {arXiv},
	eprint       = {1803.06373},
	timestamp    = {Mon, 13 Aug 2018 16:47:15 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06373.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kaplan2020scaling,
	title        = {Scaling Laws for Neural Language Models},
	author       = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2001.08361},
	url          = {https://arxiv.org/abs/2001.08361},
	eprinttype   = {arXiv},
	eprint       = {2001.08361},
	timestamp    = {Wed, 03 Jun 2020 10:55:13 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kasiviswanathan2011can,
	title        = {What Can We Learn Privately?},
	author       = {Kasiviswanathan, Shiva Prasad and Lee, Homin K. and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
	year         = {2011},
	journal      = {SIAM Journal on Computing},
	volume       = {40},
	number       = {3},
	pages        = {793--826},
	doi          = {10.1137/090756090},
	url          = {https://doi.org/10.1137/090756090},
	eprint       = {https://doi.org/10.1137/090756090},
	abstract     = {Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask, What concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. Our goal is a broad understanding of the resources required for private learning in terms of samples, computation time, and interaction. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (nonprivate) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private probabilistically approximately correct learner for the class of parity functions. This result dispels the similarity between learning with noise and private learning (both must be robust to small changes in inputs), since parity is thought to be very hard to learn given random classification noise. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Therefore, for local private learning algorithms, the similarity to learning with noise is stronger: local learning is equivalent to SQ learning, and SQ algorithms include most known noise-tolerant learning algorithms. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms. Because of the equivalence to SQ learning, this result also separates adaptive and nonadaptive SQ learning.}
}
@inproceedings{katoen2016probabilistic,
	title        = {The probabilistic model checking landscape},
	author       = {Katoen, Joost-Pieter},
	year         = {2016},
	booktitle    = {Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science},
	pages        = {31--45}
}
@inproceedings{katz2017reluplex,
	title        = {Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
	author       = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
	year         = {2017},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {97--117},
	isbn         = {978-3-319-63387-9},
	editor       = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	abstract     = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.}
}
@inproceedings{katz2017towards,
	title        = {Towards Proving the Adversarial Robustness of Deep Neural Networks},
	author       = {Guy Katz and Clark Barrett and David L. Dill and Kyle Julian and Mykel J. Kochenderfer},
	year         = {2017},
	month        = sep,
	booktitle    = {Proceedings of the First Workshop on Formal Verification of Autonomous Vehicles (FVAV '17)},
	series       = {Electronic Proceedings in Theoretical Computer Science},
	volume       = {257},
	pages        = {19--26},
	url          = {http://eptcs.web.cse.unsw.edu.au/paper.cgi?FVAV2017.3},
	note         = {Turin, Italy},
	editor       = {Lukas Bulwahn and Maryam Kamali and Sven Linker}
}
@inproceedings{katz2019marabou,
	title        = {The Marabou Framework for Verification and Analysis of Deep Neural Networks},
	author       = {Katz, Guy and Huang, Derek A. and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'{c}}, Aleksandar and Dill, David L. and Kochenderfer, Mykel J. and Barrett, Clark},
	year         = {2019},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {443--452},
	isbn         = {978-3-030-25540-4},
	editor       = {Dillig, Isil and Tasiran, Serdar},
	abstract     = {Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.}
}
@inproceedings{ke2019automated,
	title        = {Automated Essay Scoring: A Survey of the State of the Art},
	author       = {Ke, Zixuan and Ng, Vincent},
	year         = {2019},
	month        = {7},
	booktitle    = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {6300--6308},
	doi          = {10.24963/ijcai.2019/879},
	url          = {https://doi.org/10.24963/ijcai.2019/879}
}
@inproceedings{kelley2013privacy,
	title        = {Privacy as part of the app decision-making process},
	author       = {Kelley, Patrick Gage and Cranor, Lorrie Faith and Sadeh, Norman},
	year         = {2013},
	booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	location     = {Paris, France},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '13},
	pages        = {3393–3402},
	doi          = {10.1145/2470654.2466466},
	isbn         = {9781450318990},
	url          = {https://doi.org/10.1145/2470654.2466466},
	abstract     = {Smartphones have unprecedented access to sensitive personal information. While users report having privacy concerns, they may not actively consider privacy while downloading apps from smartphone application marketplaces. Currently, Android users have only the Android permissions display, which appears after they have selected an app to download, to help them understand how applications access their information. We investigate how permissions and privacy could play a more active role in app-selection decisions. We designed a short "Privacy Facts' display, which we tested in a 20-participant lab study and a 366-participant online experiment. We found that by bringing privacy information to the user when they were making the decision and by presenting it in a clearer fashion, we could assist users in choosing applications that request fewer permissions.},
	numpages     = {10},
	keywords     = {privacy, mobile, interface, decision-making, android}
}
@article{kelly2021measuring,
	title        = {Measuring Technological Innovation over the Long Run},
	author       = {Kelly, Bryan and Papanikolaou, Dimitris and Seru, Amit and Taddy, Matt},
	year         = {2021},
	month        = {September},
	journal      = {American Economic Review: Insights},
	volume       = {3},
	number       = {3},
	pages        = {303--20},
	doi          = {10.1257/aeri.20190499},
	url          = {https://www.aeaweb.org/articles?id=10.1257/aeri.20190499}
}
@misc{kemp1988kendall,
	title        = {Kendall's Advanced Theory of Statistics, Volume 1, Distribution Theory.},
	author       = {Kemp, CD},
	year         = {1988},
	publisher    = {JSTOR}
}
@inproceedings{kevselj2003n,
	title        = {N-Gram Feature Selection for Authorship Identification},
	author       = {Houvardas, John and Stamatatos, Efstathios},
	year         = {2006},
	booktitle    = {Artificial Intelligence: Methodology, Systems, and Applications},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {77--86},
	isbn         = {978-3-540-40931-1},
	editor       = {Euzenat, J{\'e}r{\^o}me and Domingue, John},
	abstract     = {Automatic authorship identification offers a valuable tool for supporting crime investigation and security. It can be seen as a multi-class, single-label text categorization task. Character n-grams are a very successful approach to represent text for stylistic purposes since they are able to capture nuances in lexical, syntactical, and structural level. So far, character n-grams of fixed length have been used for authorship identification. In this paper, we propose a variable-length n-gram approach inspired by previous work for selecting variable-length word sequences. Using a subset of the new Reuters corpus, consisting of texts on the same topic by 50 different authors, we show that the proposed approach is at least as effective as information gain for selecting the most significant n-grams although the feature sets produced by the two methods have few common members. Moreover, we explore the significance of digits for distinguishing between authors showing that an increase in performance can be achieved using simple text pre-processing.}
}
@article{khedr2023certifair,
	title        = {CertiFair: A Framework for Certified Global Fairness of Neural Networks},
	author       = {Khedr, Haitham and Shoukry, Yasser},
	year         = {2023},
	month        = {Jun.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {37},
	number       = {7},
	pages        = {8237--8245},
	doi          = {10.1609/aaai.v37i7.25994},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/25994},
	abstractnote = {We consider the problem of whether a Neural Network (NN) model satisfies global individual fairness. Individual Fairness (defined in (Dwork et al. 2012)) suggests that similar individuals with respect to a certain task are to be treated similarly by the decision model. In this work, we have two main objectives. The first is to construct a verifier which checks whether the fairness property holds for a given NN in a classification task or provides a counterexample if it is violated, i.e., the model is fair if all similar individuals are classified the same, and unfair if a pair of similar individuals are classified differently. To that end, we construct a sound and complete verifier that verifies global individual fairness properties of ReLU NN classifiers using distance-based similarity metrics. The second objective of this paper is to provide a method for training provably fair NN classifiers from unfair (biased) data. We propose a fairness loss that can be used during training to enforce fair outcomes for similar individuals. We then provide provable bounds on the fairness of the resulting NN. We run experiments on commonly used fairness datasets that are publicly available and we show that global individual fairness can be improved by 96 % without a significant drop in test accuracy.}
}
@inproceedings{kifer2011no,
	title        = {No free lunch in data privacy},
	author       = {Kifer, Daniel and Machanavajjhala, Ashwin},
	year         = {2011},
	booktitle    = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
	location     = {Athens, Greece},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGMOD '11},
	pages        = {193–204},
	doi          = {10.1145/1989323.1989345},
	isbn         = {9781450306614},
	url          = {https://doi.org/10.1145/1989323.1989345},
	abstract     = {Differential privacy is a powerful tool for providing privacy-preserving noisy query answers over statistical databases. It guarantees that the distribution of noisy query answers changes very little with the addition or deletion of any tuple. It is frequently accompanied by popularized claims that it provides privacy without any assumptions about the data and that it protects against attackers who know all but one record. In this paper we critically analyze the privacy protections offered by differential privacy.First, we use a no-free-lunch theorem, which defines non-privacy as a game, to argue that it is not possible to provide privacy and utility without making assumptions about how the data are generated. Then we explain where assumptions are needed. We argue that privacy of an individual is preserved when it is possible to limit the inference of an attacker about the participation of the individual in the data generating process. This is different from limiting the inference about the presence of a tuple (for example, Bob's participation in a social network may cause edges to form between pairs of his friends, so that it affects more than just the tuple labeled as "Bob"). The definition of evidence of participation, in turn, depends on how the data are generated -- this is how assumptions enter the picture. We explain these ideas using examples from social network research as well as tabular data for which deterministic statistics have been previously released. In both cases the notion of participation varies, the use of differential privacy can lead to privacy breaches, and differential privacy does not always adequately limit inference about participation.},
	numpages     = {12},
	keywords     = {privacy, differential privacy}
}
@inproceedings{kifer2012private,
	title        = {Private Convex Empirical Risk Minimization and High-dimensional Regression},
	author       = {Kifer, Daniel and Smith, Adam and Thakurta, Abhradeep},
	year         = {2012},
	month        = {25--27 Jun},
	booktitle    = {Proceedings of the 25th Annual Conference on Learning Theory},
	publisher    = {PMLR},
	address      = {Edinburgh, Scotland},
	series       = {Proceedings of Machine Learning Research},
	volume       = {23},
	pages        = {25.1--25.40},
	url          = {https://proceedings.mlr.press/v23/kifer12.html},
	editor       = {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
	pdf          = {http://proceedings.mlr.press/v23/kifer12/kifer12.pdf},
	abstract     = {We consider \emphdifferentially private algorithms for convex empirical risk minimization (ERM). Differential privacy (Dwork et al., 2006b) is a recently introduced notion of privacy which guarantees that an algorithm’s output does not depend on the data of any individual in the dataset. This is crucial in fields that handle sensitive data, such as genomics, collaborative filtering, and economics. Our motivation is the design of private algorithms for sparse learning problems, in which one aims to find solutions (e.g., regression parameters) with few non-zero coefficients. To this end: (a) We significantly extend the analysis of the “objective perturbation” algorithm of Chaudhuri et al. (2011) for convex ERM problems. We show that their method can be modified to use less noise (be more accurate), and to apply to problems with hard constraints and non-differentiable regularizers. We also give a tighter, data-dependent analysis of the additional error introduced by their method. A key tool in our analysis is a new nontrivial limit theorem for differential privacy which is of independent interest: if a sequence of differentially private algorithms converges, in a \emphweak sense, then the limit algorithm is also differentially private. In particular, our methods give the best known algorithms for differentially private linear regression. These methods work in settings where the number of parameters p is less than the number of samples n. (b) We give the first two private algorithms for \emphsparse regression problems in high-dimensional settings, where p is much larger than n. We analyze their performance for linear regression: under standard assumptions on the data, our algorithms have vanishing empirical risk for n = poly(s, \log p) when there exists a good regression vector with s nonzero coefficients. Our algorithms demonstrate that randomized algorithms for sparse regression problems can be both stable and accurate - a combination which is impossible for deterministic algorithms.}
}
@inproceedings{kim2016convolutional,
	title        = {Convolutional matrix factorization for document context-aware recommendation},
	author       = {Kim, Donghyun and Park, Chanyoung and Oh, Jinoh and Lee, Sungyoung and Yu, Hwanjo},
	year         = {2016},
	booktitle    = {Proceedings of the 10th ACM conference on recommender systems},
	pages        = {233--240}
}
@article{kim2020torchattacks,
	title        = {Torchattacks: A pytorch repository for adversarial attacks},
	author       = {Kim, Hoki},
	year         = {2020},
	journal      = {arXiv preprint arXiv:2010.01950}
}
@article{kim2023solar,
	title        = {{SOLAR} 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling},
	author       = {Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},
	year         = {2023},
	journal      = {CoRR},
	volume       = {abs/2312.15166},
	doi          = {10.48550/ARXIV.2312.15166},
	url          = {https://doi.org/10.48550/arXiv.2312.15166},
	eprinttype   = {arXiv},
	eprint       = {2312.15166},
	timestamp    = {Tue, 23 Jan 2024 17:11:00 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2312-15166.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kim2025fairquant,
	title        = {FairQuant: Certifying and Quantifying Fairness of Deep Neural Networks},
	author       = {Brian Hyeongseok Kim and Jingbo Wang and Chao Wang},
	year         = {2025},
	booktitle    = {Proceedings of the 47th International Conference on Software Engineering (ICSE)},
	publisher    = {IEEE Press},
	address      = {Los Angeles, CA, USA},
	keywords     = {Fairness, Deep Neural Networks, Formal Certification, Symbolic Interval Analysis},
	abstract     = {We propose a method for certifying and quantifying individual fairness of deep neural networks (DNNs). Our method applies abstraction to symbolic interval-based analysis followed by refinement to compute the percentage of individuals whose classification outputs are provably fair. Our experiments show that the method is more accurate and faster than state-of-the-art techniques.}
}
@inproceedings{kingma2014adam,
	title        = {Adam: {A} Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = {2015},
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6980},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kingma2018glow,
	title        = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	author       = {Kingma, Durk P and Dhariwal, Prafulla},
	year         = {2018},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {31},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@inproceedings{knight2000statistics,
	title        = {Statistics-Based Summarization - Step One: Sentence Compression},
	author       = {Kevin Knight and Daniel Marcu},
	year         = {2000},
	booktitle    = {Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on on Innovative Applications of Artificial Intelligence, July 30 - August 3, 2000, Austin, Texas, {USA}},
	publisher    = {{AAAI} Press / The {MIT} Press},
	pages        = {703--710},
	url          = {http://www.aaai.org/Library/AAAI/2000/aaai00-108.php},
	editor       = {Henry A. Kautz and Bruce W. Porter},
	timestamp    = {Wed, 10 Feb 2021 08:43:54 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/KnightM00.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kohavi1995study,
	title        = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	author       = {Ron Kohavi},
	year         = {1995},
	booktitle    = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, {IJCAI} 95, Montr{\'{e}}al Qu{\'{e}}bec, Canada, August 20-25 1995, 2 Volumes},
	publisher    = {Morgan Kaufmann},
	pages        = {1137--1145},
	url          = {http://ijcai.org/Proceedings/95-2/Papers/016.pdf},
	timestamp    = {Tue, 20 Aug 2019 16:17:30 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/Kohavi95.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{komkov2021advhat,
	title        = {AdvHat: Real-World Adversarial Attack on ArcFace Face ID System},
	author       = {Komkov, Stepan and Petiushko, Aleksandr},
	year         = {2021},
	booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
	volume       = {},
	number       = {},
	pages        = {819--826},
	doi          = {10.1109/ICPR48806.2021.9412236}
}
@article{koppel2009computational,
	title        = {Computational methods in authorship attribution},
	author       = {Koppel, Moshe and Schler, Jonathan and Argamon, Shlomo},
	year         = {2009},
	journal      = {Journal of the American Society for information Science and Technology},
	publisher    = {Wiley Online Library},
	volume       = {60},
	number       = {1},
	pages        = {9--26}
}
@article{koupaee2018wikihow,
	title        = {WikiHow: {A} Large Scale Text Summarization Dataset},
	author       = {Mahnaz Koupaee and William Yang Wang},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1810.09305},
	url          = {http://arxiv.org/abs/1810.09305},
	eprinttype   = {arXiv},
	eprint       = {1810.09305},
	timestamp    = {Wed, 31 Oct 2018 14:24:29 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1810-09305.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kozarzewski2011similarity,
	title        = {Similarity of symbolic sequences},
	author       = {Kozarzewski, B},
	year         = {2011},
	journal      = {arXiv preprint arXiv:1108.1979}
}
@inproceedings{kreutz2022schenql,
	title        = {SchenQL: a query language for bibliographic data with aggregations and domain-specific functions},
	author       = {Kreutz, Christin Katharina and Blum, Martin and Schenkel, Ralf},
	year         = {2022},
	booktitle    = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
	location     = {Cologne, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {JCDL '22},
	doi          = {10.1145/3529372.3533282},
	isbn         = {9781450393454},
	url          = {https://doi.org/10.1145/3529372.3533282},
	abstract     = {Current search interfaces of digital libraries are not suitable to satisfy complex or convoluted information needs directly, when it comes to cases such as "Find authors who only recently started working on a topic". They might offer possibilities to obtain this information only by requiring vast user interaction.We present SchenQL, a web interface of a domain-specific query language on bibliographic metadata, which offers information search and exploration by query formulation and navigation in the system. Our system focuses on supporting aggregation of data and providing specialised domain dependent functions while being suitable for domain experts as well as casual users of digital libraries.},
	articleno    = {37},
	numpages     = {5},
	keywords     = {digital libraries, domain-specific query language, query formulation}
}
@inproceedings{kreutz2023evaluating,
	title        = {Evaluating Digital Library Search Systems by Using Formal Process Modelling},
	author       = {Kreutz, Christin Katharina and Blum, Martin and Schaer, Philipp and Schenkel, Ralf and Weyers, Benjamin},
	year         = {2023},
	booktitle    = {2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	volume       = {},
	number       = {},
	pages        = {1--12},
	doi          = {10.1109/JCDL57899.2023.00058},
	keywords     = {Adaptation models;Systematics;Soft sensors;Switches;Libraries;Data models;User experience;digital libraries;information-seeking behaviour;user study;qualitative evaluation;human focus}
}
@book{kreyszig1991introductory,
	title        = {Introductory Functional Analysis with Applications},
	author       = {Kreyszig, Erwin},
	year         = {2007},
	publisher    = {Wiley India Pvt. Limited},
	series       = {Wiley classics library},
	volume       = {17},
	isbn         = {9788126511914},
	url          = {https://books.google.com.sg/books?id=osXw-pRsptoC}
}
@article{krizhevsky2009learning,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Krizhevsky, Alex and Hinton, Geoffrey and others},
	year         = {2009},
	publisher    = {Toronto, ON, Canada}
}
@article{krokhmal2002portfolio,
	title        = {Portfolio optimization with conditional value-at-risk objective and constraints},
	author       = {Krokhmal, Pavlo and Palmquist, Jonas and Uryasev, Stanislav},
	year         = {2002},
	journal      = {Journal of risk},
	publisher    = {Citeseer},
	volume       = {4},
	pages        = {43--68}
}
@book{kumar2014twitter,
	title        = {Twitter Data Analytics},
	author       = {Shamanth Kumar and Fred Morstatter and Huan Liu},
	year         = {2014},
	publisher    = {Springer},
	series       = {Springer Briefs in Computer Science},
	doi          = {10.1007/978-1-4614-9372-3},
	isbn         = {978-1-4614-9372-3},
	url          = {https://doi.org/10.1007/978-1-4614-9372-3},
	timestamp    = {Tue, 18 Jun 2019 16:48:39 +0200},
	biburl       = {https://dblp.org/rec/books/sp/KumarML14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kurakin2016adversarial,
	title        = {Adversarial Machine Learning at Scale},
	author       = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJm4T4Kgx},
	timestamp    = {Thu, 25 Jul 2019 14:25:40 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/KurakinGB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kurakin2018adversarial,
	title        = {Adversarial examples in the physical world},
	author       = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HJGU3Rodl},
	timestamp    = {Thu, 04 Apr 2019 13:20:08 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/KurakinGB17a.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kusner2015word,
	title        = {From Word Embeddings To Document Distances},
	author       = {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
	year         = {2015},
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	volume       = {37},
	pages        = {957--966},
	url          = {https://proceedings.mlr.press/v37/kusnerb15.html},
	editor       = {Bach, Francis and Blei, David},
	pdf          = {http://proceedings.mlr.press/v37/kusnerb15.pdf},
	abstract     = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.}
}
@inproceedings{kusner2017counterfactual,
	title        = {Counterfactual Fairness},
	author       = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	year         = {2017},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {30},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{kwan2020understanding,
	title        = {Understanding Public Sentiments, Opinions and Topics about COVID-19 using Twitter},
	author       = {Shaynn-Ly Kwan, Jolin and Hui Lim, Kwan},
	year         = {2020},
	booktitle    = {2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
	volume       = {},
	number       = {},
	pages        = {623--626},
	doi          = {10.1109/ASONAM49781.2020.9381384}
}
@article{kwiatkowska2007stochastic,
	title        = {Stochastic model checking},
	author       = {Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	year         = {2007},
	journal      = {Formal Methods for Performance Evaluation: 7th International School on Formal Methods for the Design of Computer, Communication, and Software Systems, SFM 2007, Bertinoro, Italy, May 28-June 2, 2007, Advanced Lectures 7},
	publisher    = {Springer},
	pages        = {220--270}
}
@inproceedings{kwiatkowska2011prism,
	title        = {PRISM 4.0: Verification of probabilistic real-time systems},
	author       = {Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	year         = {2011},
	booktitle    = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23},
	pages        = {585--591},
	organization = {Springer}
}
@article{lahoti2019operationalizing,
	title        = {Operationalizing individual fairness with pairwise fair representations},
	author       = {Lahoti, Preethi and Gummadi, Krishna P. and Weikum, Gerhard},
	year         = {2019},
	month        = dec,
	journal      = {Proc. VLDB Endow.},
	publisher    = {VLDB Endowment},
	volume       = {13},
	number       = {4},
	pages        = {506–518},
	doi          = {10.14778/3372716.3372723},
	issn         = {2150-8097},
	url          = {https://doi.org/10.14778/3372716.3372723},
	issue_date   = {December 2019},
	abstract     = {We revisit the notion of individual fairness proposed by Dwork et al. A central challenge in operationalizing their approach is the difficulty in eliciting a human specification of a similarity metric. In this paper, we propose an operationalization of individual fairness that does not rely on a human specification of a distance metric. Instead, we propose novel approaches to elicit and leverage side-information on equally deserving individuals to counter subordination between social groups. We model this knowledge as a fairness graph, and learn a unified Pairwise Fair Representation (PFR) of the data that captures both data-driven similarity between individuals and the pairwise side-information in fairness graph. We elicit fairness judgments from a variety of sources, including human judgments for two real-world datasets on recidivism prediction (COMPAS) and violent neighborhood prediction (Crime \& Communities). Our experiments show that the PFR model for operationalizing individual fairness is practically viable.},
	numpages     = {13}
}
@inproceedings{lamontagne2006combining,
	title        = {Combining Multiple Similarity Metrics Using a Multicriteria Approach},
	author       = {Lamontagne, Luc and Abi-Zeid, Ir{\`e}ne},
	year         = {2006},
	booktitle    = {Advances in Case-Based Reasoning},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {415--428},
	isbn         = {978-3-540-36846-5},
	editor       = {Roth-Berghofer, Thomas R. and G{\"o}ker, Mehmet H. and G{\"u}venir, H. Altay},
	abstract     = {The design of a CBR system involves the use of similarity metrics. For many applications, various functions can be adopted to compare case features and to aggregate them into a global similarity measure. Given the availability of multiple similarity metrics, the designer is hence left with two options in order to come up with a working system: Either select one similarity metric or try to combine multiple metrics in a super-metric. In this paper, we study how techniques borrowed from multicriteria decision aid can be applied to CBR for combining the results of multiple similarity metrics. The problem of multi-metrics retrieval is presented as an instance of the problem of ranking alternatives based on multiple attributes. Discrete methods such as ELECTRE II have been proposed by the multicriteria decision aid community to address such situations. We conducted our experiments for ranking cases with ELECTRE II, a procedure based on pairwise comparisons. We used textual cases and multiple metrics. Our results indicate that the use of a combination of metrics with a multicriteria decision aid method can increase retrieval precision and provide an advantage over weighted sum combinations especially when similarity is measured on scales that are different in nature.}
}
@inproceedings{lan2019albert,
	title        = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language Representations},
	author       = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=H1eA7AEtvS},
	timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{landauer1997solution,
	title        = {A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
	author       = {Landauer, Thomas K. and Dumais, Susan T.},
	year         = {1997},
	journal      = {Psychological Review},
	publisher    = {American Psychological Association},
	address      = {US},
	volume       = {104},
	number       = {2},
	pages        = {211--240},
	doi          = {10.1037/0033-295X.104.2.211},
	url          = {https://doi.org/10.1037/0033-295X.104.2.211},
	keywords     = {*Knowledge Level; *Learning; *Semantics; *Theories; Psycholinguistics},
	abstract     = {How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena and problems are sketched. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}
@book{lanham1992revising,
	title        = {Revising prose},
	author       = {Lanham, Richard A and Stodel, James},
	year         = {1992},
	publisher    = {Macmillan Publishing Company}
}
@article{lansley2016geography,
	title        = {The geography of Twitter topics in London},
	author       = {Guy Lansley and Paul A. Longley},
	year         = {2016},
	journal      = {Computers, Environment and Urban Systems},
	volume       = {58},
	pages        = {85--96},
	doi          = {https://doi.org/10.1016/j.compenvurbsys.2016.04.002},
	issn         = {0198-9715},
	url          = {https://www.sciencedirect.com/science/article/pii/S0198971516300394},
	keywords     = {Social media, Twitter, Topic modelling, Latent Dirichlet Allocation, Geotemporal},
	abstract     = {Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time-stamped and (sometimes) precisely located. Such data can be mined to provide planners, marketers and researchers with useful information about activities and opinions across time and space. However, in their raw form, textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters. This paper explores the use of an unsupervised learning algorithm to classify geo-tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups, following extensive text cleaning techniques. Our classification identifies 20 distinctive and interpretive topic groupings, which represent key types of Tweets, from describing activities or informal conversations between users, to the use of check-in applets. Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users. Topics and attitudes expressed through Tweets are found to vary substantially across Inner London, and by time of day. Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio-economic characteristics of users, but place and local activities can also exert a considerable influence. Overall, the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London.}
}
@article{lastra2017hesml,
	title        = {HESML: A scalable ontology-based semantic similarity measures library with a set of reproducible experiments and a replication dataset},
	author       = {Juan J. Lastra-Díaz and Ana García-Serrano and Montserrat Batet and Miriam Fernández and Fernando Chirigati},
	year         = {2017},
	journal      = {Information Systems},
	volume       = {66},
	pages        = {97--118},
	doi          = {https://doi.org/10.1016/j.is.2017.02.002},
	issn         = {0306-4379},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306437916303246},
	keywords     = {HESML, PosetHERep, Semantic measures library, Ontology-based semantic similarity measures, Intrinsic and corpus-based Information Content models, Reproducible experiments on word similarity, WNSimRep v1 dataset, ReproZip, WordNet-based semantic similarity measures},
	abstract     = {This work is a detailed companion reproducibility paper of the methods and experiments proposed by Lastra-Díaz and García-Serrano in (2015, 2016) [56-58], which introduces the following contributions: (1) a new and efficient representation model for taxonomies, called PosetHERep, which is an adaptation of the half-edge data structure commonly used to represent discrete manifolds and planar graphs; (2) a new Java software library called the Half-Edge Semantic Measures Library (HESML) based on PosetHERep, which implements most ontology-based semantic similarity measures and Information Content (IC) models reported in the literature; (3) a set of reproducible experiments on word similarity based on HESML and ReproZip with the aim of exactly reproducing the experimental surveys in the three aforementioned works; (4) a replication framework and dataset, called WNSimRep v1, whose aim is to assist the exact replication of most methods reported in the literature; and finally, (5) a set of scalability and performance benchmarks for semantic measures libraries. PosetHERep and HESML are motivated by several drawbacks in the current semantic measures libraries, especially the performance and scalability, as well as the evaluation of new methods and the replication of most previous methods. The reproducible experiments introduced herein are encouraged by the lack of a set of large, self-contained and easily reproducible experiments with the aim of replicating and confirming previously reported results. Likewise, the WNSimRep v1 dataset is motivated by the discovery of several contradictory results and difficulties in reproducing previously reported methods and experiments. PosetHERep proposes a memory-efficient representation for taxonomies which linearly scales with the size of the taxonomy and provides an efficient implementation of most taxonomy-based algorithms used by the semantic measures and IC models, whilst HESML provides an open framework to aid research into the area by providing a simpler and more efficient software architecture than the current software libraries. Finally, we prove the outperformance of HESML on the state-of-the-art libraries, as well as the possibility of significantly improving their performance and scalability without caching using PosetHERep.}
}
@article{lastra2019reproducible,
	title        = {A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art},
	author       = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre},
	year         = {2019},
	journal      = {Engineering Applications of Artificial Intelligence},
	volume       = {85},
	pages        = {645--665},
	doi          = {https://doi.org/10.1016/j.engappai.2019.07.010},
	issn         = {0952-1976},
	url          = {https://www.sciencedirect.com/science/article/pii/S0952197619301745},
	keywords     = {Ontology-based semantic similarity measures, Word embedding models, Information Content models, WordNet, Experimental survey, HESML},
	abstract     = {Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.}
}
@inproceedings{le2014distributed,
	title        = {Distributed Representations of Sentences and Documents},
	author       = {Le, Quoc and Mikolov, Tomas},
	year         = {2014},
	month        = {22--24 Jun},
	booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Bejing, China},
	series       = {Proceedings of Machine Learning Research},
	volume       = {32},
	number       = {2},
	pages        = {1188--1196},
	url          = {https://proceedings.mlr.press/v32/le14.html},
	editor       = {Xing, Eric P. and Jebara, Tony},
	pdf          = {http://proceedings.mlr.press/v32/le14.pdf},
	abstract     = {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, "powerful," "strong"  and "Paris" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.}
}
@article{le2015tiny,
	title        = {Tiny imagenet visual recognition challenge},
	author       = {Le, Yann and Yang, Xuan},
	year         = {2015},
	journal      = {CS 231N},
	volume       = {7},
	number       = {7},
	pages        = {3}
}
@misc{lecunmnist,
	author       = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
	year         = {1998},
	journal      = {MNIST handwritten digit database},
	url          = {http://yann.lecun.com/exdb/mnist/}
}
@inproceedings{lecuyer2019certified,
	title        = {Certified robustness to adversarial examples with differential privacy},
	author       = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	year         = {2019},
	booktitle    = {2019 IEEE Symposium on Security and Privacy (SP)},
	pages        = {656--672},
	organization = {IEEE}
}
@article{lee1988thirteen,
	title        = {Thirteen ways to look at the correlation coefficient},
	author       = {Lee Rodgers, Joseph and Nicewander, W Alan},
	year         = {1988},
	journal      = {The American Statistician},
	publisher    = {Taylor  \& Francis},
	volume       = {42},
	number       = {1},
	pages        = {59--66}
}
@article{lee2011novel,
	title        = {A novel sentence similarity measure for semantic-based expert systems},
	author       = {Ming Che Lee},
	year         = {2011},
	journal      = {Expert Systems with Applications},
	volume       = {38},
	number       = {5},
	pages        = {6392--6399},
	doi          = {https://doi.org/10.1016/j.eswa.2010.10.043},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417410011875},
	keywords     = {Sentence, Similarity, Semantic web, Ontology},
	abstract     = {A novel sentence similarity measure for semantic based expert systems is presented. The well-known problem in the fields of semantic processing, such as QA systems, is to evaluate the semantic similarity between irregular sentences. This paper takes advantage of corpus-based ontology to overcome this problem. A transformed vector space model is introduced in this article. The proposed two-phase algorithm evaluates the semantic similarity for two or more sentences via a semantic vector space. The first phase built part-of-speech (POS) based subspaces by the raw data, and the latter carried out a cosine evaluation and adopted the WordNet ontology to construct the semantic vectors. Unlike other related researches that focused only on short sentences, our algorithm is applicable to short (4-5 words), medium (8-12 words), and even long sentences (over 12 words). The experiment demonstrates that the proposed algorithm has outstanding performance in handling long sentences with complex syntax. The significance of this research lies in the semantic similarity extraction of sentences, with arbitrary structures.}
}
@article{lee2020lipschitz,
	title        = {Lipschitz-certifiable training with a tight outer bound},
	author       = {Lee, Sungyoon and Lee, Jaewook and Park, Saerom},
	year         = {2020},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {33},
	pages        = {16891--16902}
}
@inproceedings{lee2020maskgan,
	title        = {MaskGAN: Towards Diverse and Interactive Facial Image Manipulation},
	author       = {Lee, Cheng-Han and Liu, Ziwei and Wu, Lingyun and Luo, Ping},
	year         = {2020},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{lehmann2012dynamical,
	title        = {Dynamical Classes of Collective Attention in Twitter},
	author       = {Lehmann, Janette and Gon\c{c}alves, Bruno and Ramasco, Jos\'{e} J. and Cattuto, Ciro},
	year         = {2012},
	booktitle    = {Proceedings of the 21st International Conference on World Wide Web},
	location     = {Lyon, France},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '12},
	pages        = {251--260},
	doi          = {10.1145/2187836.2187871},
	isbn         = {9781450312295},
	url          = {https://doi.org/10.1145/2187836.2187871},
	abstract     = {Micro-blogging systems such as Twitter expose digital traces of social discourse with an unprecedented degree of resolution of individual behaviors. They offer an opportunity to investigate how a large-scale social system responds to exogenous or endogenous stimuli, and to disentangle the temporal, spatial and topical aspects of users' activity. Here we focus on spikes of collective attention in Twitter, and specifically on peaks in the popularity of hashtags. Users employ hashtags as a form of social annotation, to define a shared context for a specific event, topic, or meme. We analyze a large-scale record of Twitter activity and find that the evolution of hashtag popularity over time defines discrete classes of hashtags. We link these dynamical classes to the events the hashtags represent and use text mining techniques to provide a semantic characterization of the hashtag classes. Moreover, we track the propagation of hashtags in the Twitter social network and find that epidemic spreading plays a minor role in hashtag popularity, which is mostly driven by exogenous factors.},
	numpages     = {10},
	keywords     = {content analysis, online social networks, micro-blogging}
}
@inproceedings{lesk1986automatic,
	title        = {Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone},
	author       = {Michael Lesk},
	year         = {1986},
	booktitle    = {Proceedings of the 5th Annual International Conference on Systems Documentation, {SIGDOC} 1986, Toronto, Ontario, Canada, 1986},
	publisher    = {{ACM}},
	pages        = {24--26},
	doi          = {10.1145/318723.318728},
	url          = {https://doi.org/10.1145/318723.318728},
	editor       = {Virginia DeBuys},
	timestamp    = {Tue, 06 Nov 2018 11:07:46 +0100},
	biburl       = {https://dblp.org/rec/conf/sigdoc/Lesk86.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lessmann2008benchmarking,
	title        = {Benchmarking classification models for software defect prediction: A proposed framework and novel findings},
	author       = {Lessmann, Stefan and Baesens, Bart and Mues, Christophe and Pietsch, Swantje},
	year         = {2008},
	journal      = {IEEE transactions on software engineering},
	publisher    = {IEEE},
	volume       = {34},
	number       = {4},
	pages        = {485--496}
}
@inproceedings{levenshtein1966binary,
	title        = {Binary codes capable of correcting deletions, insertions, and reversals},
	author       = {Levenshtein, Vladimir I and others},
	year         = {1966},
	booktitle    = {Soviet physics doklady},
	volume       = {10},
	number       = {8},
	pages        = {707--710},
	organization = {Soviet Union}
}
@article{leveson1993investigation,
	title        = {An investigation of the Therac-25 accidents},
	author       = {Leveson, N.G. and Turner, C.S.},
	year         = {1993},
	journal      = {Computer},
	publisher    = {IEEE},
	volume       = {26},
	number       = {7},
	pages        = {18--41},
	doi          = {10.1109/MC.1993.274940}
}
@inproceedings{levy2014neural,
	title        = {Neural Word Embedding as Implicit Matrix Factorization},
	author       = {Levy, Omer and Goldberg, Yoav},
	year         = {2014},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {27},
	url          = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
	editor       = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger}
}
@inproceedings{ley2002dblp,
	title        = {The DBLP computer science bibliography: Evolution, research issues, perspectives},
	author       = {Ley, Michael},
	year         = {2002},
	booktitle    = {International symposium on string processing and information retrieval},
	pages        = {1--10},
	organization = {Springer}
}
@inproceedings{li2017neural,
	title        = {Neural attentive session-based recommendation},
	author       = {Li, Jing and Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Lian, Tao and Ma, Jun},
	year         = {2017},
	booktitle    = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	pages        = {1419--1428}
}
@inproceedings{li2019analyzing,
	title        = {Analyzing deep neural networks with symbolic propagation: Towards higher precision and faster verification},
	author       = {Li, Jianlin and Liu, Jiangchao and Yang, Pengfei and Chen, Liqian and Huang, Xiaowei and Zhang, Lijun},
	year         = {2019},
	booktitle    = {Static Analysis: 26th International Symposium, SAS 2019, Porto, Portugal, October 8--11, 2019, Proceedings 26},
	pages        = {296--319},
	organization = {Springer}
}
@inproceedings{li2019preventing,
	title        = {Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks},
	author       = {Li, Qiyang and Haque, Saminul and Anil, Cem and Lucas, James and Grosse, Roger B and Jacobsen, Joern-Henrik},
	year         = {2019},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {32},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1ce3e6e3f452828e23a0c94572bef9d9-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{li2019universal,
	title        = {Universal Perturbation Attack Against Image Retrieval},
	author       = {Li, Jie and Ji, Rongrong and Liu, Hong and Hong, Xiaopeng and Gao, Yue and Tian, Qi},
	year         = {2019},
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages        = {4899--4908}
}
@article{li2020light,
	title        = {Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems},
	author       = {Haoliang Li and Yufei Wang and Xiaofei Xie and Yang Liu and Shiqi Wang and Renjie Wan and Lap{-}Pui Chau and Alex C. Kot},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2009.06996},
	url          = {https://arxiv.org/abs/2009.06996},
	eprinttype   = {arXiv},
	eprint       = {2009.06996},
	timestamp    = {Thu, 14 Oct 2021 09:14:00 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2009-06996.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2020tilted,
	title        = {Tilted Empirical Risk Minimization},
	author       = {Tian Li and Ahmad Beirami and Maziar Sanjabi and Virginia Smith},
	year         = {2021},
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=K5YasWXZT3O},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/0005BSS21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2022ropgen,
	title        = {RoPGen: towards robust code authorship attribution via automatic coding style transformation},
	author       = {Li, Zhen and Chen, Guenevere (Qian) and Chen, Chen and Zou, Yayi and Xu, Shouhuai},
	year         = {2022},
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {1906–1918},
	doi          = {10.1145/3510003.3510181},
	isbn         = {9781450392211},
	url          = {https://doi.org/10.1145/3510003.3510181},
	abstract     = {Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style manipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called <u>Ro</u>bust coding style <u>P</u>atterns <u>Gen</u>eration (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manipulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8\% and 41.0\% of the success rate of targeted and untargeted attacks on average.},
	numpages     = {13},
	keywords     = {authorship attribution, coding style, deep learning, robustness, source code}
}
@inproceedings{li2022towards,
	title        = {Towards practical robustness analysis for DNNs based on PAC-model learning},
	author       = {Li, Renjue and Yang, Pengfei and Huang, Cheng-Chao and Sun, Youcheng and Xue, Bai and Zhang, Lijun},
	year         = {2022},
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {2189–2201},
	doi          = {10.1145/3510003.3510143},
	isbn         = {9781450392211},
	url          = {https://doi.org/10.1145/3510003.3510143},
	abstract     = {To analyse local robustness properties of deep neural networks (DNNs), we present a practical framework from a model learning perspective. Based on black-box model learning with scenario optimisation, we abstract the local behaviour of a DNN via an affine model with the probably approximately correct (PAC) guarantee. From the learned model, we can infer the corresponding PAC-model robustness property. The innovation of our work is the integration of model learning into PAC robustness analysis: that is, we construct a PAC guarantee on the model level instead of sample distribution, which induces a more faithful and accurate robustness evaluation. This is in contrast to existing statistical methods without model learning. We implement our method in a prototypical tool named DeepPAC. As a black-box method, DeepPAC is scalable and efficient, especially when DNNs have complex structures or high-dimensional inputs. We extensively evaluate DeepPAC, with 4 baselines (using formal verification, statistical methods, testing and adversarial attack) and 20 DNN models across 3 datasets, including MNIST, CIFAR-10, and ImageNet. It is shown that DeepPAC outperforms the state-of-the-art statistical method PROVERO, and it achieves more practical robustness analysis than the formal verification tool ERAN. Also, its results are consistent with existing DNN testing work like DeepGini.},
	numpages     = {13},
	keywords     = {scenario optimization, neural networks, model learning, PAC-model robustness}
}
@article{li2023accurate,
	title        = {Accurate Fairness: Improving Individual Fairness without Trading Accuracy},
	author       = {Li, Xuran and Wu, Peng and Su, Jing},
	year         = {2023},
	month        = {Jun.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {37},
	number       = {12},
	pages        = {14312--14320},
	doi          = {10.1609/aaai.v37i12.26674},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/26674},
	abstractnote = {Accuracy and individual fairness are both crucial for trustworthy machine learning, but these two aspects are often incompatible with each other so that enhancing one aspect may sacrifice the other inevitably with side effects of true bias or false fairness. We propose in this paper a new fairness criterion, accurate fairness, to align individual fairness with accuracy. Informally, it requires the treatments of an individual and the individual’s similar counterparts to conform to a uniform target, i.e., the ground truth of the individual. We prove that accurate fairness also implies typical group fairness criteria over a union of similar sub-populations. We then present a Siamese fairness in-processing approach to minimize the accuracy and fairness losses of a machine learning model under the accurate fairness constraints. To the best of our knowledge, this is the first time that a Siamese approach is adapted for bias mitigation. We also propose fairness confusion matrix-based metrics, fair-precision, fair-recall, and fair-F1 score, to quantify a trade-off between accuracy and individual fairness. Comparative case studies with popular fairness datasets show that our Siamese fairness approach can achieve on average 1.02%-8.78% higher individual fairness (in terms of fairness through awareness) and 8.38%-13.69% higher accuracy, as well as 10.09%-20.57% higher true fair rate, and 5.43%-10.01% higher fair-F1 score, than the state-of-the-art bias mitigation techniques. This demonstrates that our Siamese fairness approach can indeed improve individual fairness without trading accuracy. Finally, the accurate fairness criterion and Siamese fairness approach are applied to mitigate the possible service discrimination with a real Ctrip dataset, by on average fairly serving 112.33% more customers (specifically, 81.29% more customers in an accurately fair way) than baseline models.}
}
@inproceedings{li2023certifying,
	title        = {Certifying the Fairness of KNN in the Presence of Dataset Bias},
	author       = {Li, Yannan and Wang, Jingbo and Wang, Chao},
	year         = {2023},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {335--357},
	isbn         = {978-3-031-37703-7},
	editor       = {Enea, Constantin and Lal, Akash},
	abstract     = {We propose a method for certifying the fairness of the classification result of a widely used supervised learning algorithm, the k-nearest neighbors (KNN), under the assumption that the training data may have historical bias caused by systematic mislabeling of samples from a protected minority group. To the best of our knowledge, this is the first certification method for KNN based on three variants of the fairness definition: individual fairness, {\$}{\$}{\backslash}epsilon {\$}{\$}-fairness, and label-flipping fairness. We first define the fairness certification problem for KNN and then propose sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm. This is meant to lift the computation results from the concrete domain to an abstract domain, to reduce the computational cost. We show effectiveness of this abstract interpretation based technique through experimental evaluation on six datasets widely used in the fairness research literature. We also show that the method is accurate enough to obtain fairness certifications for a large number of test inputs, despite the presence of historical bias in the datasets.},
	organization = {Springer}
}
@inproceedings{li2023sok,
	title        = {SoK: Certified Robustness for Deep Neural Networks},
	author       = {Li, Linyi and Xie, Tao and Li, Bo},
	year         = {2023},
	booktitle    = {2023 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {1289--1310},
	doi          = {10.1109/SP46215.2023.10179303},
	keywords     = {Privacy;Adaptation models;Taxonomy;Artificial neural networks;Benchmark testing;Robustness;Security;certified robustness;neural networks;verification}
}
@inproceedings{li2023systematic,
	title        = {Systematic Testing of the Data-Poisoning Robustness of KNN},
	author       = {Li, Yannan and Wang, Jingbo and Wang, Chao},
	year         = {2023},
	booktitle    = {ACM SIGSOFT International Symposium on Software Testing and Analysis}
}
@article{li2023taggpt,
	title        = {TagGPT: Large Language Models are Zero-shot Multimodal Taggers},
	author       = {Chen Li and Yixiao Ge and Jiayong Mao and Dian Li and Ying Shan},
	year         = {2023},
	journal      = {CoRR},
	volume       = {abs/2304.03022},
	doi          = {10.48550/ARXIV.2304.03022},
	url          = {https://doi.org/10.48550/arXiv.2304.03022},
	eprinttype   = {arXiv},
	eprint       = {2304.03022},
	timestamp    = {Tue, 18 Apr 2023 17:25:12 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2304-03022.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2025detecting,
	title        = {Detecting and corrupting convolution-based unlearnable examples},
	author       = {Li, Minghui and Wang, Xianlong and Yu, Zhifei and Hu, Shengshan and Zhou, Ziqi and Zhang, Longling and Zhang, Leo Yu},
	year         = {2025},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {39},
	number       = {17},
	pages        = {18403--18411}
}
@article{liang2021pruning,
	title        = {Pruning and quantization for deep neural network acceleration: A survey},
	author       = {Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
	year         = {2021},
	journal      = {Neurocomputing},
	volume       = {461},
	pages        = {370--403},
	doi          = {https://doi.org/10.1016/j.neucom.2021.07.045},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
	keywords     = {Convolutional neural network, Neural network acceleration, Neural network quantization, Neural network pruning, Low-bit mathematics},
	abstract     = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.}
}
@article{liao2012mining,
	title        = {Data mining of social networks represented as graphs},
	author       = {David F. Nettleton},
	year         = {2013},
	journal      = {Computer Science Review},
	volume       = {7},
	pages        = {1--34},
	doi          = {https://doi.org/10.1016/j.cosrev.2012.12.001},
	issn         = {1574-0137},
	url          = {https://www.sciencedirect.com/science/article/pii/S1574013712000445},
	keywords     = {Graphs, Online social networks, Graph mining, Data mining, Statistical analysis, Data modelling},
	abstract     = {In this survey we review the literature and concepts of the data mining of social networks, with special emphasis on their representation as a graph structure. The survey is divided into two principal parts: first we conduct a survey of the literature which forms the ‘basis' and background for the field; second we define a set of ‘hot topics' which are currently in vogue in congresses and the literature. The ‘basis' or background part is divided into four major themes: graph theory, social networks, online social networks and graph mining. The graph mining theme is organized into ten subthemes. The second, ‘hot topic' part, is divided into five major themes: communities, influence and recommendation, models metrics and dynamics, behaviour and relationships, and information diffusion.}
}
@book{liddell1894greek,
	title        = {A greek-english lexicon},
	author       = {Liddell, Henry George and W., Glare P G and McKenzie, Roderick and Mackenzie, Roderick and Scott, Robert and Jones, Henry Stuart},
	year         = {1996},
	publisher    = {Clarendon Press},
	place        = {Oxford}
}
@inproceedings{lim2012topological,
	title        = {A Topological Approach for Detecting Twitter Communities with Common Interests},
	author       = {Lim, Kwan Hui and Datta, Amitava},
	year         = {2013},
	booktitle    = {Ubiquitous Social Media Analysis},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {23--43},
	isbn         = {978-3-642-45392-2},
	editor       = {Atzmueller, Martin and Chin, Alvin and Helic, Denis and Hotho, Andreas},
	abstract     = {The efficient identification of communities with common interests is a key consideration in applying targeted advertising and viral marketing to online social networking sites. Existing methods involve large scale community detection on the entire social network before determining the interests of individuals within these communities. This approach is both computationally intensive and may result in communities without a common interest. We propose an efficient topological-based approach for detecting communities that share common interests on Twitter. Our approach involves first identifying celebrities that are representative of an interest category before detecting communities based on linkages among followers of these celebrities. We also study the network characteristics and tweeting behaviour of these communities, and the effects of deepening or specialization of interest on their community structures. In particular, our evaluation on Twitter shows that these detected communities comprise members who are well-connected, cohesive and tweet about their common interest.}
}
@article{lim2016interaction,
	title        = {An interaction-based approach to detecting highly interactive Twitter communities using tweeting links},
	author       = {Lim, Kwan Hui and Datta, Amitava},
	year         = {2016},
	journal      = {Web Intelligence},
	publisher    = {IOS Press},
	volume       = {14},
	pages        = {1--15},
	doi          = {10.3233/WEB-160328},
	issn         = {2405-6464},
	url          = {https://doi.org/10.3233/WEB-160328},
	note         = {1},
	keywords     = {Twitter; tweets; social network analysis; community detection; like-minded communities; interaction links; common interests},
	abstract     = {The immense popularity and rapid growth of Online Social Networks (OSN) have attracted the interest of researchers and companies, particularly in how users group together to form communities online. While many community detection algorithms have been developed to detect communities on such OSNs, most of these algorithms are based only on topological links and researchers have observed that many topological links do not translate to actual user interaction. As such, many members of the detected communities do not communicate frequently to each other. This inactivity creates a problem in targeted advertising and viral marketing, which require the community to be highly active so as to facilitate the diffusion of product/service information. We propose an approach to detect highly interactive Twitter communities that share common interests, based on the frequency and patterns of direct tweeting among users, rather than the topological information implicit in follower/following links. Our experimental results show that communities detected by our proposed approach are more cohesive and connected within different interest groups, based on topological measures. We also show that the detected communities actively interact about the specific interests, based on the high frequency of {\#}hashtags and @mentions related to this interest. In addition, we study the trends in their tweeting patterns such as how they follow and unfollow other users, and observe that our approach detects communities comprising users whose links are more persistent compared to those in other groups of users.}
}
@inproceedings{lim2017clustop,
	title        = {ClusTop: A clustering-based topic modelling algorithm for twitter using word networks},
	author       = {Lim, Kwan Hui and Karunasekera, Shanika and Harwood, Aaron},
	year         = {2017},
	booktitle    = {2017 IEEE International Conference on Big Data (Big Data)},
	volume       = {},
	number       = {},
	pages        = {2009--2018},
	doi          = {10.1109/BigData.2017.8258147}
}
@inproceedings{lin1998information,
	title        = {An Information-Theoretic Definition of Similarity},
	author       = {Dekang Lin},
	year         = {1998},
	booktitle    = {Proceedings of the Fifteenth International Conference on Machine Learning {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
	publisher    = {Morgan Kaufmann},
	pages        = {296--304},
	editor       = {Jude W. Shavlik},
	timestamp    = {Thu, 30 Jun 2011 10:34:12 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/Lin98.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lin2014microsoft,
	title        = {Microsoft coco: Common objects in context},
	author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	year         = {2014},
	booktitle    = {European conference on computer vision},
	pages        = {740--755},
	organization = {Springer}
}
@inproceedings{lin2017structured,
	title        = {A Structured Self-Attentive Sentence Embedding},
	author       = {Zhouhan Lin and Minwei Feng and C{\'{\i}}cero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJC\_jUqxe},
	timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LinFSYXZB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lin2019nesterov,
	title        = {Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks},
	author       = {Jiadong Lin and Chuanbiao Song and Kun He and Liwei Wang and John E. Hopcroft},
	year         = {2020},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SJlHwkBYDH}
}
@inproceedings{lin2019robustness,
	title        = {Robustness Verification of Classification Deep Neural Networks via Linear Programming},
	author       = {Lin, Wang and Yang, Zhengfeng and Chen, Xin and Zhao, Qingye and Li, Xiangkun and Liu, Zhiming and He, Jifeng},
	year         = {2019},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {11418--11427}
}
@inproceedings{linstead2007mining,
	title        = {Mining Concepts from Code with Probabilistic Topic Models},
	author       = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
	year         = {2007},
	booktitle    = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
	location     = {Atlanta, Georgia, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASE '07},
	pages        = {461--464},
	doi          = {10.1145/1321631.1321709},
	isbn         = {9781595938824},
	url          = {https://doi.org/10.1145/1321631.1321709},
	abstract     = {We develop and apply statistical topic models to software as a means of extracting concepts from source code. The effectiveness of the technique is demonstrated on 1,555 projects from SourceForge and Apache consisting of 113,000 files and 19 million lines of code. In addition to providing an automated, unsupervised, solution to the problem of summarizing program functionality, the approach provides a probabilistic framework with which to analyze and visualize source file similarity. Finally, we introduce an information-theoretic approach for computing tangling and scattering of extracted concepts, and present preliminary results},
	numpages     = {4},
	keywords     = {program understanding, topic models, mining software}
}
@inproceedings{liu2014chi,
	title        = {CHI 1994-2013: Mapping Two Decades of Intellectual Progress through Co-Word Analysis},
	author       = {Liu, Yong and Goncalves, Jorge and Ferreira, Denzil and Xiao, Bei and Hosio, Simo and Kostakos, Vassilis},
	year         = {2014},
	booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	location     = {Toronto, Ontario, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '14},
	pages        = {3553--3562},
	doi          = {10.1145/2556288.2556969},
	isbn         = {9781450324731},
	url          = {https://doi.org/10.1145/2556288.2556969},
	abstract     = {This study employs hierarchical cluster analysis, strategic diagrams and network analysis to map and visualize the intellectual landscape of the CHI conference on Human Computer Interaction through the use of co-word analysis. The study quantifies and describes the thematic evolution of the field based on a total of 3152 CHI articles and their associated 16035 keywords published between 1994 and 2013. The analysis is conducted for two time periods (1994-2003, 2004-2013) and a comparison between them highlights the underlying trends in our community. More significantly, this study identifies the evolution of major themes in the discipline, and highlights individual topics as popular, core, or backbone research topics within HCI.},
	numpages     = {10},
	keywords     = {cohesion, bibliometric study, hci, conceptual evolution, coherence, co-word analysis}
}
@inproceedings{liu2015deep,
	title        = {Deep Learning Face Attributes in the Wild},
	author       = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	year         = {2015},
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{liu2015fast,
	title        = {Fast differentially private matrix factorization},
	author       = {Liu, Ziqi and Wang, Yu-Xiang and Smola, Alexander},
	year         = {2015},
	booktitle    = {Proceedings of the 9th ACM Conference on Recommender Systems},
	pages        = {171--178}
}
@inproceedings{liu2017improved,
	title        = {Improved Image Captioning via Policy Gradient Optimization of SPIDEr},
	author       = {Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin},
	year         = {2017},
	month        = {Oct},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{liu2017sphereface,
	title        = {SphereFace: Deep Hypersphere Embedding for Face Recognition},
	author       = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
	year         = {2017},
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {212--220}
}
@inproceedings{liu2018adv,
	title        = {Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network},
	author       = {Xuanqing Liu and Yao Li and Chongruo Wu and Cho{-}Jui Hsieh},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rk4Qso0cKm},
	timestamp    = {Sun, 02 Oct 2022 16:05:32 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LiuLWH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2018generative,
	title        = {Generative Adversarial Network for Abstractive Text Summarization},
	author       = {Linqing Liu and Yao Lu and Min Yang and Qiang Qu and Jia Zhu and Hongyan Li},
	year         = {2018},
	booktitle    = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018},
	publisher    = {{AAAI} Press},
	pages        = {8109--8110},
	url          = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16238},
	editor       = {Sheila A. McIlraith and Kilian Q. Weinberger},
	timestamp    = {Tue, 08 Mar 2022 21:46:37 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/LiuLYQZL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2018rethinking,
	title        = {Rethinking the Value of Network Pruning},
	author       = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJlnB3C5Ym},
	timestamp    = {Wed, 21 Aug 2024 07:35:24 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LiuSZHD19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2018towards,
	title        = {Towards Robust Neural Networks via Random Self-ensemble},
	author       = {Liu, Xuanqing and Cheng, Minhao and Zhang, Huan and Hsieh, Cho-Jui},
	year         = {2018},
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@inproceedings{liu2019adaptiveface,
	title        = {AdaptiveFace: Adaptive Margin and Sampling for Face Recognition},
	author       = {Liu, Hao and Zhu, Xiangyu and Lei, Zhen and Li, Stan Z.},
	year         = {2019},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{liu2019roberta,
	title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1907.11692},
	url          = {http://arxiv.org/abs/1907.11692},
	archiveprefix = {arXiv},
	eprint       = {1907.11692},
	timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	eprinttype   = {arXiv}
}
@inproceedings{liu2021swin,
	title        = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
	author       = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year         = {2021},
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages        = {10012--10022}
}
@inproceedings{liu2022deepstate,
	title        = {DeepState: selecting test suites to enhance the robustness of recurrent neural networks},
	author       = {Liu, Zixi and Feng, Yang and Yin, Yining and Chen, Zhenyu},
	year         = {2022},
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {598–609},
	doi          = {10.1145/3510003.3510231},
	isbn         = {9781450392211},
	url          = {https://doi.org/10.1145/3510003.3510231},
	abstract     = {Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature.In this paper, we propose a test suite selection tool DeepState towards the particular neural network structures of RNN models for reducing the data labeling and computation cost. DeepState selects data based on a stateful perspective of RNN, which identifies the possibly misclassified test by capturing the state changes of neurons in RNN models. We further design a test selection method to enable testers to obtain a test suite with strong fault detection and model improvement capability from a large dataset. To evaluate DeepState, we conduct an extensive empirical study on popular datasets and prevalent RNN models containing image and text processing tasks. The experimental results demonstrate that DeepState outperforms existing coverage-based techniques in selecting tests regarding effectiveness and the inclusiveness of bug cases. Meanwhile, we observe that the selected data can improve the robustness of RNN models effectively.},
	numpages     = {12},
	keywords     = {deep learning testing, deep neural networks, recurrent neural networks, test selection}
}
@inproceedings{liu2023reliable,
	title        = {Reliable robustness evaluation via automatically constructed attack ensembles},
	author       = {Liu, Shengcai and Peng, Fu and Tang, Ke},
	year         = {2023},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {37},
	number       = {7},
	pages        = {8852--8860}
}
@inproceedings{liu2024game,
	title        = {Game-theoretic unlearnable example generator},
	author       = {Liu, Shuang and Wang, Yihan and Gao, Xiao-Shan},
	year         = {2024},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {38},
	number       = {19},
	pages        = {21349--21358}
}
@inproceedings{liu2024stable,
	title        = {Stable unlearnable example: Enhancing the robustness of unlearnable examples via stable error-minimizing noise},
	author       = {Liu, Yixin and Xu, Kaidi and Chen, Xun and Sun, Lichao},
	year         = {2024},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {38},
	number       = {4},
	pages        = {3783--3791}
}
@article{llm_adv,
	title        = {Adversarial attacks and defenses for large language models (LLMs): methods, frameworks {\&} challenges},
	author       = {Pranjal Kumar},
	year         = {2024},
	journal      = {Int. J. Multim. Inf. Retr.},
	volume       = {13},
	number       = {3},
	pages        = {26},
	doi          = {10.1007/S13735-024-00334-8},
	url          = {https://doi.org/10.1007/s13735-024-00334-8},
	timestamp    = {Sun, 04 Aug 2024 19:51:19 +0200},
	biburl       = {https://dblp.org/rec/journals/ijmir/Kumar24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{locatello2019fairness,
	title        = {On the Fairness of Disentangled Representations},
	author       = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Thomas and Bauer, Stefan and Sch\"{o}lkopf, Bernhard and Bachem, Olivier},
	year         = {2019},
	journal      = {Advances in neural information processing systems},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {32},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{logeswaran2018efficient,
	title        = {An efficient framework for learning sentence representations},
	author       = {Lajanugen Logeswaran and Honglak Lee},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJvJXZb0W},
	timestamp    = {Thu, 25 Jul 2019 14:25:49 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LogeswaranL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{long_2013,
	title        = {Hemingway Editor},
	author       = {Adam and Long, Ben},
	year         = {2013},
	journal      = {Hemingway Editor},
	url          = {https://hemingwayapp.com/}
}
@article{lopez2019word,
	title        = {Word n-gram attention models for sentence similarity and inference},
	author       = {I. Lopez-Gazpio and M. Maritxalar and M. Lapata and E. Agirre},
	year         = {2019},
	journal      = {Expert Systems with Applications},
	volume       = {132},
	pages        = {1--11},
	doi          = {https://doi.org/10.1016/j.eswa.2019.04.054},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417419302842},
	keywords     = {Attention models, Deep learning, Natural language understanding, Natural Language Inference, Semantic textual similarity},
	abstract     = {Semantic Textual Similarity and Natural Language Inference are two popular natural language understanding tasks used to benchmark sentence representation models where two sentences are paired. In such tasks sentences are represented as bag of words, sequences, trees or convolutions, but the attention model is based on word pairs. In this article we introduce the use of word n-grams in the attention model. Our results on five datasets show an error reduction of up to 41\% with respect to the word-based attention model. The improvements are especially relevant with low data regimes and, in the case of natural language inference, on the recently released hard subset of Natural Language Inference datasets.}
}
@inproceedings{loshchilov2017decoupled,
	title        = {Decoupled Weight Decay Regularization},
	author       = {Ilya Loshchilov and Frank Hutter},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1711.05101},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
	timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{loshchilov2017sgdr,
	title        = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	author       = {Ilya Loshchilov and Frank Hutter},
	year         = {2017},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Skq89Scxx}
}
@article{lowe2017boolean,
	title        = {The Boolean is Dead, Long Live the Boolean! Natural Language versus Boolean Searching in Introductory Undergraduate Instruction},
	author       = {M. Lowe and Bronwen Maxson and Sean Stone and Willie Miller and Eric Snajdr and Kathleen Hanna},
	year         = {2018},
	journal      = {College \& Research Libraries},
	volume       = {79},
	number       = {4},
	pages        = {517},
	doi          = {10.5860/crl.79.4.517},
	issn         = {2150-6701},
	url          = {https://crl.acrl.org/index.php/crl/article/view/16729},
	keywords     = {},
	abstract     = {Boolean logic can be a difficult concept for first‐year, introductory students to grasp. This paper compares the results of Boolean and natural language searching across several databases with searches created from student research questions. Performance differences between databases varied. Overall, natural search language is at least as good as Boolean searching. With evidence that students struggle to grasp Boolean searching, and may not use it even after instruction, it could be left out of first‐year instruction, freeing up valuable class time to focus on concepts such as question development and source evaluation. As the Framework for Information Literary does not specifically address Boolean operators, the authors suggest it should have less prominence in first‐year Information Literacy instruction.}
}
@inproceedings{lu2023exploring,
	title        = {Exploring the limits of model-targeted indiscriminate data poisoning attacks},
	author       = {Lu, Yiwei and Kamath, Gautam and Yu, Yaoliang},
	year         = {2023},
	booktitle    = {International Conference on Machine Learning},
	pages        = {22856--22879},
	organization = {PMLR}
}
@article{luhn1957statistical,
	title        = {A Statistical Approach to Mechanized Encoding and Searching of Literary Information},
	author       = {Luhn, H. P.},
	year         = {1957},
	journal      = {IBM Journal of Research and Development},
	volume       = {1},
	number       = {4},
	pages        = {309--317},
	doi          = {10.1147/rd.14.0309}
}
@article{lund1996producing,
	title        = {Producing high-dimensional semantic spaces from lexical co-occurrence},
	author       = {Lund, Kevin and Burgess, Curt},
	year         = {1996},
	month        = {Jun},
	day          = {01},
	journal      = {Behavior Research Methods, Instruments, {\&} Computers},
	volume       = {28},
	number       = {2},
	pages        = {203--208},
	doi          = {10.3758/BF03204766},
	issn         = {1532-5970},
	url          = {https://doi.org/10.3758/BF03204766},
	abstract     = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).}
}
@article{lyon2004theoretical,
	title        = {A theoretical basis to the automated detection of copying between texts, and its practical implementation in the Ferret plagiarism and collusion detector},
	author       = {Lyon, Caroline and Barrett, Ruth and Malcolm, James},
	year         = {2004},
	journal      = {Plagiarism: Prevention, Practice and Policies}
}
@inproceedings{lyu2021towards,
	title        = {Towards Evaluating and Training Verifiably Robust Neural Networks},
	author       = {Lyu, Zhaoyang and Guo, Minghao and Wu, Tong and Xu, Guodong and Zhang, Kehuan and Lin, Dahua},
	year         = {2021},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {4308--4317}
}
@inproceedings{ma2012will,
	title        = {Will This \#hashtag Be Popular Tomorrow?},
	author       = {Ma, Zongyang and Sun, Aixin and Cong, Gao},
	year         = {2012},
	booktitle    = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Portland, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '12},
	pages        = {1173--1174},
	doi          = {10.1145/2348283.2348525},
	isbn         = {9781450314725},
	url          = {https://doi.org/10.1145/2348283.2348525},
	abstract     = {Hashtags are widely used in Twitter to define a shared context for events or topics. In this paper, we aim to predict hashtag popularity in near future (i.e., next day). Given a hashtag that has the potential to be popular in the next day, we construct a hashtag profile using the tweets containing the hashtag, and extract both content and context features for hashtag popularity prediction. We model this prediction problem as a classification problem and evaluate the effectiveness of the extracted features and classification models.},
	numpages     = {2},
	keywords     = {hashtag, hashtag clarity, popularity prediction, twitter}
}
@inproceedings{ma2018characterizing,
	title        = {Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},
	author       = {Xingjun Ma and Bo Li and Yisen Wang and Sarah M. Erfani and Sudanthi N. R. Wijewickrema and Grant Schoenebeck and Dawn Song and Michael E. Houle and James Bailey},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=B1gJ1L2aW},
	timestamp    = {Sun, 27 Oct 2019 17:57:12 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/Ma0WEWSSHB18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{madry2017towards,
	title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	author       = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJzIBfZAb},
	timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{maheshwary2021generating,
	title        = {Generating Natural Language Attacks in a Hard Label Black Box Setting},
	author       = {Rishabh Maheshwary and Saket Maheshwary and Vikram Pudi},
	year         = {2021},
	booktitle    = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021},
	publisher    = {{AAAI} Press},
	pages        = {13525--13533},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17595},
	timestamp    = {Mon, 07 Jun 2021 11:46:04 +0200},
	biburl       = {https://dblp.org/rec/conf/aaai/MaheshwaryMP21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{maier2018applying,
	title        = {Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology},
	author       = {Daniel Maier and A. Waldherr and P. Miltner and G. Wiedemann and A. Niekler and A. Keinert and B. Pfetsch and G. Heyer and U. Reber and T. Häussler and H. Schmid-Petri and S. Adam},
	year         = {2018},
	journal      = {Communication Methods and Measures},
	publisher    = {Routledge},
	volume       = {12},
	number       = {2-3},
	pages        = {93--118},
	doi          = {10.1080/19312458.2018.1430754},
	url          = {https://doi.org/10.1080/19312458.2018.1430754},
	eprint       = {https://doi.org/10.1080/19312458.2018.1430754}
}
@inproceedings{malek2021antipodes,
	title        = {Antipodes of Label Differential Privacy: PATE and ALIBI},
	author       = {Malek Esmaeili, Mani and Mironov, Ilya and Prasad, Karthik and Shilov, Igor and Tramer, Florian},
	year         = {2021},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {34},
	pages        = {6934--6945},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2021/file/37ecd27608480aa3569a511a638ca74f-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@inproceedings{mangold2023differential,
	title        = {Differential Privacy has Bounded Impact on Fairness in Classification},
	author       = {Mangold, Paul and Perrot, Micha\"{e}l and Bellet, Aur\'{e}lien and Tommasi, Marc},
	year         = {2023},
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {202},
	pages        = {23681--23705},
	url          = {https://proceedings.mlr.press/v202/mangold23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/mangold23a/mangold23a.pdf},
	abstract     = {We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use this Lipschitz property to prove a non-asymptotic bound showing that, as the number of samples increases, the fairness level of private models gets closer to the one of their non-private counterparts. This bound also highlights the importance of the confidence margin of a model on the disparate impact of differential privacy.}
}
@article{manning2006local,
	title        = {Local textual inference: it's hard to circumscribe, but you know it when you see it-and nlp needs it.},
	author       = {Manning, Christopher D},
	year         = {2006},
	publisher    = {Citeseer}
}
@unpublished{marteau2018sequence,
	title        = {{Sequence Covering Similarity for Symbolic Sequence Comparison}},
	author       = {Marteau, Pierre-Fran{\c c}ois},
	year         = {2018},
	month        = {Mar},
	url          = {https://hal.archives-ouvertes.fr/hal-01689286},
	note         = {working paper or preprint},
	keywords     = {Symbolic Sequence Match- ing ; Sequence Covering ; Sequence Covering Similarity ; String Matching ; Index terms- Sequence Covering Similarity ; Sequence Mining ; Similarity ; Symbolic Sequence Matching},
	pdf          = {https://hal.archives-ouvertes.fr/hal-01689286v3/file/CoveringSimilarity-v2.pdf},
	hal_id       = {hal-01689286},
	hal_version  = {v3}
}
@article{martinez2013semantic,
	title        = {Semantic similarity measurement using historical google search patterns},
	author       = {Martinez-Gil, Jorge and Aldana-Montes, Jos{\'e} F.},
	year         = {2013},
	month        = {Jul},
	day          = {01},
	journal      = {Information Systems Frontiers},
	volume       = {15},
	number       = {3},
	pages        = {399--410},
	doi          = {10.1007/s10796-012-9404-7},
	issn         = {1572-9419},
	url          = {https://doi.org/10.1007/s10796-012-9404-7},
	abstract     = {Computing the semantic similarity between terms (or short text expressions) that have the same meaning but which are not lexicographically similar is an important challenge in the information integration field. The problem is that techniques for textual semantic similarity measurement often fail to deal with words not covered by synonym dictionaries. In this paper, we try to solve this problem by determining the semantic similarity for terms using the knowledge inherent in the search history logs from the Google search engine. To do this, we have designed and evaluated four algorithmic methods for measuring the semantic similarity between terms using their associated history search patterns. These algorithmic methods are: a) frequent co-occurrence of terms in search patterns, b) computation of the relationship between search patterns, c) outlier coincidence on search patterns, and d) forecasting comparisons. We have shown experimentally that some of these methods correlate well with respect to human judgment when evaluating general purpose benchmark datasets, and significantly outperform existing methods when evaluating datasets containing terms that do not usually appear in dictionaries.}
}
@article{martinez2014overview,
	title        = {An overview of textual semantic similarity measures based on web intelligence},
	author       = {Martinez-Gil, Jorge},
	year         = {2014},
	month        = {Dec},
	day          = {01},
	journal      = {Artificial Intelligence Review},
	volume       = {42},
	number       = {4},
	pages        = {935--943},
	doi          = {10.1007/s10462-012-9349-8},
	issn         = {1573-7462},
	url          = {https://doi.org/10.1007/s10462-012-9349-8},
	abstract     = {Computing the semantic similarity between terms (or short text expressions) that have the same meaning but which are not lexicographically similar is a key challenge in many computer related fields. The problem is that traditional approaches to semantic similarity measurement are not suitable for all situations, for example, many of them often fail to deal with terms not covered by synonym dictionaries or are not able to cope with acronyms, abbreviations, buzzwords, brand names, proper nouns, and so on. In this paper, we present and evaluate a collection of emerging techniques developed to avoid this problem. These techniques use some kinds of web intelligence to determine the degree of similarity between text expressions. These techniques implement a variety of paradigms including the study of co-occurrence, text snippet comparison, frequent pattern finding, or search log analysis. The goal is to substitute the traditional techniques where necessary.}
}
@inproceedings{mastropaolo2023robustness,
	title        = {On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot},
	author       = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
	year         = {2023},
	booktitle    = {Proceedings of the 45th International Conference on Software Engineering},
	location     = {Melbourne, Victoria, Australia},
	publisher    = {IEEE Press},
	series       = {ICSE '23},
	pages        = {2149–2160},
	doi          = {10.1109/ICSE48619.2023.00181},
	isbn         = {9781665457019},
	url          = {https://doi.org/10.1109/ICSE48619.2023.00181},
	abstract     = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in ~46\% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (±28\%).},
	numpages     = {12},
	keywords     = {empirical study, recommender systems}
}
@article{matloff1984asymptotic,
	title        = {The asymptotic distribution of an estimator of the Bayes error rate},
	author       = {Norman Matloff and Ronald Pruitt},
	year         = {1984},
	journal      = {Pattern Recognition Letters},
	volume       = {2},
	number       = {5},
	pages        = {271--274},
	doi          = {https://doi.org/10.1016/0167-8655(84)90013-8},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/0167865584900138},
	keywords     = {Asymptotic distribution, Bayes error rate, risk averaging, logistic model},
	abstract     = {The asymptotic distribution of a class of estimators of the Bayes error rate in pattern recognition problems is derived. The class consists of estimators of the risk-averaging type, applied to parametric models such as the logistic function.}
}
@inproceedings{mccann2017learned,
	title        = {Learned in Translation: Contextualized Word Vectors},
	author       = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	year         = {2017},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {30},
	url          = {https://proceedings.neurips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{mccann2018natural,
	title        = {The Natural Language Decathlon: Multitask Learning as Question Answering},
	author       = {Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
	year         = {2018},
	journal      = {CoRR},
	volume       = {abs/1806.08730},
	url          = {http://arxiv.org/abs/1806.08730},
	eprinttype   = {arXiv},
	eprint       = {1806.08730},
	timestamp    = {Mon, 13 Aug 2018 16:49:05 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1806-08730.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{mcgowan_2021,
	title        = {Topic model based recommendation systems},
	author       = {McGowan, Jamie},
	year         = {2021},
	month        = {Sep},
	journal      = {Medium},
	publisher    = {Towards Data Science},
	url          = {https://towardsdatascience.com/topic-model-based-recommendation-systems-a02d198408b7}
}
@book{mead1990design,
	title        = {The design of experiments: statistical principles for practical applications},
	author       = {Mead, Roger},
	year         = {1990},
	publisher    = {Cambridge university press}
}
@article{meho2007impact,
	title        = {Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar},
	author       = {Meho, Lokman I. and Yang, Kiduk},
	year         = {2007},
	journal      = {Journal of the American Society for Information Science and Technology},
	volume       = {58},
	number       = {13},
	pages        = {2105--2125},
	doi          = {https://doi.org/10.1002/asi.20677},
	url          = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.20677},
	eprint       = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20677},
	abstract     = {Abstract The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours.}
}
@article{mehrabi2021survey,
	title        = {A survey on bias and fairness in machine learning},
	author       = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year         = {2021},
	journal      = {ACM computing surveys (CSUR)},
	publisher    = {ACM New York, NY, USA},
	volume       = {54},
	number       = {6},
	pages        = {1--35}
}
@inproceedings{mehrotra2013improving,
	title        = {Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling},
	author       = {Mehrotra, Rishabh and Sanner, Scott and Buntine, Wray and Xie, Lexing},
	year         = {2013},
	booktitle    = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Dublin, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '13},
	pages        = {889--892},
	doi          = {10.1145/2484028.2484166},
	isbn         = {9781450320344},
	url          = {https://doi.org/10.1145/2484028.2484166},
	abstract     = {Twitter, or the world of 140 characters poses serious challenges to the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodified LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA topic models on Twitter content.},
	numpages     = {4},
	keywords     = {lda, microblogs, topic modeling}
}
@inproceedings{meihan2022fedcdr,
	title        = {Fedcdr: federated cross-domain recommendation for privacy-preserving rating prediction},
	author       = {Meihan, Wu and Li, Li and Tao, Chang and Rigall, Eric and Xiaodong, Wang and Cheng-Zhong, Xu},
	year         = {2022},
	booktitle    = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
	pages        = {2179--2188}
}
@article{menzies2006data,
	title        = {Data mining static code attributes to learn defect predictors},
	author       = {Menzies, Tim and Greenwald, Jeremy and Frank, Art},
	year         = {2006},
	journal      = {IEEE transactions on software engineering},
	publisher    = {IEEE},
	volume       = {33},
	number       = {1},
	pages        = {2--13}
}
@inproceedings{merity2016pointer,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Byj72udxe},
	timestamp    = {Thu, 25 Jul 2019 14:25:57 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{merriam_webster_concise,
	title        = {Concise definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/concise}
}
@inproceedings{metzen2017detecting,
	title        = {On Detecting Adversarial Perturbations},
	author       = {Jan Hendrik Metzen and Tim Genewein and Volker Fischer and Bastian Bischoff},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SJzCSf9xg},
	timestamp    = {Thu, 05 Mar 2020 12:59:38 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/MetzenGFB17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{metzger2022realizing,
	title        = {Realizing self-adaptive systems via online reinforcement learning and feature-model-guided exploration},
	author       = {Metzger, Andreas and Quinton, Cl{\'e}ment and Mann, Zolt{\'a}n {\'A}d{\'a}m and Baresi, Luciano and Pohl, Klaus},
	year         = {2022},
	month        = {Mar},
	day          = {01},
	journal      = {Computing},
	doi          = {10.1007/s00607-022-01052-x},
	issn         = {1436-5057},
	url          = {https://doi.org/10.1007/s00607-022-01052-x},
	abstract     = {A self-adaptive system can automatically maintain its quality requirements in the presence of dynamic environment changes. Developing a self-adaptive system may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. To realize self-adaptive systems in the presence of design time uncertainty, online machine learning, i.e., machine learning at runtime, is increasingly used. In particular, online reinforcement learning is proposed, which learns suitable adaptation actions through interactions with the environment at runtime. To learn about its environment, online reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens impacts the performance of the learning process. We focus on two problems related to how adaptation actions are explored. First, existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions. Second, they are unaware of system evolution, and thus may explore new adaptation actions introduced during evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and system evolution. Experimental results for two realistic self-adaptive systems indicate an average speed-up of the learning process of 33.7{\%} in the presence of many adaptation actions, and of 50.6{\%} in the presence of evolution.}
}
@article{mhasawade2021machine,
	title        = {Machine learning and algorithmic fairness in public and population health},
	author       = {Mhasawade, Vishwali and Zhao, Yuan and Chunara, Rumi},
	year         = {2021},
	month        = {Aug},
	day          = {01},
	journal      = {Nature Machine Intelligence},
	volume       = {3},
	number       = {8},
	pages        = {659--666},
	doi          = {10.1038/s42256-021-00373-4},
	issn         = {2522-5839},
	url          = {https://doi.org/10.1038/s42256-021-00373-4},
	abstract     = {Until now, much of the work on machine learning and health has focused on processes inside the hospital or clinic. However, this represents only a narrow set of tasks and challenges related to health; there is greater potential for impact by leveraging machine learning in health tasks more broadly. In this Perspective we aim to highlight potential opportunities and challenges for machine learning within a holistic view of health and its influences. To do so, we build on research in population and public health that focuses on the mechanisms between different cultural, social and environmental factors and their effect on the health of individuals and communities. We present a brief introduction to research in these fields, data sources and types of tasks, and use these to identify settings where machine learning is relevant and can contribute to new knowledge. Given the key foci of health equity and disparities within public and population health, we juxtapose these topics with the machine learning subfield of algorithmic fairness to highlight specific opportunities where machine learning, public and population health may synergize to achieve health equity.}
}
@misc{michaelis2019benchmarking,
	title        = {Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming},
	author       = {Claudio Michaelis and Benjamin Mitzkus and Robert Geirhos and Evgenia Rusak and Oliver Bringmann and Alexander S. Ecker and Matthias Bethge and Wieland Brendel},
	year         = {2020},
	url          = {https://openreview.net/forum?id=ryljMpNtwr}
}
@inproceedings{mihalcea2007wikify,
	title        = {Wikify! Linking Documents to Encyclopedic Knowledge},
	author       = {Mihalcea, Rada and Csomai, Andras},
	year         = {2007},
	booktitle    = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
	location     = {Lisbon, Portugal},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CIKM '07},
	pages        = {233--242},
	doi          = {10.1145/1321440.1321475},
	isbn         = {9781595938039},
	url          = {https://doi.org/10.1145/1321440.1321475},
	abstract     = {This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.},
	numpages     = {10},
	keywords     = {keyword extraction, wikipedia, word sense disambiguation, semantic annotation}
}
@inproceedings{mikolov2013distributed,
	title        = {Distributed Representations of Words and Phrases and their Compositionality},
	author       = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year         = {2013},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {26},
	url          = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	editor       = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger}
}
@inproceedings{mikolov2013efficient,
	title        = {Efficient Estimation of Word Representations in Vector Space},
	author       = {Tom{\'{a}}s Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year         = {2013},
	booktitle    = {1st International Conference on Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
	url          = {http://arxiv.org/abs/1301.3781},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Mon, 28 Dec 2020 11:31:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{miller1995wordnet,
	title        = {WordNet: A Lexical Database for English},
	author       = {Miller, George A.},
	year         = {1995},
	month        = {nov},
	journal      = {Commun. ACM},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {38},
	number       = {11},
	pages        = {39--41},
	doi          = {10.1145/219717.219748},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/219717.219748},
	issue_date   = {Nov. 1995},
	abstract     = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	numpages     = {3}
}
@book{miller1998wordnet,
	title        = {WordNet: An electronic lexical database},
	author       = {Miller, George A},
	year         = {1998},
	publisher    = {The MIT Press},
	editor       = {Fellbaum, Christiane},
	place        = {Cambridge, MA}
}
@inbook{mirjalili2019genetic,
	title        = {Genetic Algorithm},
	author       = {Mirjalili, Seyedali},
	year         = {2019},
	booktitle    = {Evolutionary Algorithms and Neural Networks: Theory and Applications},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {43--55},
	doi          = {10.1007/978-3-319-93025-1_4},
	isbn         = {978-3-319-93025-1},
	url          = {https://doi.org/10.1007/978-3-319-93025-1_4},
	abstract     = {Genetic Algorithm (GA) is one of the first population-based stochastic algorithm proposed in the history. Similar to other EAs, the main operators of GA are selection, crossover, and mutation. This chapter briefly presents this algorithm and applies it to several case studies to observe its performance.}
}
@inproceedings{mirman2018differentiable,
	title        = {Differentiable Abstract Interpretation for Provably Robust Neural Networks},
	author       = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
	year         = {2018},
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {80},
	pages        = {3578--3586},
	url          = {https://proceedings.mlr.press/v80/mirman18b.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf},
	abstract     = {We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.}
}
@article{mirman2021fundamental,
	title        = {The Fundamental Limits of Neural Networks for Interval Certified Robustness},
	author       = {Matthew B Mirman and Maximilian Baader and Martin Vechev},
	year         = {2022},
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=fsacLLU35V},
	note         = {}
}
@inproceedings{missier2016tracking,
	title        = {Tracking Dengue Epidemics Using Twitter Content Classification and Topic Modelling},
	author       = {Missier, Paolo and Romanovsky, Alexander and Miu, Tudor and Pal, Atinder and Daniilakis, Michael and Garcia, Alessandro and Cedrim, Diego and da Silva Sousa, Leonardo},
	year         = {2016},
	booktitle    = {Current Trends in Web Engineering},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {80--92},
	isbn         = {978-3-319-46963-8},
	editor       = {Casteleyn, Sven and Dolog, Peter and Pautasso, Cesare},
	abstract     = {Detecting and preventing outbreaks of mosquito-borne diseases such as Dengue and Zika in Brasil and other tropical regions has long been a priority for governments in affected areas. Streaming social media content, such as Twitter, is increasingly being used for health vigilance applications such as flu detection. However, previous work has not addressed the complexity of drastic seasonal changes on Twitter content across multiple epidemic outbreaks. In order to address this gap, this paper contrasts two complementary approaches to detecting Twitter content that is relevant for Dengue outbreak detection, namely supervised classification and unsupervised clustering using topic modelling. Each approach has benefits and shortcomings. Our classifier achieves a prediction accuracy of about 80 {\%} based on a small training set of about 1,000 instances, but the need for manual annotation makes it hard to track seasonal changes in the nature of the epidemics, such as the emergence of new types of virus in certain geographical locations. In contrast, LDA-based topic modelling scales well, generating cohesive and well-separated clusters from larger samples. While clusters can be easily re-generated following changes in epidemics, however, this approach makes it hard to clearly segregate relevant tweets into well-defined clusters.}
}
@inproceedings{mittal2002employing,
	title        = {Employing discrete Bayes error rate for discretization and feature selection tasks},
	author       = {Mittal, A. and Loong-Fah Cheong},
	year         = {2002},
	booktitle    = {2002 IEEE International Conference on Data Mining, 2002. Proceedings.},
	volume       = {},
	number       = {},
	pages        = {298--305},
	doi          = {10.1109/ICDM.2002.1183916}
}
@article{miyato2018virtual,
	title        = {Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning},
	author       = {Miyato, Takeru and Maeda, Shin-Ichi and Koyama, Masanori and Ishii, Shin},
	year         = {2019},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {41},
	number       = {8},
	pages        = {1979--1993},
	doi          = {10.1109/TPAMI.2018.2858821},
	url          = {https://doi.org/10.1109/TPAMI.2018.2858821},
	timestamp    = {Mon, 26 Oct 2020 09:04:26 +0100},
	biburl       = {https://dblp.org/rec/journals/pami/MiyatoMKI19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mnih2013learning,
	title        = {Learning word embeddings efficiently with noise-contrastive estimation},
	author       = {Mnih, Andriy and Kavukcuoglu, Koray},
	year         = {2013},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {26},
	url          = {https://proceedings.neurips.cc/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
	editor       = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger}
}
@inproceedings{mockus2000identifying,
	title        = {Identifying reasons for software changes using historic databases},
	author       = {Mockus and Votta},
	year         = {2000},
	booktitle    = {Proceedings 2000 international conference on software maintenance},
	pages        = {120--130},
	organization = {IEEE}
}
@book{mohri2018foundations,
	title        = {Foundations of Machine Learning},
	author       = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year         = {2012},
	publisher    = {The MIT Press},
	isbn         = {026201825X},
	abstract     = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.}
}
@inproceedings{monjezi2023information,
	title        = {Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks},
	author       = {Monjezi, Verya and Trivedi, Ashutosh and Tan, Gang and Tizpaz-Niari, Saeid},
	year         = {2023},
	booktitle    = {Proceedings of the 45th International Conference on Software Engineering},
	location     = {Melbourne, Victoria, Australia},
	publisher    = {IEEE Press},
	series       = {ICSE '23},
	pages        = {1571–1582},
	doi          = {10.1109/ICSE48619.2023.00136},
	isbn         = {9781665457019},
	url          = {https://doi.org/10.1109/ICSE48619.2023.00136},
	abstract     = {The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions---amplifying existing biases or introducing new ones---that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids---such as severity and causal explanations---crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs.The key goal of DICE is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quantitative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that DICE efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.},
	numpages     = {12}
}
@inproceedings{moon2015meta,
	title        = {Meta learning of bounds on the Bayes classifier error},
	author       = {Moon, Kevin R. and Hero, Alfred O. and Delouille, B. Véronique},
	year         = {2015},
	booktitle    = {2015 IEEE Signal Processing and Signal Processing Education Workshop (SP/SPE)},
	volume       = {},
	number       = {},
	pages        = {13--18},
	doi          = {10.1109/DSP-SPE.2015.7369520},
	keywords     = {Bit error rate;Estimation;Signal processing;Convergence;Conferences;Moon;Bayes error;divergence estimation;meta learning;classification;sunspots}
}
@inproceedings{moosavi2016deepfool,
	title        = {DeepFool: {A} Simple and Accurate Method to Fool Deep Neural Networks},
	author       = {Seyed{-}Mohsen Moosavi{-}Dezfooli and Alhussein Fawzi and Pascal Frossard},
	year         = {2016},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {2574--2582},
	doi          = {10.1109/CVPR.2016.282},
	url          = {https://doi.org/10.1109/CVPR.2016.282},
	timestamp    = {Sun, 02 Oct 2022 15:58:35 +0200},
	biburl       = {https://dblp.org/rec/conf/cvpr/Moosavi-Dezfooli16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{moro2014data,
	title        = {A data-driven approach to predict the success of bank telemarketing},
	author       = {Sérgio Moro and Paulo Cortez and Paulo Rita},
	year         = {2014},
	journal      = {Decision Support Systems},
	volume       = {62},
	pages        = {22--31},
	doi          = {https://doi.org/10.1016/j.dss.2014.03.001},
	issn         = {0167-9236},
	url          = {https://www.sciencedirect.com/science/article/pii/S016792361400061X},
	keywords     = {Bank deposits, Telemarketing, Savings, Classification, Neural networks, Variable selection},
	abstract     = {We propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the effects of the recent financial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes. A semi-automatic feature selection was explored in the modeling phase, performed with the data prior to July 2012 and that allowed to select a reduced set of 22 features. We also compared four DM models: logistic regression, decision trees (DTs), neural network (NN) and support vector machine. Using two metrics, area of the receiver operating characteristic curve (AUC) and area of the LIFT cumulative curve (ALIFT), the four models were tested on an evaluation set, using the most recent data (after July 2012) and a rolling window scheme. The NN presented the best results (AUC=0.8 and ALIFT=0.7), allowing to reach 79% of the subscribers by selecting the half better classified clients. Also, two knowledge extraction methods, a sensitivity analysis and a DT, were applied to the NN model and revealed several key attributes (e.g., Euribor rate, direction of the call and bank agent experience). Such knowledge extraction confirmed the obtained model as credible and valuable for telemarketing campaign managers.}
}
@incollection{morris1938foundations,
	title        = {Foundations of the Theory of Signs},
	author       = {Morris, Charles William},
	year         = {1938},
	booktitle    = {International encyclopedia of unified science},
	publisher    = {Chicago University Press},
	pages        = {1--59}
}
@inproceedings{moschitti2006efficient,
	title        = {Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees},
	author       = {Moschitti, Alessandro},
	year         = {2006},
	booktitle    = {Machine Learning: ECML 2006},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {318--329},
	isbn         = {978-3-540-46056-5},
	editor       = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
	abstract     = {In this paper, we provide a study on the use of tree kernels to encode syntactic parsing information in natural language learning. In particular, we propose a new convolution kernel, namely the Partial Tree (PT) kernel, to fully exploit dependency trees. We also propose an efficient algorithm for its computation which is futhermore sped-up by applying the selection of tree nodes with non-null kernel. The experiments with Support Vector Machines on the task of semantic role labeling and question classification show that (a) the kernel running time is linear on the average case and (b) the PT kernel improves on the other tree kernels when applied to the appropriate parsing paradigm.}
}
@inproceedings{moschitti2007fast,
	title        = {Fast and Effective Kernels for Relational Learning from Texts},
	author       = {Moschitti, Alessandro and Zanzotto, Fabio Massimo},
	year         = {2007},
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	location     = {Corvalis, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICML '07},
	pages        = {649--656},
	doi          = {10.1145/1273496.1273578},
	isbn         = {9781595937933},
	url          = {https://doi.org/10.1145/1273496.1273578},
	abstract     = {In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.},
	numpages     = {8}
}
@inproceedings{moschitti2008kernel,
	title        = {Kernel Methods, Syntax and Semantics for Relational Text Categorization},
	author       = {Moschitti, Alessandro},
	year         = {2008},
	booktitle    = {Proceedings of the 17th ACM Conference on Information and Knowledge Management},
	location     = {Napa Valley, California, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CIKM '08},
	pages        = {253--262},
	doi          = {10.1145/1458082.1458118},
	isbn         = {9781595939913},
	url          = {https://doi.org/10.1145/1458082.1458118},
	abstract     = {Previous work on Natural Language Processing for Information Retrieval has shown the inadequateness of semantic and syntactic structures for both document retrieval and categorization. The main reason is the high reliability and effectiveness of language models, which are sufficient to accurately solve such retrieval tasks. However, when the latter involve the computation of relational semantics between text fragments simple statistical models may result ineffective. In this paper, we show that syntactic and semantic structures can be used to greatly improve complex categorization tasks such as determining if an answer correctly responds to a question. Given the high complexity of representing semantic/syntactic structures in learning algorithms, we applied kernel methods along with Support Vector Machines to better exploit the needed relational information. Our experiments on answer classification on Web and TREC data show that our models greatly improve on bag-of-words.},
	numpages     = {10},
	keywords     = {text categorization, support vector machines, question answering, kernel methods, natural language processing}
}
@inproceedings{moschoglou2017agedb,
	title        = {AgeDB: The First Manually Collected, In-The-Wild Age Database},
	author       = {Moschoglou, Stylianos and Papaioannou, Athanasios and Sagonas, Christos and Deng, Jiankang and Kotsia, Irene and Zafeiriou, Stefanos},
	year         = {2017},
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	pages        = {51--59}
}
@book{mosteller2007inference,
	title        = {Inference and disputed authorship: The Federalist},
	author       = {Mosteller, Frederick and Wallace, David L.},
	year         = {2007},
	publisher    = {Center for the Study of Language and Information},
	place        = {Stanford, Calif}
}
@article{mu2022clustering,
	title        = {A clustering-based topic model using word networks and word embeddings},
	author       = {Mu, Wenchuan and Lim, Kwan Hui and Liu, Junhua and Karunasekera, Shanika and Falzon, Lucia and Harwood, Aaron},
	year         = {2022},
	month        = {Apr},
	day          = {11},
	journal      = {Journal of Big Data},
	volume       = {9},
	number       = {1},
	pages        = {38},
	doi          = {10.1186/s40537-022-00585-4},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-022-00585-4},
	abstract     = {Online social networking services like Twitter are frequently used for discussions on numerous topics of interest, which range from mainstream and popular topics (e.g., music and movies) to niche and specialized topics (e.g., politics). Due to the popularity of such services, it is a challenging task to automatically model and determine the numerous discussion topics given the large amount of tweets. Adding on this complexity is the need to identify these topics with the absence of prior knowledge about both the types and number of topics, while having the requirement of the relevant technical expertise to tune the numerous parameters for the various models. To address this challenge, we develop the Clustering-based Topic Modelling (ClusTop) algorithm that first constructs different types of word networks based on different types of n-grams co-occurrence and word embedding distances. Using these word networks, ClusTop is then able to automatically determine the discussion topics using community detection approaches. In contrast to traditional topic models, ClusTop does not require the tuning or setting of numerous parameters and instead uses community detection approaches to automatically determine the appropriate number of topics. The ClusTop algorithm is also able to capture the syntactic meaning in tweets via the use of bigrams, trigrams, other word combinations and word embedding techniques in constructing the word network graph, and utilizes edge weights based on word embedding. Using three Twitter datasets with labelled crises and events as topics, we show that ClusTop outperforms various traditional baselines in terms of topic coherence, pointwise mutual information, precision, recall and F-score.}
}
@inproceedings{mu2023modelling,
	title        = {Modelling Text Similarity: A Survey},
	author       = {Mu, Wenchuan and Lim, Kwan Hui},
	year         = {2024},
	booktitle    = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
	location     = {Kusadasi, Turkiye},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASONAM '23},
	pages        = {698–705},
	doi          = {10.1145/3625007.3627305},
	isbn         = {9798400704093},
	url          = {https://doi.org/10.1145/3625007.3627305},
	abstract     = {Online social networking services such as Twitter and Instagram have become pervasive platforms for engaging in discussions on a wide array of topics. These platforms cater to both mainstream subjects, like music and movies, as well as more specialized areas, such as politics. With the growing volume of textual data generated on these platforms, the ability to define and identify similar texts becomes crucial for effective investigation and clustering. In this paper, we explore the challenges and significance of text similarity regression models in the context of online social networking services. We delve into the methods and techniques employed to define and find similarities among texts, enabling the extraction of meaningful patterns and insights. Specifically, we categorize text similarity regression models into four distinct types: set-theoretic, sequence-theoretic, real-vector, and end-to-end methods. This categorization is based on the mathematical formalisation of similarity used by each model. Ultimately, our survey aims to provide a comprehensive overview of the interlinkages between independently proposed methods for text similarity. By understanding the strengths and weaknesses of these methods, researchers can make informed decisions when designing novel approaches and algorithms. We hope this survey serves as a valuable resource for advancing the state-of-the-art in addressing the complex problem of text similarity.},
	numpages     = {8},
	keywords     = {modelling and simulation, deep learning and embeddings, algorithms and techniques}
}
@inbook{mu2024fast,
	title        = {Fast Bibliography Pre-Selection via Two-Vector Semantic Representations},
	author       = {Mu, Wenchuan and Liu, Junhua and Lim, Kwan Hui},
	year         = {2025},
	booktitle    = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	isbn         = {9798400710933},
	url          = {https://doi.org/10.1145/3677389.3702495},
	abstract     = {In academic writing, bibliography compilations is essential but time-consuming, often requiring repeated searches for references. Hence, an efficient tool for faster bibliography compilation is needed. Our work offers a solution to the challenges of managing large-scale bibliographic databases, introducing a new algorithm that improves both efficiency and sensitivity. Using two-vector semantic modelling, bibliographic entries and queries are embedded into the same vector space to select relevant references based on semantic similarity. Experimental results with 3.37 million entries show the method reduces the time needed to generate a manageable subset, streamlining scholarly writing. Our code and dataset are publicly available at https://github.com/cestwc/bibliography-pre-selection.},
	articleno    = {23},
	numpages     = {6}
}
@inproceedings{mu2024label,
	title        = {Label-Free Topic-Focused Summarization Using Query Augmentation},
	author       = {Mu, Wenchuan and Lim, Kwan Hui},
	year         = {2024},
	booktitle    = {2024 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/IJCNN60899.2024.10650321},
	keywords     = {Training;Technological innovation;Adaptation models;Accuracy;Computational modeling;Neural networks;Decision making;Topic-focused summarization;Query augmentation;Transfer learning}
}
@inproceedings{mukherjee2020two,
	title        = {Two Simple Ways to Learn Individual Fairness Metrics from Data},
	author       = {Mukherjee, Debarghya and Yurochkin, Mikhail and Banerjee, Moulinath and Sun, Yuekai},
	year         = {2020},
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {119},
	pages        = {7097--7107},
	url          = {https://proceedings.mlr.press/v119/mukherjee20a.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/mukherjee20a/mukherjee20a.pdf},
	abstract     = {Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.}
}
@article{muller2022certified,
	title        = {Certified Training: Small Boxes are All You Need},
	author       = {Mark Niklas M{\"{u}}ller and Franziska Eckert and Marc Fischer and Martin T. Vechev},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2210.04871},
	doi          = {10.48550/arXiv.2210.04871},
	url          = {https://doi.org/10.48550/arXiv.2210.04871},
	eprinttype   = {arXiv},
	eprint       = {2210.04871},
	timestamp    = {Thu, 13 Oct 2022 14:33:15 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2210-04871.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{muller2022prima,
	title        = {PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations},
	author       = {M\"{u}ller, Mark Niklas and Makarchuk, Gleb and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
	year         = {2022},
	month        = {jan},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {6},
	number       = {POPL},
	doi          = {10.1145/3498704},
	url          = {https://doi.org/10.1145/3498704},
	issue_date   = {January 2022},
	abstract     = {Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20\%, 30\%, and 34\% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.},
	articleno    = {43},
	numpages     = {33},
	keywords     = {Polyhedra, Robustness, Abstract Interpretation, Convexity}
}
@misc{mutnick2020,
	title        = {District Court, N.D. Illinois, Illinois, filed January 22, 2020, 1:20-cv-00512 (United States)},
	author       = {Mutnick v. Clearview AI, Inc.},
	year         = {2020},
	month        = {jan},
	day          = {22},
	url          = {https://www.courthousenews.com/wp-content/uploads/2020/01/Surveillance-1.pdf},
	note         = {Court Case}
}
@misc{mw_sentence,
	title        = {Sentence definition},
	author       = {Merriam-Webster},
	journal      = {Dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/sentence}
}
@misc{mw_similar,
	title        = {Similar definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/similar}
}
@misc{mw_similarity,
	title        = {Similarity definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/similarity}
}
@misc{mw_text,
	title        = {Text definition},
	author       = {Merriam-Webster},
	journal      = {Merriam-Webster.com dictionary},
	publisher    = {Merriam-Webster},
	url          = {https://www.merriam-webster.com/dictionary/text}
}
@article{nadaraya1964estimating,
	title        = {On estimating regression},
	author       = {Nadaraya, Elizbar A},
	year         = {1964},
	journal      = {Theory of Probability \& Its Applications},
	publisher    = {SIAM},
	volume       = {9},
	number       = {1},
	pages        = {141--142}
}
@article{nagwani2015summarizing,
	title        = {Summarizing large text collection using topic modeling and clustering based on MapReduce framework},
	author       = {Nagwani, N. K.},
	year         = {2015},
	month        = {Jun},
	day          = {26},
	journal      = {Journal of Big Data},
	volume       = {2},
	number       = {1},
	pages        = {6},
	doi          = {10.1186/s40537-015-0020-5},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-015-0020-5},
	abstract     = {Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis.}
}
@article{naili2017comparative,
	title        = {Comparative study of word embedding methods in topic segmentation},
	author       = {Marwa Naili and Anja Habacha Chaibi and Henda Hajjami {Ben Ghezala}},
	year         = {2017},
	journal      = {Procedia Computer Science},
	volume       = {112},
	pages        = {340--349},
	doi          = {https://doi.org/10.1016/j.procs.2017.08.009},
	issn         = {1877-0509},
	url          = {https://www.sciencedirect.com/science/article/pii/S1877050917313480},
	note         = {Knowledge-Based and Intelligent Information  \& Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
	keywords     = {Word embedding, LSA, Word2Vec, GloVe, Topic segmentation},
	abstract     = {The vector representations of words are very useful in different natural language processing tasks in order to capture the semantic meaning of words. In this context, the three known methods are: LSA, Word2Vec and GloVe. In this paper, these methods will be investigated in the field of topic segmentation for both languages Arabic and English. Moreover, Word2Vec is studied in depth by using different models and approximation algorithms. As results, we found out that LSA, Word2Vec and GloVe depend on the used language. However, Word2Vec presents the best word vector representation yet it depends on the choice of model.}
}
@inproceedings{nakashima2022can,
	title        = {Can vision transformers learn without natural images?},
	author       = {Nakashima, Kodai and Kataoka, Hirokatsu and Matsumoto, Asato and Iwata, Kenji and Inoue, Nakamasa and Satoh, Yutaka},
	year         = {2022},
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = {36},
	number       = {2},
	pages        = {1990--1998}
}
@article{nallapati2016abstractive,
	title        = {Abstractive text summarization using sequence-to-sequence rnns and beyond},
	author       = {Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
	year         = {2016},
	journal      = {arXiv preprint arXiv:1602.06023}
}
@inproceedings{nallapati2017summarunner,
	title        = {SummaRuNNer: {A} Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
	author       = {Ramesh Nallapati and Feifei Zhai and Bowen Zhou},
	year         = {2017},
	booktitle    = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA}},
	publisher    = {{AAAI} Press},
	pages        = {3075--3081},
	url          = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636},
	editor       = {Satinder Singh and Shaul Markovitch},
	timestamp    = {Tue, 19 Apr 2022 16:03:28 +0200},
	biburl       = {https://dblp.org/rec/conf/aaai/NallapatiZZ17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@conference{namata2012query,
	title        = {Query-Driven Active Surveying for Collective Classification},
	author       = {Galileo Mark Namata, Ben London, Lise Getoor and Bert Huang},
	year         = {2012},
	booktitle    = {International Workshop on Mining and Learning with Graphs},
	publisher    = {MLG},
	address      = {Edinburgh, Scotland}
}
@inproceedings{naveed2011searching,
	title        = {Searching microblogs: coping with sparsity and document quality},
	author       = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'e}r{\^o}me and Alhadi, Arifah Che},
	year         = {2011},
	booktitle    = {Proceedings of the 20th ACM international conference on Information and knowledge management},
	pages        = {183--188}
}
@article{navigli2012babelnet,
	title        = {BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network},
	author       = {Roberto Navigli and Simone Paolo Ponzetto},
	year         = {2012},
	journal      = {Artificial Intelligence},
	volume       = {193},
	pages        = {217--250},
	doi          = {https://doi.org/10.1016/j.artint.2012.07.001},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370212000793},
	keywords     = {Knowledge acquisition, Word sense disambiguation, Graph algorithms, Semantic networks},
	abstract     = {We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.}
}
@misc{nelson_2011,
	title        = {Of monsters, men - and topic modeling},
	author       = {Nelson, Robert K.},
	year         = {2011},
	month        = {May},
	journal      = {The New York Times},
	publisher    = {The New York Times},
	url          = {http://opinionator.blogs.nytimes.com/2011/05/29/of-monsters-men-and-topic-modeling/}
}
@inproceedings{netzer2011reading,
	title        = {Reading Digits in Natural Images with Unsupervised Feature Learning},
	author       = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
	year         = {2011},
	booktitle    = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011},
	url          = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf}
}
@inproceedings{nevmyvaka2006reinforcement,
	title        = {Reinforcement Learning for Optimized Trade Execution},
	author       = {Nevmyvaka, Yuriy and Feng, Yi and Kearns, Michael},
	year         = {2006},
	booktitle    = {Proceedings of the 23rd International Conference on Machine Learning},
	location     = {Pittsburgh, Pennsylvania, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICML '06},
	pages        = {673--680},
	doi          = {10.1145/1143844.1143929},
	isbn         = {1595933832},
	url          = {https://doi.org/10.1145/1143844.1143929},
	abstract     = {We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.},
	numpages     = {8}
}
@inproceedings{neyshabur2018towards,
	title        = {The role of over-parametrization in generalization of neural networks},
	author       = {Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
	year         = {2019},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=BygfghAcYX}
}
@article{ng2001discriminative,
	title        = {On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes},
	author       = {Ng, Andrew and Jordan, Michael},
	year         = {2001},
	journal      = {Advances in neural information processing systems},
	volume       = {14}
}
@inproceedings{nguyen2016ms,
	title        = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
	author       = {Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng},
	year         = {2016},
	booktitle    = {Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems {(NIPS} 2016), Barcelona, Spain, December 9, 2016},
	publisher    = {CEUR-WS.org},
	series       = {{CEUR} Workshop Proceedings},
	volume       = {1773},
	url          = {http://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
	editor       = {Tarek Richard Besold and Antoine Bordes and Artur S. d'Avila Garcez and Greg Wayne},
	timestamp    = {Wed, 12 Feb 2020 16:44:20 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{nguyen2022master,
	title        = {Master Face Attacks on Face Recognition Systems},
	author       = {Nguyen, Huy H. and Marcel, Sebastien and Yamagishi, Junichi and Echizen, Isao},
	year         = {2022},
	journal      = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
	volume       = {4},
	number       = {3},
	pages        = {398--411},
	doi          = {10.1109/TBIOM.2022.3166206}
}
@article{nielsen2014generalized,
	title        = {Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means},
	author       = {Frank Nielsen},
	year         = {2014},
	journal      = {Pattern Recognition Letters},
	volume       = {42},
	pages        = {25--34},
	doi          = {https://doi.org/10.1016/j.patrec.2014.01.002},
	issn         = {0167-8655},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167865514000166},
	keywords     = {Affinity coefficient, Divergence, Chernoff information, Bhattacharrya distance, Total variation distance, Quasi-arithmetic means},
	abstract     = {Bayesian classification labels observations based on given prior information, namely class-a priori and class-conditional probabilities. Bayes’ risk is the minimum expected classification cost that is achieved by the Bayes’ test, the optimal decision rule. When no cost incurs for correct classification and unit cost is charged for misclassification, Bayes’ test reduces to the maximum a posteriori decision rule, and Bayes risk simplifies to Bayes’ error, the probability of error. Since calculating this probability of error is often intractable, several techniques have been devised to bound it with closed-form formula, introducing thereby measures of similarity and divergence between distributions like the Bhattacharyya coefficient and its associated Bhattacharyya distance. The Bhattacharyya upper bound can further be tightened using the Chernoff information that relies on the notion of best error exponent. In this paper, we first express Bayes’ risk using the total variation distance on scaled distributions. We then elucidate and extend the Bhattacharyya and the Chernoff upper bound mechanisms using generalized weighted means. We provide as a byproduct novel notions of statistical divergences and affinity coefficients. We illustrate our technique by deriving new upper bounds for the univariate Cauchy and the multivariate t-distributions, and show experimentally that those bounds are not too distant to the computationally intractable Bayes’ error.}
}
@article{nikolenko2017topic,
	title        = {Topic Modelling for Qualitative Studies},
	author       = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
	year         = {2017},
	month        = {feb},
	journal      = {J. Inf. Sci.},
	publisher    = {Sage Publications, Inc.},
	address      = {USA},
	volume       = {43},
	number       = {1},
	pages        = {88--102},
	doi          = {10.1177/0165551515617393},
	issn         = {0165-5515},
	url          = {https://doi.org/10.1177/0165551515617393},
	issue_date   = {2 2017},
	abstract     = {Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation LDA. However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach ISLDA where certain predefined sets of keywords that define the topics researchers are interested in are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.},
	numpages     = {15},
	keywords     = {Latent Dirichlet allocation, LDA extensions, topic quality, topic modelling}
}
@inproceedings{niu2014semi,
	title        = {Semi-supervised Relational Topic Model for Weakly Annotated Image Recognition in Social Media},
	author       = {Niu, Zhenxing and Hua, Gang and Gao, Xinbo and Tian, Qi},
	year         = {2014},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inbook{nivre2006inductive,
	title        = {Inductive Dependency Parsing},
	year         = {2006},
	booktitle    = {Inductive Dependency Parsing},
	publisher    = {Springer Netherlands},
	address      = {Dordrecht},
	pages        = {87--120},
	doi          = {10.1007/1-4020-4889-0_4},
	isbn         = {978-1-4020-4889-0},
	url          = {https://doi.org/10.1007/1-4020-4889-0_4}
}
@inproceedings{nomoto2012re,
	title        = {Re-ranking bibliographic records for personalized library search},
	author       = {Nomoto, Tadashi},
	year         = {2012},
	booktitle    = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
	location     = {Washington, DC, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {JCDL '12},
	pages        = {125–128},
	doi          = {10.1145/2232817.2232841},
	isbn         = {9781450311540},
	url          = {https://doi.org/10.1145/2232817.2232841},
	abstract     = {This work will introduce a new approach to ranking bibliographic records in library search, which is currently dominated by an OPAC style search paradigm, where results are typically not ranked by relevance. The approach we propose in the paper provides the user with the ability to access bibliographic records in a way responsive to his or her preferences, which is essentially done by looking at a community or a group of people who share interests with the user and making use of their publication records to re-rank search results. The experiment found that the present approach gives a clear edge over conventional search methods.},
	numpages     = {4},
	keywords     = {relevance ranking, OPAC}
}
@inproceedings{noraset2017definition,
	title        = {Definition modeling: Learning to define word embeddings in natural language},
	author       = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
	year         = {2017},
	booktitle    = {Thirty-First AAAI Conference on Artificial Intelligence}
}
@article{noshad2019learning,
	title        = {Learning to benchmark: Determining best achievable misclassification error from training data},
	author       = {Noshad, Morteza and Xu, Li and Hero, Alfred},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1909.07192}
}
@inproceedings{olteanu2014crisislex,
	title        = {Crisislex: A lexicon for collecting and filtering microblogged communications in crises},
	author       = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and Vieweg, Sarah},
	year         = {2014},
	booktitle    = {Eighth international AAAI conference on weblogs and social media}
}
@inproceedings{olteanu2015expect,
	title        = {What to Expect When the Unexpected Happens: Social Media Communications Across Crises},
	author       = {Olteanu, Alexandra and Vieweg, Sarah and Castillo, Carlos},
	year         = {2015},
	booktitle    = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work Social Computing},
	location     = {Vancouver, BC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CSCW '15},
	pages        = {994--1009},
	doi          = {10.1145/2675133.2675242},
	isbn         = {9781450329224},
	url          = {https://doi.org/10.1145/2675133.2675242},
	abstract     = {The use of social media to communicate timely information during crisis situations has become a common practice in recent years. In particular, the one-to-many nature of Twitter has created an opportunity for stakeholders to disseminate crisis-relevant messages, and to access vast amounts of information they may not otherwise have. Our goal is to understand what affected populations, response agencies and other stakeholders can expect-and not expect-from these data in various types of disaster situations. Anecdotal evidence suggests that different types of crises elicit different reactions from Twitter users, but we have yet to see whether this is in fact the case. In this paper, we investigate several crises-including natural hazards and human-induced disasters-in a systematic manner and with a consistent methodology. This leads to insights about the prevalence of different information types and sources across a variety of crisis situations.},
	numpages     = {16},
	keywords     = {emergency management, social media}
}
@misc{opennlp,
	title        = {The {Apache} {OpenNLP} library},
	author       = {The Apache Software Foundation},
	year         = {2017},
	note         = {http://opennlp.apache.org},
	howpublished = {Internet}
}
@article{osborne2004power,
	title        = {The power of outliers (and why researchers should always check for them)},
	author       = {Osborne, Jason W and Overbay, Amy},
	year         = {2004},
	journal      = {Practical Assessment, Research, and Evaluation},
	volume       = {9},
	number       = {1},
	pages        = {6}
}
@article{osterreicher2003new,
	title        = {A new class of metric divergences on probability spaces and its applicability in statistics},
	author       = {{\"O}sterreicher, Ferdinand and Vajda, Igor},
	year         = {2003},
	month        = {Sep},
	day          = {01},
	journal      = {Annals of the Institute of Statistical Mathematics},
	volume       = {55},
	number       = {3},
	pages        = {639--653},
	doi          = {10.1007/BF02517812},
	issn         = {1572-9052},
	url          = {https://doi.org/10.1007/BF02517812},
	abstract     = {The classIf$\beta$, $\beta$$\epsilon$(0, ∞], off-divergences investigated in this paper is defined in terms of a class of entropies introduced by Arimoto (1971,Information and Control,19, 181--194). It contains the squared Hellinger distance (for $\beta$=1/2), the sumI(Q1{\textbardbl}(Q1+Q2)/2)+I(Q2{\textbardbl}(Q1+Q2)/2) of Kullback-Leibler divergences (for $\beta$=1) and half of the variation distance (for $\beta$=∞) and continuously extends the class of squared perimeter-type distances introduced by {\"O}sterreicher (1996,Kybernetika,32, 389--393) (for $\beta$$\epsilon$ (1, ∞]). It is shown that{\$}{\$}(I{\_}{\{}f{\_}{\backslash}beta  {\}} (Q{\_}1 ,Q{\_}2 ))^{\{}{\backslash}min ({\backslash}beta ,1/2){\}}{\$}{\$}are distances of probability distributionsQ1,Q2 for $\beta$ $\epsilon$ (0, ∞). The applicability of{\$}{\$}I{\_}{\{}f{\_}{\backslash}beta  {\}}{\$}{\$}-divergences in statistics is also considered. In particular, it is shown that the{\$}{\$}I{\_}{\{}f{\_}{\backslash}beta  {\}}{\$}{\$}-projections of appropriate empirical distributions to regular families define distribution estimates which are in the case of an i.i.d. sample of size'n consistent. The order of consistency is investigated as well.}
}
@article{otsuka1936faunal,
	title        = {The faunal character of the Japanese Pleistocene marine Mollusca, as evidence of climate having become colder during the Pleistocene in Japan},
	author       = {Otsuka, Yanosuke},
	year         = {1936},
	journal      = {Biogeograph Soc Japan},
	volume       = {6},
	number       = {16},
	pages        = {165--170}
}
@article{over2007duc,
	title        = {DUC in context},
	author       = {Over, Paul and Dang, Hoa and Harman, Donna},
	year         = {2007},
	journal      = {Information Processing  \& Management},
	publisher    = {Elsevier},
	volume       = {43},
	number       = {6},
	pages        = {1506--1520}
}
@article{palivela2021optimization,
	title        = {Optimization of paraphrase generation and identification using language models in natural language processing},
	author       = {Hemant Palivela},
	year         = {2021},
	journal      = {International Journal of Information Management Data Insights},
	volume       = {1},
	number       = {2},
	pages        = {100025},
	doi          = {https://doi.org/10.1016/j.jjimei.2021.100025},
	issn         = {2667-0968},
	url          = {https://www.sciencedirect.com/science/article/pii/S2667096821000185},
	keywords     = {Paraphrase identification, Paraphrase generation, Natural language generation, Language model, Encoder decoder, Transformer},
	abstract     = {Paraphrase Generation is one of the most important and challenging tasks in the field of Natural Language Generation. The paraphrasing techniques help to identify or to extract/generate phrases/sentences conveying the similar meaning. The paraphrasing task can be bifurcated into two sub-tasks namely, Paraphrase Identification (PI) and Paraphrase Generation (PG). Most of the existing proposed state-of-the-art systems have the potential to solve only one problem at a time. This paper proposes a light-weight unified model that can simultaneously classify whether given pair of sentences are paraphrases of each other and the model can also generate multiple paraphrases given an input sentence. Paraphrase Generation module aims to generate fluent and semantically similar paraphrases and the Paraphrase Identification system aims to classify whether sentences pair are paraphrases of each other or not. The proposed approach uses an amalgamation of data sampling or data variety with a granular fine-tuned Text-To-Text Transfer Transformer (T5) model. This paper proposes a unified approach which aims to solve the problems of Paraphrase Identification and generation by using carefully selected data-points and a fine-tuned T5 model. The highlight of this study is that the same light-weight model trained by keeping the objective of Paraphrase Generation can also be used for solving the Paraphrase Identification task. Hence, the proposed system is light-weight in terms of the model's size along with the data used to train the model which facilitates the quick learning of the model without having to compromise with the results. The proposed system is then evaluated against the popular evaluation metrics like BLEU (BiLingual Evaluation Understudy):, ROUGE (Recall-Oriented Understudy for Gisting Evaluation), METEOR, WER (Word Error Rate), and GLEU (Google-BLEU) for Paraphrase Generation and classification metrics like accuracy, precision, recall and F1-score for Paraphrase Identification system. The proposed model achieves state-of-the-art results on both the tasks of Paraphrase Identification and paraphrase Generation.}
}
@article{pang2002thumbs,
	title        = {Thumbs up? Sentiment classification using machine learning techniques},
	author       = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
	year         = {2002},
	journal      = {arXiv preprint cs/0205070}
}
@article{pang2019robust,
	title        = {Robust heterogeneous discriminative analysis for face recognition with single sample per person},
	author       = {Meng Pang and Yiu-ming Cheung and Binghui Wang and Risheng Liu},
	year         = {2019},
	journal      = {Pattern Recognition},
	volume       = {89},
	pages        = {91--107},
	doi          = {https://doi.org/10.1016/j.patcog.2019.01.005},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/S0031320319300111},
	keywords     = {Face recognition, Single sample per person, Heterogeneous representation, Fisher-like criterion, Joint majority voting},
	abstract     = {Single sample per person face recognition is one of the most challenging problems in face recognition (FR), where only single sample per person (SSPP) is enrolled in the gallery set for training. Although the existing patch-based methods have achieved great success in FR with SSPP, they still have limitations in feature extraction and identification stages when handling complex facial variations. In this work, we propose a new patch-based method called Robust Heterogeneous Discriminative Analysis (RHDA), for FR with SSPP. To enhance the robustness against complex facial variations, we first present a new graph-based Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing the similarities between neighboring patches from the different persons. Then, we introduce two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion strategy to combine the recognition outputs of above two distance metrics via a joint majority voting for identification. Experimental results on various benchmark datasets demonstrate the effectiveness of the proposed method.}
}
@inproceedings{pang2022robustness,
	title        = {Robustness and Accuracy Could Be Reconcilable by (Proper) Definition},
	author       = {Tianyu Pang and Min Lin and Xiao Yang and Jun Zhu and Shuicheng Yan},
	year         = {2022},
	booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = {162},
	pages        = {17258--17277},
	url          = {https://proceedings.mlr.press/v162/pang22a.html},
	editor       = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan Sabato},
	timestamp    = {Tue, 12 Jul 2022 17:36:52 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/PangLYZY22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{papernot2016limitations,
	title        = {The limitations of deep learning in adversarial settings},
	author       = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	year         = {2016},
	booktitle    = {2016 IEEE European Symposium on Security and Privacy (EuroS\&P)},
	pages        = {372--387},
	organization = {IEEE}
}
@article{papernot2016transferability,
	title        = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples},
	author       = {Nicolas Papernot and Patrick D. McDaniel and Ian J. Goodfellow},
	year         = {2016},
	journal      = {CoRR},
	volume       = {abs/1605.07277},
	url          = {http://arxiv.org/abs/1605.07277},
	eprinttype   = {arXiv},
	eprint       = {1605.07277},
	timestamp    = {Mon, 13 Aug 2018 16:48:28 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/PapernotMG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{papernot2017practical,
	title        = {Practical black-box attacks against machine learning},
	author       = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
	year         = {2017},
	booktitle    = {Proceedings of the 2017 ACM on Asia conference on computer and communications security},
	pages        = {506--519}
}
@article{paranyushkin2011identifying,
	title        = {Identifying the pathways for meaning circulation using text network analysis},
	author       = {Paranyushkin, Dmitry},
	year         = {2011},
	journal      = {Nodus Labs},
	volume       = {26},
	pages        = {1--26}
}
@article{parthasarathi2020evaluate,
	title        = {How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for Token-level Evaluation Metrics},
	author       = {Prasanna Parthasarathi and Joelle Pineau and Sarath Chandar},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2008.10427},
	url          = {https://arxiv.org/abs/2008.10427},
	eprinttype   = {arXiv},
	eprint       = {2008.10427},
	timestamp    = {Fri, 28 Aug 2020 12:11:44 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2008-10427.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{paszke2019pytorch,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = {2019},
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	volume       = {32},
	pages        = {8024--8035},
	url          = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{paulsen2020neurodiff,
	title        = {NeuroDiff: scalable differential verification of neural networks using fine-grained approximation},
	author       = {Paulsen, Brandon and Wang, Jingbo and Wang, Jiawei and Wang, Chao},
	year         = {2021},
	booktitle    = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
	location     = {Virtual Event, Australia},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASE '20},
	pages        = {784–796},
	doi          = {10.1145/3324884.3416560},
	isbn         = {9781450367684},
	url          = {https://doi.org/10.1145/3324884.3416560},
	abstract     = {As neural networks make their way into safety-critical systems, where misbehavior can lead to catastrophes, there is a growing interest in certifying the equivalence of two structurally similar neural networks - a problem known as differential verification. For example, compression techniques are often used in practice for deploying trained neural networks on computationally- and energy-constrained devices, which raises the question of how faithfully the compressed network mimics the original network. Unfortunately, existing methods either focus on verifying a single network or rely on loose approximations to prove the equivalence of two networks. Due to overly conservative approximation, differential verification lacks scalability in terms of both accuracy and computational cost. To overcome these problems, we propose NeuroDiff, a symbolic and fine-grained approximation technique that drastically increases the accuracy of differential verification on feed-forward ReLU networks while achieving many orders-of-magnitude speedup. NeuroDiff has two key contributions. The first one is new convex approximations that more accurately bound the difference of two networks under all possible inputs. The second one is judicious use of symbolic variables to represent neurons whose difference bounds have accumulated significant error. We find that these two techniques are complementary, i.e., when combined, the benefit is greater than the sum of their individual benefits. We have evaluated NeuroDiff on a variety of differential verification tasks. Our results show that NeuroDiff is up to 1000X faster and 5X more accurate than the state-of-the-art tool.},
	numpages     = {13}
}
@inproceedings{paulus2017deep,
	title        = {A Deep Reinforced Model for Abstractive Summarization},
	author       = {Romain Paulus and Caiming Xiong and Richard Socher},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HkAClQgA-},
	timestamp    = {Thu, 25 Jul 2019 14:25:58 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/PaulusXS18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{pawar2019challenging,
	title        = {Challenging the Boundaries of Unsupervised Learning for Semantic Similarity},
	author       = {Pawar, Atish and Mago, Vijay},
	year         = {2019},
	journal      = {IEEE Access},
	volume       = {7},
	pages        = {16291--16308},
	doi          = {10.1109/ACCESS.2019.2891692}
}
@article{pedersen2007measures,
	title        = {Measures of semantic similarity and relatedness in the biomedical domain},
	author       = {Ted Pedersen and Serguei V.S. Pakhomov and Siddharth Patwardhan and Christopher G. Chute},
	year         = {2007},
	journal      = {Journal of Biomedical Informatics},
	volume       = {40},
	number       = {3},
	pages        = {288--299},
	doi          = {https://doi.org/10.1016/j.jbi.2006.06.004},
	issn         = {1532-0464},
	url          = {https://www.sciencedirect.com/science/article/pii/S1532046406000645},
	keywords     = {Semantic similarity, Path based measures, Information content, Context vectors, SNOMED-CT},
	abstract     = {Measures of semantic similarity between concepts are widely used in Natural Language Processing. In this article, we show how six existing domain-independent measures can be adapted to the biomedical domain. These measures were originally based on WordNet, an English lexical database of concepts and relations. In this research, we adapt these measures to the SNOMED-CT® ontology of medical concepts. The measures include two path-based measures, and three measures that augment path-based measures with information content statistics from corpora. We also derive a context vector measure based on medical corpora that can be used as a measure of semantic relatedness. These six measures are evaluated against a newly created test bed of 30 medical concept pairs scored by three physicians and nine medical coders. We find that the medical coders and physicians differ in their ratings, and that the context vector measure correlates most closely with the physicians, while the path-based measures and one of the information content measures correlates most closely with the medical coders. We conclude that there is a role both for more flexible measures of relatedness based on information derived from corpora, as well as for measures that rely on existing ontological structures.}
}
@article{pedregosa2011scikit,
	title        = {Scikit-learn: Machine Learning in Python},
	author       = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
	year         = {2011},
	journal      = {Journal of Machine Learning Research},
	volume       = {12},
	number       = {85},
	pages        = {2825--2830},
	url          = {http://jmlr.org/papers/v12/pedregosa11a.html}
}
@inproceedings{pei2017deepxplore,
	title        = {DeepXplore: Automated Whitebox Testing of Deep Learning Systems},
	author       = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
	year         = {2017},
	booktitle    = {Proceedings of the 26th Symposium on Operating Systems Principles},
	location     = {Shanghai, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SOSP '17},
	pages        = {1–18},
	doi          = {10.1145/3132747.3132785},
	isbn         = {9781450350853},
	url          = {https://doi.org/10.1145/3132747.3132785},
	abstract     = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.We design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3\%.},
	numpages     = {18},
	keywords     = {whitebox testing, differential testing, Deep learning testing}
}
@inproceedings{perez2017fair,
	title        = {Fair Kernel Learning},
	author       = {P{\'e}rez-Suay, Adri{\'a}n and Laparra, Valero and Mateo-Garc{\'i}a, Gonzalo and Mu{\~{n}}oz-Mar{\'i}, Jordi and G{\'o}mez-Chova, Luis and Camps-Valls, Gustau},
	year         = {2017},
	booktitle    = {Machine Learning and Knowledge Discovery in Databases},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {339--355},
	isbn         = {978-3-319-71249-9},
	editor       = {Ceci, Michelangelo and Hollm{\'e}n, Jaakko and Todorovski, Ljup{\v{c}}o and Vens, Celine and D{\v{z}}eroski, Sa{\v{s}}o},
	abstract     = {New social and economic activities massively exploit big data and machine learning algorithms to do inference on people's lives. Applications include automatic curricula evaluation, wage determination, and risk assessment for credits and loans. Recently, many governments and institutions have raised concerns about the lack of fairness, equity and ethics in machine learning to treat these problems. It has been shown that not including sensitive features that bias fairness, such as gender or race, is not enough to mitigate the discrimination when other related features are included. Instead, including fairness in the objective function has been shown to be more efficient.}
}
@inproceedings{peterson2019human,
	title        = {Human Uncertainty Makes Classification More Robust},
	author       = {Peterson, Joshua C. and Battleday, Ruairidh M. and Griffiths, Thomas L. and Russakovsky, Olga},
	year         = {2019},
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}
}
@inproceedings{peychev2022latent,
	title        = {Latent Space Smoothing for Individually Fair Representations},
	author       = {Peychev, Momchil and Ruoss, Anian and Balunovi{\'{c}}, Mislav and Baader, Maximilian and Vechev, Martin},
	year         = {2022},
	booktitle    = {Computer Vision -- ECCV 2022},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {535--554},
	isbn         = {978-3-031-19778-9},
	editor       = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	abstract     = {Fair representation learning transforms user data into a representation that ensures fairness and utility regardless of the downstream application. However, learning individually fair representations, i.e., guaranteeing that similar individuals are treated similarly, remains challenging in high-dimensional settings such as computer vision. In this work, we introduce LASSI, the first representation learning method for certifying individual fairness of high-dimensional data. Our key insight is to leverage recent advances in generative modeling to capture the set of similar individuals in the generative latent space. This enables us to learn individually fair representations that map similar individuals close together by using adversarial training to minimize the distance between their representations. Finally, we employ randomized smoothing to provably map similar individuals close together, in turn ensuring that local robustness verification of the downstream application results in end-to-end fairness certification. Our experimental evaluation on challenging real-world image data demonstrates that our method increases certified individual fairness by up to 90{\%} without significantly affecting task utility.}
}
@misc{phillips2013distances,
	title        = {L7-Distances},
	author       = {Phillips, Jeff M},
	year         = {2013},
	journal      = {cs5955},
	publisher    = {University of Utah},
	url          = {https://www.cs.utah.edu/~jeffp/teaching/cs5955/L7-Distances.pdf}
}
@inproceedings{pierazzi2020intriguing,
	title        = {Intriguing Properties of Adversarial ML Attacks in the Problem Space},
	author       = {Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
	year         = {2020},
	booktitle    = {2020 IEEE Symposium on Security and Privacy (SP)},
	volume       = {},
	number       = {},
	pages        = {1332--1349},
	doi          = {10.1109/SP40000.2020.00073}
}
@inproceedings{plummer2015flickr30k,
	title        = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
	author       = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
	year         = {2015},
	month        = {December},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@article{pocock1977group,
	title        = {Group sequential methods in the design and analysis of clinical trials},
	author       = {Pocock, Stuart J},
	year         = {1977},
	journal      = {Biometrika},
	publisher    = {Oxford University Press},
	volume       = {64},
	number       = {2},
	pages        = {191--199}
}
@inproceedings{pomponi2022pixle,
	title        = {Pixle: a fast and effective black-box attack based on rearranging pixels},
	author       = {Pomponi, Jary and Scardapane, Simone and Uncini, Aurelio},
	year         = {2022},
	booktitle    = {2022 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1--7},
	doi          = {10.1109/IJCNN55064.2022.9892966}
}
@article{ponomareva2023dp,
	title        = {How to dp-fy ml: A practical guide to machine learning with differential privacy},
	author       = {Ponomareva, Natalia and Hazimeh, Hussein and Kurakin, Alex and Xu, Zheng and Denison, Carson and McMahan, H Brendan and Vassilvitskii, Sergei and Chien, Steve and Thakurta, Abhradeep Guha},
	year         = {2023},
	journal      = {Journal of Artificial Intelligence Research},
	volume       = {77},
	pages        = {1113--1201}
}
@article{postaire1982unsupervised,
	title        = {An unsupervised Bayes classifier for normal patterns based on marginal densities analysis},
	author       = {J.-G. Postaire},
	year         = {1982},
	journal      = {Pattern Recognition},
	volume       = {15},
	number       = {2},
	pages        = {103--111},
	doi          = {https://doi.org/10.1016/0031-3203(82)90005-X},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/003132038290005X},
	keywords     = {Unsupervised classification, Normal mixture identification, Marginal densities, Minimum error-rate classification},
	abstract     = {In this paper, an approach to unsupervised pattern classification is discussed. The classification scheme is based on an approximation of the probability densities of each class under the assumption that the input patterns are of a normal mixture. The description of the marginal densities in terms of convexity allows one to determine, from a totally unlabelled set of samples, the number of components and, for each of them, approximate values of the mean vector, the covariance matrix and the a priori probability. Discriminant functions can then be constructed. Computer simulations show that the procedure yields decision rules whose performance remains close to the optimum Bayes minimum error-rate, while involving only a small amount of computation.}
}
@inproceedings{pulina2010abstraction,
	title        = {An Abstraction-Refinement Approach to Verification of Artificial Neural Networks},
	author       = {Pulina, Luca and Tacchella, Armando},
	year         = {2010},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {243--257},
	isbn         = {978-3-642-14295-6},
	editor       = {Touili, Tayssir and Cook, Byron and Jackson, Paul},
	abstract     = {A key problem in the adoption of artificial neural networks in safety-related applications is that misbehaviors can be hardly ruled out with traditional analytical or probabilistic techniques. In this paper we focus on specific networks known as Multi-Layer Perceptrons (MLPs), and we propose a solution to verify their safety using abstractions to Boolean combinations of linear arithmetic constraints. We show that our abstractions are consistent, i.e., whenever the abstract MLP is declared to be safe, the same holds for the concrete one. Spurious counterexamples, on the other hand, trigger refinements and can be leveraged to automate the correction of misbehaviors. We describe an implementation of our approach based on the HySAT solver, detailing the abstraction-refinement process and the automated correction strategy. Finally, we present experimental results confirming the feasibility of our approach on a realistic case study.}
}
@article{pymoo,
	title        = {pymoo: Multi-Objective Optimization in Python},
	author       = {J. {Blank} and K. {Deb}},
	year         = {2020},
	journal      = {IEEE Access},
	volume       = {8},
	number       = {},
	pages        = {89497--89509}
}
@misc{pytorchexample,
	title        = {Basic MNIST Example},
	author       = {Pytorch},
	journal      = {GitHub},
	url          = {https://github.com/pytorch/examples/tree/main/mnist}
}
@article{qu2018computing,
	title        = {Computing semantic similarity based on novel models of semantic representation using Wikipedia},
	author       = {Rong Qu and Yongyi Fang and Wen Bai and Yuncheng Jiang},
	year         = {2018},
	journal      = {Information Processing  \& Management},
	volume       = {54},
	number       = {6},
	pages        = {1002--1021},
	doi          = {https://doi.org/10.1016/j.ipm.2018.07.002},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457317309226},
	keywords     = {Semantic similarity, Concept similarity, Information content, Feature-based methods, Wikipedia},
	abstract     = {Computing Semantic Similarity (SS) between concepts is one of the most critical issues in many domains such as Natural Language Processing and Artificial Intelligence. Over the years, several SS measurement methods have been proposed by exploiting different knowledge resources. Wikipedia provides a large domain-independent encyclopedic repository and a semantic network for computing SS between concepts. Traditional feature-based measures rely on linear combinations of different properties with two main limitations, the insufficient information and the loss of semantic information. In this paper, we propose several hybrid SS measurement approaches by using the Information Content (IC) and features of concepts, which avoid the limitations introduced above. Considering integrating discrete properties into one component, we present two models of semantic representation, called CORM and CARM. Then, we compute SS based on these models and take the IC of categories as a supplement of SS measurement. The evaluation, based on several widely used benchmarks and a benchmark developed by ourselves, sustains the intuitions with respect to human judgments. In summary, our approaches are more efficient in determining SS between concepts and have a better human correlation than previous methods such as Word2Vec and NASARI.}
}
@article{radford2019language,
	title        = {Language models are unsupervised multitask learners},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = {2019},
	journal      = {OpenAI blog},
	volume       = {1},
	number       = {8},
	pages        = {9}
}
@inproceedings{radford2021learning,
	title        = {Learning transferable visual models from natural language supervision},
	author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	year         = {2021},
	booktitle    = {International conference on machine learning},
	pages        = {8748--8763},
	organization = {PmLR}
}
@article{raffel2019exploring,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1910.10683},
	url          = {http://arxiv.org/abs/1910.10683},
	eprinttype   = {arXiv},
	eprint       = {1910.10683},
	timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{raghavan2007near,
	title        = {Near linear time algorithm to detect community structures in large-scale networks},
	author       = {Raghavan, Usha Nandini and Albert, R\'eka and Kumara, Soundar},
	year         = {2007},
	month        = {Sep},
	journal      = {Phys. Rev. E},
	publisher    = {American Physical Society},
	volume       = {76},
	pages        = {036106},
	doi          = {10.1103/PhysRevE.76.036106},
	url          = {https://link.aps.org/doi/10.1103/PhysRevE.76.036106},
	issue        = {3},
	numpages     = {11}
}
@inproceedings{raghavan2020mitigating,
	title        = {Mitigating bias in algorithmic hiring: evaluating claims and practices},
	author       = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
	year         = {2020},
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	location     = {Barcelona, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAT* '20},
	pages        = {469–481},
	doi          = {10.1145/3351095.3372828},
	isbn         = {9781450369367},
	url          = {https://doi.org/10.1145/3351095.3372828},
	abstract     = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
	numpages     = {13},
	keywords     = {algorithmic bias, algorithmic hiring, discrimination law}
}
@inproceedings{raghunathan2018certified,
	title        = {Certified Defenses against Adversarial Examples},
	author       = {Aditi Raghunathan and Jacob Steinhardt and Percy Liang},
	year         = {2018},
	journal      = {arXiv preprint arXiv:1801.09344},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Bys4ob-Rb},
	timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/RaghunathanSL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{raghunathan2019adversarial,
	title        = {Adversarial Training Can Hurt Generalization},
	author       = {Aditi Raghunathan and Sang Michael Xie and Fanny Yang and John C. Duchi and Percy Liang},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1906.06032},
	url          = {http://arxiv.org/abs/1906.06032},
	eprinttype   = {arXiv},
	eprint       = {1906.06032},
	timestamp    = {Sat, 23 Jan 2021 01:14:32 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1906-06032.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{ragoza2017protein,
	title        = {Protein-Ligand Scoring with Convolutional Neural Networks},
	author       = {Ragoza, Matthew and Hochuli, Joshua and Idrobo, Elisa and Sunseri, Jocelyn and Koes, David Ryan},
	year         = {2017},
	journal      = {Journal of Chemical Information and Modeling},
	volume       = {57},
	number       = {4},
	pages        = {942--957},
	doi          = {10.1021/acs.jcim.6b00740},
	url          = {https://doi.org/10.1021/acs.jcim.6b00740},
	note         = {PMID: 28368587},
	eprint       = {https://doi.org/10.1021/acs.jcim.6b00740}
}
@article{rajagopalan1995kernel,
	title        = {A kernel estimator for discrete distributions},
	author       = {Balaji Rajagopalan and Upmanu Lall},
	year         = {1995},
	journal      = {Journal of Nonparametric Statistics},
	publisher    = {Taylor \& Francis},
	volume       = {4},
	number       = {4},
	pages        = {409--426},
	doi          = {10.1080/10485259508832629},
	url          = {https://doi.org/10.1080/10485259508832629},
	eprint       = {https://doi.org/10.1080/10485259508832629}
}
@inbook{rajaraman2011mining,
	title        = {Finding Similar Items},
	author       = {Leskovec, Jurij and Rajaraman, Anand and Ullman, Jeffrey David},
	year         = {2022},
	booktitle    = {Mining of massive datasets},
	publisher    = {Cambridge University Press},
	pages        = {76--78},
	url          = {http://infolab.stanford.edu/~ullman/mmds/ch3.pdf},
	place        = {Cambridge etc.}
}
@article{ramachandra2017presentation,
	title        = {Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey},
	author       = {Ramachandra, Raghavendra and Busch, Christoph},
	year         = {2017},
	month        = {mar},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {50},
	number       = {1},
	doi          = {10.1145/3038924},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3038924},
	issue_date   = {January 2018},
	abstract     = {The vulnerability of face recognition systems to presentation attacks (also known as direct attacks or spoof attacks) has received a great deal of interest from the biometric community. The rapid evolution of face recognition systems into real-time applications has raised new concerns about their ability to resist presentation attacks, particularly in unattended application scenarios such as automated border control. The goal of a presentation attack is to subvert the face recognition system by presenting a facial biometric artifact. Popular face biometric artifacts include a printed photo, the electronic display of a facial photo, replaying video using an electronic display, and 3D face masks. These have demonstrated a high security risk for state-of-the-art face recognition systems. However, several presentation attack detection (PAD) algorithms (also known as countermeasures or antispoofing methods) have been proposed that can automatically detect and mitigate such targeted attacks. The goal of this survey is to present a systematic overview of the existing work on face presentation attack detection that has been carried out. This paper describes the various aspects of face presentation attacks, including different types of face artifacts, state-of-the-art PAD algorithms and an overview of the respective research labs working in this domain, vulnerability assessments and performance evaluation metrics, the outcomes of competitions, the availability of public databases for benchmarking new PAD algorithms in a reproducible manner, and finally a summary of the relevant international standardization in this field. Furthermore, we discuss the open challenges and future work that need to be addressed in this evolving field of biometrics.},
	articleno    = {8},
	numpages     = {37},
	keywords     = {security, countermeasure, face recognition, antispoofing, attacks, Biometrics}
}
@inproceedings{ramachandra2019custom,
	title        = {Custom silicone Face Masks: Vulnerability of Commercial Face Recognition Systems \& Presentation Attack Detection},
	author       = {Ramachandra, Raghavendra and Venkatesh, Sushma and Raja, Kiran B. and Bhattacharjee, Sushil and Wasnik, Pankaj and Marcel, Sebastien and Busch, Christoph},
	year         = {2019},
	booktitle    = {2019 7th International Workshop on Biometrics and Forensics (IWBF)},
	volume       = {},
	number       = {},
	pages        = {1--6},
	doi          = {10.1109/IWBF.2019.8739236}
}
@article{ramesh2021automated,
	title        = {An automated essay scoring systems: a systematic literature review},
	author       = {Dadi Ramesh and Suresh Kumar Sanampudi},
	year         = {2022},
	journal      = {Artif. Intell. Rev.},
	volume       = {55},
	number       = {3},
	pages        = {2495--2527},
	doi          = {10.1007/s10462-021-10068-2},
	url          = {https://doi.org/10.1007/s10462-021-10068-2},
	timestamp    = {Fri, 01 Apr 2022 11:23:48 +0200},
	biburl       = {https://dblp.org/rec/journals/air/RameshS22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ramos2003using,
	title        = {Using tf-idf to determine word relevance in document queries},
	author       = {Ramos, Juan and others},
	year         = {2003},
	booktitle    = {Proceedings of the first instructional conference on machine learning},
	volume       = {242},
	number       = {1},
	pages        = {29--48},
	organization = {Citeseer}
}
@article{rasooli-tetrault-2015,
	title        = {Yara Parser: {A} Fast and Accurate Dependency Parser},
	author       = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
	year         = {2015},
	journal      = {Computing Research Repository},
	volume       = {arXiv:1503.06733},
	url          = {http://arxiv.org/abs/1503.06733},
	note         = {version 2}
}
@article{rauber2020foolbox,
	title        = {Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX},
	author       = {Jonas Rauber and Roland Zimmermann and Matthias Bethge and Wieland Brendel},
	year         = {2020},
	journal      = {Journal of Open Source Software},
	publisher    = {The Open Journal},
	volume       = {5},
	number       = {53},
	pages        = {2607},
	doi          = {10.21105/joss.02607},
	url          = {https://doi.org/10.21105/joss.02607}
}
@article{ravaut2022summaReranker,
	title        = {SummaReranker: {A} Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization},
	author       = {Mathieu Ravaut and Shafiq Joty and Nancy F. Chen},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2203.06569},
	doi          = {10.48550/arXiv.2203.06569},
	url          = {https://doi.org/10.48550/arXiv.2203.06569},
	eprinttype   = {arXiv},
	eprint       = {2203.06569},
	timestamp    = {Wed, 16 Mar 2022 16:41:29 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2203-06569.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{rawte2020comparative,
	title        = {A Comparative Analysis of Temporal Long Text Similarity: Application to Financial Documents},
	author       = {Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed J.},
	year         = {2021},
	booktitle    = {Mining Data for Financial Applications},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {77--91},
	isbn         = {978-3-030-66981-2},
	editor       = {Bitetta, Valerio and Bordino, Ilaria and Ferretti, Andrea and Gullo, Francesco and Ponti, Giovanni and Severini, Lorenzo},
	abstract     = {Temporal text documents exist in many real-world domains. These may span over long periods of time during which there tend to be many variations in the text. In particular, variations or the similarities in a pair of documents over two consecutive years could be meaningful. Most of the textual analysis work like text classification focuses on the entire text snippet as a data instance. It is therefore important to study such similarities besides the entire text document. In Natural Language Processing (NLP), the task of textual similarity is important for search and query retrieval. This task is also better known as Semantic Textual Similarity (STS) that aims to capture the semantics of two texts while comparing them. Also, state-of-the-art methods predominantly target short texts. Thus, measuring the semantic similarity between a pair of long texts is still a challenge. In this paper, we compare different text matching methods for the documents over two consecutive years. We focus on their similarities for our comparative analysis and evaluation of financial documents, namely public 10-K filings to the SEC (Securities and Exchange Commission). We further perform textual regression analysis on six quantitative bank variables including Return on Assets (ROA), Earnings per Share (EPS), Tobin's Q Ratio, Tier 1 Capital Ratio, Leverage Ratio, and Z-score, and show that textual features can be effective in predicting these variables.}
}
@inproceedings{raz2021group,
	title        = {Group Fairness: Independence Revisited},
	author       = {R\"{a}z, Tim},
	year         = {2021},
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Virtual Event, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '21},
	pages        = {129–137},
	doi          = {10.1145/3442188.3445876},
	isbn         = {9781450383097},
	url          = {https://doi.org/10.1145/3442188.3445876},
	abstract     = {This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.},
	numpages     = {9},
	keywords     = {accuracy, affirmative action, demographic parity, fairness, independence, separation, statistical parity, sufficiency}
}
@inproceedings{ren2022cross,
	title        = {Cross-network social user embedding with hybrid differential privacy guarantees},
	author       = {Ren, Jiaqian and Jiang, Lei and Peng, Hao and Lyu, Lingjuan and Liu, Zhiwei and Chen, Chaochao and Wu, Jia and Bai, Xu and Yu, Philip S},
	year         = {2022},
	booktitle    = {Proceedings of the 31st ACM international conference on information \& knowledge management},
	pages        = {1685--1695}
}
@article{ren2022transferable,
	title        = {Transferable unlearnable examples},
	author       = {Ren, Jie and Xu, Han and Wan, Yuxuan and Ma, Xingjun and Sun, Lichao and Tang, Jiliang},
	year         = {2022},
	journal      = {arXiv preprint arXiv:2210.10114}
}
@inproceedings{rendle2010factorizing,
	title        = {Factorizing personalized markov chains for next-basket recommendation},
	author       = {Rendle, Steffen and Freudenthaler, Christoph and Schmidt-Thieme, Lars},
	year         = {2010},
	booktitle    = {Proceedings of the 19th international conference on World wide web},
	pages        = {811--820}
}
@inproceedings{rendle2012bpr,
	title        = {BPR: Bayesian personalized ranking from implicit feedback},
	author       = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
	year         = {2009},
	booktitle    = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
	location     = {Montreal, Quebec, Canada},
	publisher    = {AUAI Press},
	address      = {Arlington, Virginia, USA},
	series       = {UAI '09},
	pages        = {452–461},
	isbn         = {9780974903958},
	abstract     = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
	numpages     = {10}
}
@article{renggli2020ease,
	title        = {Ease.Ml/Snoopy in Action: Towards Automatic Feasibility Analysis for Machine Learning Application Development},
	author       = {Renggli, Cedric and Rimanic, Luka and Kolar, Luka and Wu, Wentao and Zhang, Ce},
	year         = {2020},
	month        = {aug},
	journal      = {Proc. VLDB Endow.},
	publisher    = {VLDB Endowment},
	volume       = {13},
	number       = {12},
	pages        = {2837–2840},
	doi          = {10.14778/3415478.3415488},
	issn         = {2150-8097},
	url          = {https://doi.org/10.14778/3415478.3415488},
	issue_date   = {August 2020},
	abstract     = {We demonstrate ease.ml/snoopy, a data analytics system that performs feasibility analysis for machine learning (ML) applications before they are developed. Given a performance target of an ML application (e.g., accuracy above 0.95), ease.ml/snoopy provides a decisive answer to ML developers regarding whether the target is achievable or not. We formulate the feasibility analysis problem as an instance of Bayes error estimation. That is, for a data (distribution) on which the ML application should be performed, ease.ml/snoopy provides an estimate of the Bayes error - the minimum error rate that can be achieved by any classifier. It is well-known that estimating the Bayes error is a notoriously hard task. In ease.ml/snoopy we explore and employ estimators based on the combination of (1) nearest neighbor (NN) classifiers and (2) pre-trained feature transformations. To the best of our knowledge, this is the first work on Bayes error estimation that combines (1) and (2). In today's cost-driven business world, feasibility of an ML project is an ideal piece of information for ML application developers - ease.ml/snoopy plays the role of a reliable "consultant."},
	numpages     = {4}
}
@inproceedings{renggli2021evaluating,
	title        = {Evaluating Bayes Error Estimators on Real-World Datasets with FeeBee},
	author       = {C{\'{e}}dric Renggli and Luka Rimanic and Nora Hollenstein and Ce Zhang},
	year         = {2021},
	booktitle    = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual},
	url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/045117b0e0a11a242b9765e79cbf113f-Abstract-round2.html},
	editor       = {Joaquin Vanschoren and Sai{-}Kit Yeung},
	timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/RenggliRH021.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{resnik1995using,
	title        = {Using Information Content to Evaluate Semantic Similarity in a Taxonomy},
	author       = {Philip Resnik},
	year         = {1995},
	booktitle    = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, {IJCAI} 95, Montr{\'{e}}al Qu{\'{e}}bec, Canada, August 20-25 1995, 2 Volumes},
	publisher    = {Morgan Kaufmann},
	pages        = {448--453},
	url          = {http://ijcai.org/Proceedings/95-1/Papers/059.pdf},
	timestamp    = {Tue, 20 Aug 2019 16:17:30 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/Resnik95.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{rice2021robustness,
	title        = {Robustness between the worst and average case},
	author       = {Rice, Leslie and Bair, Anna and Zhang, Huan and Kolter, J Zico},
	year         = {2021},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {34},
	pages        = {27840--27851}
}
@phdthesis{rich2017collaborative,
	title        = {Collaborative scientific publishing: a new research ecosystem},
	author       = {Rich, Travis Travis Sebastian},
	year         = {2017},
	school       = {Massachusetts Institute of Technology}
}
@inproceedings{richardson2007predicting,
	title        = {Predicting clicks: estimating the click-through rate for new ads},
	author       = {Richardson, Matthew and Dominowska, Ewa and Ragno, Robert},
	year         = {2007},
	booktitle    = {Proceedings of the 16th International Conference on World Wide Web},
	location     = {Banff, Alberta, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '07},
	pages        = {521–530},
	doi          = {10.1145/1242572.1242643},
	isbn         = {9781595936547},
	url          = {https://doi.org/10.1145/1242572.1242643},
	abstract     = {Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.},
	numpages     = {10},
	keywords     = {CPC, CTR, click-through rate, paid search, ranking, sponsored search, web advertising}
}
@book{ripley1996pattern,
	title        = {Pattern Recognition and Neural Networks},
	author       = {Ripley, Brian D.},
	year         = {1996},
	publisher    = {Cambridge University Press},
	doi          = {10.1017/CBO9780511812651},
	place        = {Cambridge}
}
@inproceedings{ritter2012open,
	title        = {Open Domain Event Extraction from Twitter},
	author       = {Ritter, Alan and Mausam and Etzioni, Oren and Clark, Sam},
	year         = {2012},
	booktitle    = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '12},
	pages        = {1104--1112},
	doi          = {10.1145/2339530.2339704},
	isbn         = {9781450314626},
	url          = {https://doi.org/10.1145/2339530.2339704},
	abstract     = {Tweets are the most up-to-date and inclusive stream of in- formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter's unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal-- the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately extracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and classifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14\% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our system can be viewed at http://statuscalendar.com; Our NLP tools are available at http://github.com/aritter/ twitter_nlp.},
	numpages     = {9},
	keywords     = {social media, information extraction}
}
@article{robbins1951stochastic,
	title        = {A stochastic approximation method},
	author       = {Robbins, Herbert and Monro, Sutton},
	year         = {1951},
	journal      = {The annals of mathematical statistics},
	publisher    = {JSTOR},
	pages        = {400--407}
}
@article{roberts2014structural,
	title        = {Structural Topic Models for Open-Ended Survey Responses},
	author       = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	year         = {2014},
	journal      = {American Journal of Political Science},
	volume       = {58},
	number       = {4},
	pages        = {1064--1082},
	doi          = {https://doi.org/10.1111/ajps.12103},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12103},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12103},
	abstract     = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.}
}
@article{roberts2016model,
	title        = {A Model of Text for Experimentation in the Social Sciences},
	author       = {Margaret E. Roberts and Brandon M. Stewart and Edoardo M. Airoldi},
	year         = {2016},
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = {111},
	number       = {515},
	pages        = {988--1003},
	doi          = {10.1080/01621459.2016.1141684},
	url          = {https://doi.org/10.1080/01621459.2016.1141684},
	eprint       = {https://doi.org/10.1080/01621459.2016.1141684}
}
@article{robertson1976relevance,
	title        = {Relevance weighting of search terms},
	author       = {Robertson, Stephen E and Jones, K Sparck},
	year         = {1976},
	journal      = {Journal of the American Society for Information science},
	publisher    = {Wiley Online Library},
	volume       = {27},
	number       = {3},
	pages        = {129--146}
}
@inproceedings{robey2022probabilistically,
	title        = {Probabilistically Robust Learning: Balancing Average and Worst-case Performance},
	author       = {Robey, Alexander and Chamon, Luiz and Pappas, George J. and Hassani, Hamed},
	year         = {2022},
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {162},
	pages        = {18667--18686},
	url          = {https://proceedings.mlr.press/v162/robey22a.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/robey22a/robey22a.pdf},
	abstract     = {Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness. Our code is available at: https://github.com/arobey1/advbench.}
}
@article{rodriguez2003determining,
	title        = {Determining semantic similarity among entity classes from different ontologies},
	author       = {Rodriguez, M.A. and Egenhofer, M.J.},
	year         = {2003},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {15},
	number       = {2},
	pages        = {442--456},
	doi          = {10.1109/TKDE.2003.1185844}
}
@article{rosvall2008maps,
	title        = {Maps of random walks on complex networks reveal community structure},
	author       = {Martin Rosvall  and Carl T. Bergstrom},
	year         = {2008},
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = {105},
	number       = {4},
	pages        = {1118--1123},
	doi          = {10.1073/pnas.0706851105},
	url          = {https://www.pnas.org/doi/abs/10.1073/pnas.0706851105},
	eprint       = {https://www.pnas.org/doi/pdf/10.1073/pnas.0706851105},
	abstract     = {To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of \&gt;6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network—including physics, chemistry, molecular biology, and medicine—information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences.}
}
@book{rudin1964principles,
	title        = {Principles of Mathematical Analysis},
	author       = {Rudin, Walter},
	year         = {1976},
	publisher    = {McGraw-Hill},
	series       = {International series in pure and applied mathematics},
	isbn         = {9780070856134},
	url          = {https://books.google.com.sg/books?id=kwqzPAAACAAJ},
	lccn         = {75179033}
}
@incollection{rummel1976understanding,
	title        = {The Vector Approach},
	author       = {Rummel, Rudolph J},
	year         = {1976},
	journal      = {Honolulu: Department of Political Science, University of Hawaii},
	booktitle    = {Understanding correlation},
	url          = {http://www.hawaii.edu/powerkills/UC.HTM#C5},
	chapter      = {5}
}
@inproceedings{ruoss2020learning,
	title        = {Learning Certified Individually Fair Representations},
	author       = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
	year         = {2020},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {33},
	pages        = {7584--7596},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{sai2019re,
	title        = {Re-Evaluating ADEM: A Deeper Look at Scoring Dialogue Responses},
	author       = {Sai, Ananya B. and Gupta, Mithun Das and Khapra, Mitesh M. and Srinivasan, Mukundhan},
	year         = {2019},
	month        = {Jul.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {33},
	number       = {01},
	pages        = {6220--6227},
	doi          = {10.1609/aaai.v33i01.33016220},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4581},
	abstractnote = {\&lt;p\&gt;Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. ADEM (Lowe et al. 2017) formulated the automatic evaluation of dialogue systems as a learning problem and showed that such a model was able to predict responses which correlate significantly with human judgements, both at utterance and system level. Their system was shown to have beaten word-overlap metrics such as BLEU with large margins. We start with the question of whether an adversary can game the ADEM model. We design a battery of targeted attacks at the neural network based ADEM evaluation system and show that automatic evaluation of dialogue systems still has a long way to go. ADEM can get confused with a variation as simple as reversing the word order in the text! We report experiments on several such adversarial scenarios that draw out counterintuitive scores on the dialogue responses. We take a systematic look at the scoring function proposed by ADEM and connect it to linear system theory to predict the shortcomings evident in the system. We also devise an attack that can fool such a system to rate a response generation system as favorable. Finally, we allude to future research directions of using the adversarial attacks to design a truly automated dialogue evaluation system.\&lt;/p\&gt;}
}
@article{sai2022survey,
	title        = {A Survey of Evaluation Metrics Used for NLG Systems},
	author       = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
	year         = {2022},
	month        = {jan},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {55},
	number       = {2},
	doi          = {10.1145/3485766},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3485766},
	issue_date   = {March 2023},
	abstract     = {In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.},
	articleno    = {26},
	numpages     = {39},
	keywords     = {correlations, question generation, image captioning, data-to-text generation, Automatic evaluation metrics, question answering, abstractive summarization}
}
@article{salazar2023proxy,
	title        = {A proxy learning curve for the Bayes classifier},
	author       = {Addisson Salazar and Luis Vergara and Enrique Vidal},
	year         = {2023},
	journal      = {Pattern Recognition},
	volume       = {136},
	pages        = {109240},
	doi          = {https://doi.org/10.1016/j.patcog.2022.109240},
	issn         = {0031-3203},
	url          = {https://www.sciencedirect.com/science/article/pii/S0031320322007191},
	keywords     = {Classification, Parameter learning, Sample size, Training set size, Probability of error},
	abstract     = {In this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.}
}
@inproceedings{salman2019convex,
	title        = {A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks},
	author       = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
	year         = {2019},
	journal      = {Advances in Neural Information Processing Systems},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {32},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@article{salman2019provably,
	title        = {Provably robust deep learning via adversarially trained smoothed classifiers},
	author       = {Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
	year         = {2019},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {32}
}
@article{salton1965smart,
	title        = {The SMART automatic document retrieval systems—an illustration},
	author       = {Salton, Gerard and Lesk, Michael E},
	year         = {1965},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = {8},
	number       = {6},
	pages        = {391--398}
}
@article{salton1983modern,
	title        = {Modern information retrieval},
	author       = {Salton, Gerard},
	year         = {1983},
	journal      = {(No Title)},
	publisher    = {mcgraw-hill}
}
@article{salton1988term,
	title        = {Term-weighting approaches in automatic text retrieval},
	author       = {Salton, Gerard and Buckley, Christopher},
	year         = {1988},
	journal      = {Information processing \& management},
	publisher    = {Elsevier},
	volume       = {24},
	number       = {5},
	pages        = {513--523}
}
@article{sanchez2011ontology,
	title        = {Ontology-based information content computation},
	author       = {David Sánchez and Montserrat Batet and David Isern},
	year         = {2011},
	journal      = {Knowledge-Based Systems},
	volume       = {24},
	number       = {2},
	pages        = {297--303},
	doi          = {https://doi.org/10.1016/j.knosys.2010.10.001},
	issn         = {0950-7051},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950705110001619},
	keywords     = {Information content, Semantic similarity, Ontologies, Taxonomic knowledge, WordNet},
	abstract     = {The information content (IC) of a concept provides an estimation of its degree of generality/concreteness, a dimension which enables a better understanding of concept's semantics. As a result, IC has been successfully applied to the automatic assessment of the semantic similarity between concepts. In the past, IC has been estimated as the probability of appearance of concepts in corpora. However, the applicability and scalability of this method are hampered due to corpora dependency and data sparseness. More recently, some authors proposed IC-based measures using taxonomical features extracted from an ontology for a particular concept, obtaining promising results. In this paper, we analyse these ontology-based approaches for IC computation and propose several improvements aimed to better capture the semantic evidence modelled in the ontology for the particular concept. Our approach has been evaluated and compared with related works (both corpora and ontology-based ones) when applied to the task of semantic similarity estimation. Results obtained for a widely used benchmark show that our method enables similarity estimations which are better correlated with human judgements than related works.}
}
@article{sanchez2012ontology,
	title        = {Ontology-based semantic similarity: A new feature-based approach},
	author       = {David Sánchez and Montserrat Batet and David Isern and Aida Valls},
	year         = {2012},
	journal      = {Expert Systems with Applications},
	volume       = {39},
	number       = {9},
	pages        = {7718--7728},
	doi          = {https://doi.org/10.1016/j.eswa.2012.01.082},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417412000954},
	keywords     = {Semantic similarity, Semantic relatedness, Ontologies, Feature-based similarity, WordNet},
	abstract     = {Estimation of the semantic likeness between words is of great importance in many applications dealing with textual data such as natural language processing, knowledge acquisition and information retrieval. Semantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent years, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an structured knowledge representation. Thanks to the possibilities that ontologies enable regarding semantic interpretation of terms many ontology-based similarity measures have been developed. According to the principle in which those measures base the similarity assessment and the way in which ontologies are exploited or complemented with other sources several families of measures can be identified. In this paper, we survey and classify most of the ontology-based approaches developed in order to evaluate their advantages and limitations and compare their expected performance both from theoretical and practical points of view. We also present a new ontology-based measure relying on the exploitation of taxonomical features. The evaluation and comparison of our approach's results against those reported by related works under a common framework suggest that our measure provides a high accuracy without some of the limitations observed in other works.}
}
@article{sanchez2013semantic,
	title        = {A semantic similarity method based on information content exploiting multiple ontologies},
	author       = {David Sánchez and Montserrat Batet},
	year         = {2013},
	journal      = {Expert Systems with Applications},
	volume       = {40},
	number       = {4},
	pages        = {1393--1399},
	doi          = {https://doi.org/10.1016/j.eswa.2012.08.049},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S095741741201010X},
	keywords     = {Information content, Semantic similarity, Ontologies, MeSH, SNOMED CT},
	abstract     = {The quantification of the semantic similarity between terms is an important research area that configures a valuable tool for text understanding. Among the different paradigms used by related works to compute semantic similarity, in recent years, information theoretic approaches have shown promising results by computing the information content (IC) of concepts from the knowledge provided by ontologies. These approaches, however, are hampered by the coverage offered by the single input ontology. In this paper, we propose extending IC-based similarity measures by considering multiple ontologies in an integrated way. Several strategies are proposed according to which ontology the evaluated terms belong. Our proposal has been evaluated by means of a widely used benchmark of medical terms and MeSH and SNOMED CT as ontologies. Results show an improvement in the similarity assessment accuracy when multiple ontologies are considered.}
}
@misc{sandhaus2008new,
	title        = {The New York Times annotated corpus},
	author       = {Sandhaus, Evan},
	year         = {2008},
	journal      = {Linguistic Data Consortium},
	publisher    = {LDC2008T19.  Web Download. Philadelphia: Linguistic Data Consortium},
	url          = {https://catalog.ldc.upenn.edu/LDC2008T19}
}
@article{sanh2019distilbert,
	title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper and lighter},
	author       = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1910.01108},
	url          = {http://arxiv.org/abs/1910.01108},
	eprinttype   = {arXiv},
	eprint       = {1910.01108},
	timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sanyal2020benign,
	title        = {How Benign is Benign Overfitting ?},
	author       = {Amartya Sanyal and Puneet K. Dokania and Varun Kanade and Philip H. S. Torr},
	year         = {2021},
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=g-wu9TMPODo},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/SanyalDKT21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{sarridis2024flac,
	title        = {FLAC: Fairness-Aware Representation Learning by Suppressing Attribute-Class Associations},
	author       = {Sarridis, Ioannis and Koutlis, Christos and Papadopoulos, Symeon and Diou, Christos},
	year         = {2025},
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = {47},
	number       = {2},
	pages        = {1148--1160},
	doi          = {10.1109/TPAMI.2024.3487254},
	keywords     = {Prevention and mitigation;Artificial intelligence;Training;Data models;Mutual information;Accuracy;Visualization;Representation learning;Predictive models;Minimization;Bias mitigation;fairness;mutual information}
}
@inproceedings{sarwar2001item,
	title        = {Item-based collaborative filtering recommendation algorithms},
	author       = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
	year         = {2001},
	booktitle    = {Proceedings of the 10th international conference on World Wide Web},
	pages        = {285--295}
}
@inproceedings{sarwar2002incremental,
	title        = {Incremental singular value decomposition algorithms for highly scalable recommender systems},
	author       = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
	year         = {2002},
	booktitle    = {Fifth international conference on computer and information science},
	volume       = {1},
	number       = {012002},
	pages        = {27--8},
	organization = {Citeseer}
}
@article{sawant2019age,
	title        = {Age invariant face recognition: a survey on facial aging databases, techniques and effect of aging},
	author       = {Sawant, Manisha M. and Bhurchandi, Kishor M.},
	year         = {2019},
	month        = {Aug},
	day          = {01},
	journal      = {Artificial Intelligence Review},
	volume       = {52},
	number       = {2},
	pages        = {981--1008},
	doi          = {10.1007/s10462-018-9661-z},
	issn         = {1573-7462},
	url          = {https://doi.org/10.1007/s10462-018-9661-z},
	abstract     = {Age invariant face recognition (AIFR) is highly required in many applications like law enforcement, national databases and security. Recognizing faces across aging is difficult even for humans; hence, it presents a unique challenge for computer vision systems. Face recognition under various intra-person variations such as expression, pose and occlusion has been an intensively researched field. However, age invariant face recognition still faces many challenges due to age related biological transformations in presence of the other appearance variations. In this paper, we present a comprehensive review of literature on cross age face recognition. Starting with the biological effects of aging, this paper presents a survey of techniques, effects of aging on performance analysis and facial aging databases. The published AIFR techniques are reviewed and categorized into generative, discriminative and deep learning methods on the basis of face representation and learning techniques. Analysis of the effect of aging on the performance of age-invariant face recognition system is an important dimension. Hence, such analysis is reviewed and summarized. In addition, important facial aging databases are briefly described in terms of the number of subjects and images per subject along with their age ranges. We finally present discussions on the findings, conclusions and future directions for new researchers.}
}
@article{schapire1990strength,
	title        = {The strength of weak learnability},
	author       = {Schapire, Robert E.},
	year         = {1990},
	month        = {Jun},
	day          = {01},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = {5},
	number       = {2},
	pages        = {197--227},
	doi          = {10.1007/BF00116037},
	issn         = {1573-0565},
	url          = {https://doi.org/10.1007/BF00116037},
	abstract     = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.}
}
@article{schmidt1998validity,
	title        = {The validity and utility of selection methods in personnel psychology: Practical and theoretical implications of 85 years of research findings.},
	author       = {Schmidt, Frank L and Hunter, John E},
	year         = {1998},
	journal      = {Psychological bulletin},
	publisher    = {American Psychological Association},
	volume       = {124},
	number       = {2},
	pages        = {262}
}
@inproceedings{schroff2015facenet,
	title        = {FaceNet: A unified embedding for face recognition and clustering},
	author       = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year         = {2015},
	booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {815--823},
	doi          = {10.1109/CVPR.2015.7298682}
}
@article{schwinn2023exploring,
	title        = {Exploring misclassifications of robust neural networks to enhance adversarial attacks},
	author       = {Schwinn, Leo and Raab, Ren{\'e} and Nguyen, An and Zanca, Dario and Eskofier, Bjoern},
	year         = {2023},
	month        = {Mar},
	day          = {21},
	journal      = {Applied Intelligence},
	doi          = {10.1007/s10489-023-04532-5},
	issn         = {1573-7497},
	url          = {https://doi.org/10.1007/s10489-023-04532-5},
	abstract     = {Progress in making neural networks more robust against adversarial attacks is mostly marginal, despite the great efforts of the research community. Moreover, the robustness evaluation is often imprecise, making it challenging to identify promising approaches. We do an observational study on the classification decisions of 19 different state-of-the-art neural networks trained to be robust against adversarial attacks. This analysis gives a new indication of the limits of the robustness of current models on a common benchmark. In addition, our findings suggest that current untargeted adversarial attacks induce misclassification toward only a limited amount of different classes. Similarly, we find that previous attacks under-explore the perturbation space during optimization. This leads to unsuccessful attacks for samples where the initial gradient direction is not a good approximation of the final adversarial perturbation direction. Additionally, we observe that both over- and under-confidence in model predictions result in an inaccurate assessment of model robustness. Based on these observations, we propose a novel loss function for adversarial attacks that consistently improves their efficiency and success rate compared to prior attacks for all 30 analyzed models.}
}
@article{scikit-learn,
	title        = {Scikit-learn: Machine Learning in {P}ython},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = {2011},
	journal      = {Journal of Machine Learning Research},
	volume       = {12},
	pages        = {2825--2830}
}
@article{scott1998dewey,
	title        = {Dewey decimal classification},
	author       = {Scott, Mona L and SCOTT, MONA L},
	year         = {1998},
	journal      = {Libraries Unlimited}
}
@inproceedings{seddik2022neural,
	title        = {Neural networks classify through the class-wise means of their representations},
	author       = {Seddik, Mohamed El Amine and Tamaazousti, Mohamed},
	year         = {2022},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {36},
	number       = {8},
	pages        = {8204--8211}
}
@inproceedings{sehwag2019analyzing,
	title        = {Analyzing the Robustness of Open-World Machine Learning},
	author       = {Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek},
	year         = {2019},
	booktitle    = {Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
	location     = {London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {AISec'19},
	pages        = {105--116},
	doi          = {10.1145/3338501.3357372},
	isbn         = {9781450368339},
	url          = {https://doi.org/10.1145/3338501.3357372},
	abstract     = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing \o{}odAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that \o{}odAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
	numpages     = {12},
	keywords     = {adversarial example, open world recognition, deep learning}
}
@article{sekeh2020learning,
	title        = {Learning to Bound the Multi-Class Bayes Error},
	author       = {Sekeh, Salimeh Yasaei and Oselio, Brandon and Hero, Alfred O.},
	year         = {2020},
	journal      = {IEEE Transactions on Signal Processing},
	volume       = {68},
	number       = {},
	pages        = {3793--3807},
	doi          = {10.1109/TSP.2020.2994807}
}
@inproceedings{sengupta2016frontal,
	title        = {Frontal to profile face verification in the wild},
	author       = {Sengupta, Soumyadip and Chen, Jun-Cheng and Castillo, Carlos and Patel, Vishal M. and Chellappa, Rama and Jacobs, David W.},
	year         = {2016},
	booktitle    = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	volume       = {},
	number       = {},
	pages        = {1--9},
	doi          = {10.1109/WACV.2016.7477558}
}
@incollection{sep-natural-deduction,
	title        = {{Natural Deduction Systems in Logic}},
	author       = {Pelletier, Francis Jeffry and Hazen, Allen},
	year         = {2021},
	booktitle    = {The {Stanford} Encyclopedia of Philosophy},
	publisher    = {Metaphysics Research Lab, Stanford University},
	editor       = {Edward N. Zalta},
	howpublished = {\url{https://plato.stanford.edu/archives/win2021/entries/natural-deduction/}},
	edition      = {{W}inter 2021}
}
@inproceedings{severyn2012structural,
	title        = {Structural Relationships for Large-Scale Learning of Answer Re-Ranking},
	author       = {Severyn, Aliaksei and Moschitti, Alessandro},
	year         = {2012},
	booktitle    = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	location     = {Portland, Oregon, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {SIGIR '12},
	pages        = {741--750},
	doi          = {10.1145/2348283.2348383},
	isbn         = {9781450314725},
	url          = {https://doi.org/10.1145/2348283.2348383},
	abstract     = {Supervised learning applied to answer re-ranking can highly improve on the overall accuracy of question answering (QA) systems. The key aspect is that the relationships and properties of the question/answer pair composed of a question and the supporting passage of an answer candidate, can be efficiently compared with those captured by the learnt model.In this paper, we define novel supervised approaches that exploit structural relationships between a question and their candidate answer passages to learn a re-ranking model. We model structural representations of both questions and answers and their mutual relationships by just using an off-the-shelf shallow syntactic parser. We encode structures in Support Vector Machines (SVMs) by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces. Such models together with the latest approach to fast kernel-based learning enabled the training of our rerankers on hundreds of thousands of instances, which previously rendered intractable for kernelized SVMs. The results on two different QA datasets, e.g., Answerbag and Jeopardy! data, show that our models deliver large improvement on passage re-ranking tasks, reducing the error in Recall of BM25 baseline by about 18\%. One of the key findings of this work is that, despite its simplicity, shallow syntactic trees allow for learning complex relational structures, which exhibits a steep learning curve with the increase in the training size.},
	numpages     = {10},
	keywords     = {kernel methods, large-scale learning, support vector machines, question answering, structural kernels}
}
@inproceedings{shahmirzadi2019text,
	title        = {Text Similarity in Vector Space Models: A Comparative Study},
	author       = {Shahmirzadi, Omid and Lugowski, Adam and Younge, Kenneth},
	year         = {2019},
	booktitle    = {2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)},
	pages        = {659--666},
	doi          = {10.1109/ICMLA.2019.00120}
}
@article{shannon1948mathematical,
	title        = {A mathematical theory of communication},
	author       = {Shannon, Claude E},
	year         = {1948},
	journal      = {The Bell system technical journal},
	publisher    = {Nokia Bell Labs},
	volume       = {27},
	number       = {3},
	pages        = {379--423}
}
@article{shannon1951prediction,
	title        = {Prediction and entropy of printed English},
	author       = {Shannon, Claude E},
	year         = {1951},
	journal      = {Bell system technical journal},
	publisher    = {Wiley Online Library},
	volume       = {30},
	number       = {1},
	pages        = {50--64}
}
@inproceedings{sharif2016accessorize,
	title        = {Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition},
	author       = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
	year         = {2016},
	booktitle    = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	location     = {Vienna, Austria},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CCS '16},
	pages        = {1528--1540},
	doi          = {10.1145/2976749.2978392},
	isbn         = {9781450341394},
	url          = {https://doi.org/10.1145/2976749.2978392},
	abstract     = {Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk.In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.},
	numpages     = {13},
	keywords     = {face recognition, face detection, adversarial machine learning, neural networks}
}
@inproceedings{sharif2018nneval,
	title        = {NNEval: Neural Network based Evaluation Metric for Image Captioning},
	author       = {Sharif, Naeha and White, Lyndon and Bennamoun, Mohammed and Shah, Syed Afaq Ali},
	year         = {2018},
	month        = {September},
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)}
}
@book{shawe2004kernel,
	title        = {Kernel Methods for Pattern Analysis},
	author       = {John Shawe{-}Taylor and Nello Cristianini},
	year         = {2004},
	publisher    = {Cambridge University Press},
	isbn         = {978-0-521-81397-6},
	url          = {http://www.cambridge.org/gb/knowledge/isbn/item1169757/Kernel\%20Methods\%20for\%20Pattern\%20Analysis/?site\_locale=en\_GB},
	timestamp    = {Thu, 05 May 2011 16:48:09 +0200},
	biburl       = {https://dblp.org/rec/books/daglib/0026002.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{shen2014automatic,
	title        = {Automatic Fake Followers Detection in Chinese Micro-blogging System},
	author       = {Shen, Yi and Yu, Jianjun and Dong, Kejun and Nan, Kai},
	year         = {2014},
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {596--607},
	isbn         = {978-3-319-06605-9},
	editor       = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	abstract     = {Micro-blogging, which has greatly influenced people's life, is experiencing fantastic success in the worldwide. However, during its rapid development, it has encountered the problem of content pollution. Various pollution in the micro-blogging platforms has hurt the credibility of micro-blogging and caused significantly negative effect. In this paper, we mainly focus on detecting fake followers which may lead to a problematic situation on social media networks. By extracting major features of fake followers in Sina Weibo, we propose a binary classifier to distinguish fake followers from the legitimate users. The experiments show that all the proposed features are important and our method greatly outperforms to detect fake followers. We also present an elaborate analysis on the phenomenon of fake followers, infer the supported algorithms and principles behind them, and finally provide several suggestions for micro-blogging systems and ordinary users to deal with the fake followers.}
}
@inproceedings{shi2021fast,
	title        = {Fast Certified Robust Training with Short Warmup},
	author       = {Shi, Zhouxing and Wang, Yihan and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
	year         = {2021},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {34},
	pages        = {18335--18349},
	url          = {https://proceedings.neurips.cc/paper/2021/file/988f9153ac4fd966ea302dd9ab9bae15-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{shi2021neural,
	title        = {Neural Abstractive Text Summarization with Sequence-to-Sequence Models},
	author       = {Shi, Tian and Keneshloo, Yaser and Ramakrishnan, Naren and Reddy, Chandan K.},
	year         = {2021},
	month        = {jan},
	journal      = {ACM/IMS Trans. Data Sci.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {2},
	number       = {1},
	doi          = {10.1145/3419106},
	issn         = {2691-1922},
	url          = {https://doi.org/10.1145/3419106},
	issue_date   = {February 2021},
	abstract     = {In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efficiency and parallelism for training a model. In this article, we provide a comprehensive literature survey on different seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were first proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in NATS on the two recently released datasets, namely, Newsroom and Bytecup.},
	articleno    = {1},
	numpages     = {37},
	keywords     = {beam search, sequence-to-sequence models, pointer-generator network, attention model, deep reinforcement learning, Abstractive text summarization}
}
@article{shimanaka2019machine,
	title        = {Machine Translation Evaluation with {BERT} Regressor},
	author       = {Hiroki Shimanaka and Tomoyuki Kajiwara and Mamoru Komachi},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1907.12679},
	url          = {http://arxiv.org/abs/1907.12679},
	eprinttype   = {arXiv},
	eprint       = {1907.12679},
	timestamp    = {Fri, 02 Aug 2019 09:43:42 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1907-12679.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{shin2018privacy,
	title        = {Privacy enhanced matrix factorization for recommendation with local differential privacy},
	author       = {Shin, Hyejin and Kim, Sungwook and Shin, Junbum and Xiao, Xiaokui},
	year         = {2018},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE},
	volume       = {30},
	number       = {9},
	pages        = {1770--1782}
}
@article{shirong2023binary,
	title        = {Binary Classification under Local Label Differential Privacy Using Randomized Response Mechanisms},
	author       = {Shirong XU and Chendi Wang and Will Wei Sun and Guang Cheng},
	year         = {2023},
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=uKCGOw9bGG},
	note         = {}
}
@article{shorten2019survey,
	title        = {A survey on Image Data Augmentation for Deep Learning},
	author       = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year         = {2019},
	month        = {Jul},
	day          = {06},
	journal      = {Journal of Big Data},
	volume       = {6},
	number       = {1},
	pages        = {60},
	doi          = {10.1186/s40537-019-0197-0},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-019-0197-0},
	abstract     = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.}
}
@article{shu2017fake,
	title        = {Fake News Detection on Social Media: A Data Mining Perspective},
	author       = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
	year         = {2017},
	month        = {sep},
	journal      = {SIGKDD Explor. Newsl.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {19},
	number       = {1},
	pages        = {22--36},
	doi          = {10.1145/3137597.3137600},
	issn         = {1931-0145},
	url          = {https://doi.org/10.1145/3137597.3137600},
	issue_date   = {June 2017},
	abstract     = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
	numpages     = {15}
}
@article{siddharthan2006syntactic,
	title        = {Syntactic simplification and text cohesion},
	author       = {Siddharthan, Advaith},
	year         = {2006},
	journal      = {Research on Language and Computation},
	publisher    = {Springer},
	volume       = {4},
	number       = {1},
	pages        = {77--109}
}
@inproceedings{silva2018duplicate,
	title        = {Duplicate question detection in stack overflow: A reproducibility study},
	author       = {Silva, Rodrigo F. G. and Paixão, Klérisson and de Almeida Maia, Marcelo},
	year         = {2018},
	booktitle    = {2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
	volume       = {},
	number       = {},
	pages        = {572--581},
	doi          = {10.1109/SANER.2018.8330262}
}
@article{silva2020opportunities,
	title        = {Opportunities and Challenges in Deep Learning Adversarial Robustness: {A} Survey},
	author       = {Samuel Henrique Silva and Peyman Najafirad},
	year         = {2020},
	journal      = {CoRR},
	volume       = {abs/2007.00753},
	url          = {https://arxiv.org/abs/2007.00753},
	eprinttype   = {arXiv},
	eprint       = {2007.00753},
	timestamp    = {Mon, 06 Jul 2020 15:26:01 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2007-00753.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{simon2019sciences,
	title        = {The Sciences of the Artificial, reissue of the third edition with a new introduction by John Laird},
	author       = {Simon, Herbert Alexander},
	year         = {2019},
	publisher    = {MIT press}
}
@misc{simonoff_2010,
	title        = {Statistics and Data Analysis (COR1-GB.1305)},
	author       = {Simonoff, Jeffrey},
	year         = {2010}
}
@article{singh2019abstract,
	title        = {An Abstract Domain for Certifying Neural Networks},
	author       = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
	year         = {2019},
	month        = {jan},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {3},
	number       = {POPL},
	doi          = {10.1145/3290354},
	url          = {https://doi.org/10.1145/3290354},
	issue_date   = {January 2019},
	abstract     = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
	articleno    = {41},
	numpages     = {30},
	keywords     = {Adversarial attacks, Abstract Interpretation, Deep Learning}
}
@article{singh2019beyond,
	title        = {Beyond the single neuron convex barrier for neural network certification},
	author       = {Singh, Gagandeep and Ganvir, Rupanshu and P{\"u}schel, Markus and Vechev, Martin},
	year         = {2019},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {32}
}
@inproceedings{singh2021brightness,
	title        = {On Brightness Agnostic Adversarial Examples Against Face Recognition Systems},
	author       = {Singh, Inderjeet and Momiyama, Satoru and Kakizaki, Kazuya and Araki, Toshinori},
	year         = {2021},
	booktitle    = {2021 International Conference of the Biometrics Special Interest Group (BIOSIG)},
	volume       = {},
	number       = {},
	pages        = {1--5},
	doi          = {10.1109/BIOSIG52210.2021.9548291}
}
@inproceedings{singhal2023photozilla,
	title        = {Photozilla: An Image Dataset of Photography Styles and its Application to Visual Embedding and Style Detection},
	author       = {Singhal, Trisha and Liu, Junhua and Mu, Wenchuan and Blessing, Lucienne T. M. and Lim, Kwan Hui},
	year         = {2024},
	booktitle    = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
	location     = {Kusadasi, Turkiye},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASONAM '23},
	pages        = {445–449},
	doi          = {10.1145/3625007.3627476},
	isbn         = {9798400704093},
	url          = {https://doi.org/10.1145/3625007.3627476},
	abstract     = {The widespread sharing of digital photography and images have led to the rapid development of various vision-related applications, such as photography style detection. Towards this effort, we introduce a photography style dataset termed Photozilla, which comprises over 990k images belonging to 10 different photographic styles. We used Photozilla to train 3 classification models for categorizing images into the relevant style and achieve an accuracy of ~96\%. To better detect new photography styles that are constantly emerging, we also present a Siamese-based network that uses the trained classification models as the base architecture to adapt and classify unseen styles with only 25 training samples. Experiment results show an accuracy of over 68\% in terms of identifying 10 additional distinct categories of photography styles. This dataset can be found at https://trisha025.github.io/Photozilla/.},
	numpages     = {5},
	keywords     = {image recognition, visual embedding, image classification, neural networks}
}
@article{sinoara2019knowledge,
	title        = {Knowledge-enhanced document embeddings for text classification},
	author       = {Roberta A. Sinoara and Jose Camacho-Collados and Rafael G. Rossi and Roberto Navigli and Solange O. Rezende},
	year         = {2019},
	journal      = {Knowledge-Based Systems},
	volume       = {163},
	pages        = {955--971},
	doi          = {https://doi.org/10.1016/j.knosys.2018.10.026},
	issn         = {0950-7051},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950705118305124},
	keywords     = {Semantic representation, Document embeddings, Text classification, Text mining},
	abstract     = {Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.}
}
@article{smith1981citation,
	title        = {Citation analysis},
	author       = {Smith, Linda C},
	year         = {1981},
	publisher    = {Graduate School of Library and Information Science. University of Illinois~…}
}
@misc{smith2007robustness,
	title        = {Robustness},
	author       = {Smith, Martha K.},
	year         = {2007},
	url          = {https://web.ma.utexas.edu/users/mks/384G07/robustness.pdf},
	note         = {Department of Mathematics, University of Texas at Austin, 1 University Station C1200, Austin, TX 78712},
	howpublished = {Lecture Notes in M374G/M384G/CAM384T: Regression Analysis}
}
@inproceedings{smith2019super,
	title        = {{Super-convergence: very fast training of neural networks using large learning rates}},
	author       = {Leslie N. Smith and Nicholay Topin},
	year         = {2019},
	booktitle    = {Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications},
	publisher    = {SPIE},
	volume       = {11006},
	pages        = {1100612},
	doi          = {10.1117/12.2520589},
	url          = {https://doi.org/10.1117/12.2520589},
	editor       = {Tien Pham},
	organization = {International Society for Optics and Photonics},
	keywords     = {Deep learning, machine learning, neural networks, learning rates, large batch training}
}
@inproceedings{sohn2016improved,
	title        = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	author       = {Sohn, Kihyuk},
	year         = {2016},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {29},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett}
}
@inproceedings{song2020mpnet,
	title        = {MPNet: masked and permuted pre-training for language understanding},
	author       = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	year         = {2020},
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS '20},
	isbn         = {9781713829546},
	abstract     = {BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting.},
	articleno    = {1414},
	numpages     = {11}
}
@article{song2021classification,
	title        = {Classification aware neural topic model for COVID-19 disinformation categorisation},
	author       = {Song, Xingyi and Petrak, Johann and Jiang, Ye and Singh, Iknoor and Maynard, Diana and Bontcheva, Kalina},
	year         = {2021},
	journal      = {PloS one},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = {16},
	number       = {2},
	pages        = {e0247086}
}
@article{souganciouglu2017biosses,
	title        = {{BIOSSES: a semantic sentence similarity estimation system for the biomedical domain}},
	author       = {Soğancıoğlu, Gizem and Öztürk, Hakime and Özgür, Arzucan},
	year         = {2017},
	month        = {07},
	journal      = {Bioinformatics},
	volume       = {33},
	number       = {14},
	pages        = {i49-i58},
	doi          = {10.1093/bioinformatics/btx238},
	issn         = {1367-4803},
	url          = {https://doi.org/10.1093/bioinformatics/btx238},
	abstract     = {{The amount of information available in textual format is rapidly increasing in the biomedical domain. Therefore, natural language processing (NLP) applications are becoming increasingly important to facilitate the retrieval and analysis of these data. Computing the semantic similarity between sentences is an important component in many NLP tasks including text retrieval and summarization. A number of approaches have been proposed for semantic sentence similarity estimation for generic English. However, our experiments showed that such approaches do not effectively cover biomedical knowledge and produce poor results for biomedical text.We propose several approaches for sentence-level semantic similarity computation in the biomedical domain, including string similarity measures and measures based on the distributed vector representations of sentences learned in an unsupervised manner from a large biomedical corpus. In addition, ontology-based approaches are presented that utilize general and domain-specific ontologies. Finally, a supervised regression based model is developed that effectively combines the different similarity computation metrics. A benchmark data set consisting of 100 sentence pairs from the biomedical literature is manually annotated by five human experts and used for evaluating the proposed methods.The experiments showed that the supervised semantic sentence similarity computation approach obtained the best performance (0.836 correlation with gold standard human annotations) and improved over the state-of-the-art domain-independent systems up to 42.6\% in terms of the Pearson correlation metric.A web-based system for biomedical semantic sentence similarity computation, the source code, and the annotated benchmark data set are available at: http://tabilab.cmpe.boun.edu.tr/BIOSSES/.}},
	eprint       = {https://academic.oup.com/bioinformatics/article-pdf/33/14/i49/25157316/btx238.pdf}
}
@software{spacy,
	title        = {{spaCy: Industrial-strength Natural Language Processing in Python}},
	author       = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	year         = {2020},
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.1212303},
	url          = {https://doi.org/10.5281/zenodo.1212303}
}
@article{spruill2007asymptotic,
	title        = {Asymptotic Distribution of Coordinates  on High Dimensional Spheres},
	author       = {Marcus Spruill},
	year         = {2007},
	journal      = {Electron. Commun. Probab.},
	volume       = {12},
	pages        = {no. 23, 234--247},
	doi          = {10.1214/ECP.v12-1294},
	issn         = {1083-589X},
	url          = {http://ecp.ejpecp.org/article/view/1294},
	fjournal     = {Electronic Communications in Probability},
	keywords     = {empiric distribution; dependent arrays; micro-canonical ensemble;Minkowski area; isoperimetry},
	abstract     = {The coordinates $x_i$ of a point $x = (x_1, x_2, \dots, x_n)$ chosen at     random according to a uniform distribution on the $\ell_2(n)$-sphere of     radius $n^{1/2}$  have approximately a normal distribution when $n$ is large. The coordinates $x_i$   of points uniformly distributed on the $\ell_1(n)$-sphere of radius $n$  have  approximately a double exponential distribution.  In these and all   the $\ell_p(n),1 \le p \le \infty,$ convergence of the distribution of coordinates   as the dimension $n$ increases is at the rate $\sqrt{n}$ and is described   precisely in terms of weak convergence of a normalized empirical process to   a limiting Gaussian process, the sum of a Brownian bridge and a simple normal process.}
}
@misc{statlog_(german_credit_data)_144,
	title        = {{Statlog (German Credit Data)}},
	author       = {Hofmann, Hans},
	year         = {1994},
	note         = {{DOI}: https://doi.org/10.24432/C5NC77},
	howpublished = {UCI Machine Learning Repository}
}
@misc{stats_twitter,
	title        = {Twitter Usage Statistics},
	title        = {Twitter usage statistics},
	author       = {Internet Live Statistics},
	year         = {2016},
	journal      = {Twitter Usage Statistics - Internet Live Stats},
	url          = {http://www.internetlivestats.com/twitter-statistics/},
	note         = {http://www.internetlivestats.com/twitter-statistics/},
	howpublished = {Internet}
}
@inproceedings{stent2005evaluating,
	title        = {Evaluating Evaluation Methods for Generation in the Presence of Variation},
	author       = {Stent, Amanda and Marge, Matthew and Singhai, Mohit},
	year         = {2005},
	booktitle    = {Computational Linguistics and Intelligent Text Processing},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {341--351},
	isbn         = {978-3-540-30586-6},
	editor       = {Gelbukh, Alexander},
	abstract     = {Recent years have seen increasing interest in automatic metrics for the evaluation of generation systems. When a system can generate syntactic variation, automatic evaluation becomes more difficult. In this paper, we compare the performance of several automatic evaluation metrics using a corpus of automatically generated paraphrases. We show that these evaluation metrics can at least partially measure adequacy (similarity in meaning), but are not good measures of fluency (syntactic correctness). We make several proposals for improving the evaluation of generation systems that produce variation.}
}
@inproceedings{su1992new,
	title        = {A new quantitative quality measure for machine translation systems},
	author       = {Su, Keh-Yih and Wu, Ming-Wen and Chang, Jing-Shin},
	year         = {1992},
	booktitle    = {COLING 1992 Volume 2: The 14th International Conference on Computational Linguistics}
}
@article{su2019one,
	title        = {One Pixel Attack for Fooling Deep Neural Networks},
	author       = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
	year         = {2019},
	journal      = {IEEE Transactions on Evolutionary Computation},
	volume       = {23},
	number       = {5},
	pages        = {828--841},
	doi          = {10.1109/TEVC.2019.2890858}
}
@article{sun2015deepid3,
	title        = {DeepID3: Face Recognition with Very Deep Neural Networks},
	author       = {Yi Sun and Ding Liang and Xiaogang Wang and Xiaoou Tang},
	year         = {2015},
	journal      = {CoRR},
	volume       = {abs/1502.00873},
	url          = {http://arxiv.org/abs/1502.00873},
	eprinttype   = {arXiv},
	eprint       = {1502.00873},
	timestamp    = {Tue, 10 Dec 2019 15:37:26 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/SunLWT15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sun2019bert4rec,
	title        = {BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer},
	author       = {Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
	year         = {2019},
	booktitle    = {Proceedings of the 28th ACM international conference on information and knowledge management},
	pages        = {1441--1450}
}
@article{sun2020ernie,
	title        = {ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding},
	author       = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	year         = {2020},
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {34},
	number       = {05},
	pages        = {8968--8975},
	doi          = {10.1609/aaai.v34i05.6428},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6428},
	abstractnote = {\&lt;p\&gt;Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.\&lt;/p\&gt;}
}
@inproceedings{sun2022causality,
	title        = {Causality-Based Neural Network Repair},
	author       = {Sun, Bing and Sun, Jun and Pham, Long H. and Shi, Jie},
	year         = {2022},
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {338--349},
	doi          = {10.1145/3510003.3510080},
	isbn         = {9781450392211},
	url          = {https://doi.org/10.1145/3510003.3510080},
	abstract     = {Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the 'guilty' neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91\% on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98\% to less than 1\%. For safety property repair tasks, CARE reduces the property violation rate to less than 1\%. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.},
	numpages     = {12}
}
@inproceedings{sun2023toward,
	title        = {Toward the Tradeoffs Between Privacy, Fairness and Utility in Federated Learning},
	author       = {Sun, Kangkang and Zhang, Xiaojin and Lin, Xi and Li, Gaolei and Wang, Jing and Li, Jianhua},
	year         = {2024},
	booktitle    = {Emerging Information Security and Applications},
	publisher    = {Springer Nature Singapore},
	address      = {Singapore},
	pages        = {118--132},
	isbn         = {978-981-99-9614-8},
	editor       = {Shao, Jun and Katsikas, Sokratis K. and Meng, Weizhi},
	abstract     = {Federated Learning (FL) is a novel privacy-protection distributed machine learning paradigm that guarantees user privacy and prevents the risk of data leakage due to the advantage of the client's local training. Researchers have struggled to design fair FL systems that ensure fairness of results. However, the interplay between fairness and privacy has been less studied. Increasing the fairness of FL systems can have an impact on user privacy, while an increase in user privacy can affect fairness. In this work, on the client side, we use the fairness metrics, such as Demographic Parity (DemP), Equalized Odds (EOs), and Disparate Impact (DI), to construct the local fair model. To protect the privacy of the client model, we propose a privacy-protection fairness FL method. The results show that the accuracy of the fair model with privacy increases because privacy breaks the constraints of the fairness metrics. In our experiments, we conclude the relationship between privacy, fairness and utility, and there is a tradeoff between these.}
}
@article{surian2016characterizing,
	title        = {Characterizing Twitter Discussions About HPV Vaccines Using Topic Modeling and Community Detection},
	author       = {Surian, Didi and Nguyen, Dat Quoc and Kennedy, Georgina and Johnson, Mark and Coiera, Enrico and Dunn, Adam G},
	year         = {2016},
	month        = {Aug},
	day          = {29},
	journal      = {J Med Internet Res},
	volume       = {18},
	number       = {8},
	pages        = {e232},
	doi          = {10.2196/jmir.6045},
	issn         = {1438-8871},
	url          = {http://www.jmir.org/2016/8/e232/},
	url          = {https://doi.org/10.2196/jmir.6045},
	url          = {http://www.ncbi.nlm.nih.gov/pubmed/27573910},
	keywords     = {topic modelling; graph algorithms analysis; social media; public health surveillance},
	abstract     = {Background: In public health surveillance, measuring how information enters and spreads through online communities may help us understand geographical variation in decision making associated with poor health outcomes. Objective: Our aim was to evaluate the use of community structure and topic modeling methods as a process for characterizing the clustering of opinions about human papillomavirus (HPV) vaccines on Twitter. Methods: The study examined Twitter posts (tweets) collected between October 2013 and October 2015 about HPV vaccines. We tested Latent Dirichlet Allocation and Dirichlet Multinomial Mixture (DMM) models for inferring topics associated with tweets, and community agglomeration (Louvain) and the encoding of random walks (Infomap) methods to detect community structure of the users from their social connections. We examined the alignment between community structure and topics using several common clustering alignment measures and introduced a statistical measure of alignment based on the concentration of specific topics within a small number of communities. Visualizations of the topics and the alignment between topics and communities are presented to support the interpretation of the results in context of public health communication and identification of communities at risk of rejecting the safety and efficacy of HPV vaccines. Results: We analyzed 285,417 Twitter posts (tweets) about HPV vaccines from 101,519 users connected by 4,387,524 social connections. Examining the alignment between the community structure and the topics of tweets, the results indicated that the Louvain community detection algorithm together with DMM produced consistently higher alignment values and that alignments were generally higher when the number of topics was lower. After applying the Louvain method and DMM with 30 topics and grouping semantically similar topics in a hierarchy, we characterized 163,148 (57.16{\%}) tweets as evidence and advocacy, and 6244 (2.19{\%}) tweets describing personal experiences. Among the 4548 users who posted experiential tweets, 3449 users (75.84{\%}) were found in communities where the majority of tweets were about evidence and advocacy. Conclusions: The use of community detection in concert with topic modeling appears to be a useful way to characterize Twitter communities for the purpose of opinion surveillance in public health applications. Our approach may help identify online communities at risk of being influenced by negative opinions about public health interventions such as HPV vaccines.}
}
@inproceedings{szegedy2013intriguing,
	title        = {Intriguing properties of neural networks},
	author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
	year         = {2014},
	booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1312.6199},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:35:25 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{szegedy2016rethinking,
	title        = {Rethinking the Inception Architecture for Computer Vision},
	author       = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	year         = {2016},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {2818--2826}
}
@article{tack2022consistency,
	title        = {Consistency Regularization for Adversarial Robustness},
	author       = {Tack, Jihoon and Yu, Sihyun and Jeong, Jongheon and Kim, Minseon and Hwang, Sung Ju and Shin, Jinwoo},
	year         = {2022},
	month        = {Jun.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {36},
	number       = {8},
	pages        = {8414--8422},
	doi          = {10.1609/aaai.v36i8.20817},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20817},
	abstractnote = {Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary `consistency’ regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.}
}
@inproceedings{taigman2014deepface,
	title        = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},
	author       = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year         = {2014},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {1701--1708}
}
@article{tambon2023probabilistic,
	title        = {A probabilistic framework for mutation testing in deep neural networks},
	author       = {Florian Tambon and Foutse Khomh and Giuliano Antoniol},
	year         = {2023},
	journal      = {Information and Software Technology},
	volume       = {155},
	pages        = {107129},
	doi          = {https://doi.org/10.1016/j.infsof.2022.107129},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584922002385},
	keywords     = {Deep Learning, Mutation Testing, Bayesian Probability},
	abstract     = {Context: Mutation Testing (MT) is an important tool in traditional Software Engineering (SE) white-box testing. It aims to artificially inject faults in a system to evaluate a test suite's capability to detect them, assuming that the test suite defects finding capability will then translate to real faults. If MT has long been used in SE, it is only recently that it started gaining the attention of the Deep Learning (DL) community, with researchers adapting it to improve the testability of DL models and improve the trustworthiness of DL systems. Objective: If several techniques have been proposed for MT, most of them neglected the stochasticity inherent to DL resulting from the training phase. Even the latest MT approaches in DL, which propose to tackle MT through a statistical approach, might give inconsistent results. Indeed, as their statistic is based on a fixed set of sampled training instances, it can lead to different results across instances set when results should be consistent for any instance. Methods: In this work, we propose a Probabilistic Mutation Testing (PMT) approach that alleviates the inconsistency problem and allows for a more consistent decision on whether a mutant is killed or not. Results: We show that PMT effectively allows a more consistent and informed decision on mutations through evaluation using three models and eight mutation operators used in previously proposed MT methods. We also analyze the trade-off between the approximation error and the cost of our method, showing that relatively small error can be achieved for a manageable cost. Conclusion: Our results showed the limitation of current MT practices in DNN and the need to rethink them. We believe PMT is the first step in that direction which effectively removes the lack of consistency across test executions of previous methods caused by the stochasticity of DNN training.}
}
@inproceedings{tan2019efficientnet,
	title        = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
	author       = {Tan, Mingxing and Le, Quoc},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {6105--6114},
	url          = {https://proceedings.mlr.press/v97/tan19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf}
}
@inproceedings{tang2018personalized,
	title        = {Personalized top-n sequential recommendation via convolutional sequence embedding},
	author       = {Tang, Jiaxi and Wang, Ke},
	year         = {2018},
	booktitle    = {Proceedings of the eleventh ACM international conference on web search and data mining},
	pages        = {565--573}
}
@article{tang2022machine,
	title        = {Machine Learning with Differentially Private Labels: Mechanisms and Frameworks},
	author       = {Xinyu Tang and Milad Nasr and Saeed Mahloujifar and Virat Shejwalkar and Liwei Song and Amir Houmansadr and Prateek Mittal},
	year         = {2022},
	journal      = {Proc. Priv. Enhancing Technol.},
	volume       = {2022},
	number       = {4},
	pages        = {332--350},
	doi          = {10.56553/POPETS-2022-0112},
	url          = {https://doi.org/10.56553/popets-2022-0112},
	timestamp    = {Mon, 28 Aug 2023 21:35:56 +0200},
	biburl       = {https://dblp.org/rec/journals/popets/TangNMSSHM22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tantithamthavorn2016towards,
	title        = {Towards a better understanding of the impact of experimental components on defect prediction modelling},
	author       = {Tantithamthavorn, Chakkrit},
	year         = {2016},
	booktitle    = {Proceedings of the 38th International Conference on Software Engineering Companion},
	pages        = {867--870}
}
@inproceedings{tempo1996probabilistic,
	title        = {Probabilistic robustness analysis: explicit bounds for the minimum number of samples},
	author       = {Tempo, R. and Bai, E.W. and Dabbene, F.},
	year         = {1996},
	booktitle    = {Proceedings of 35th IEEE Conference on Decision and Control},
	volume       = {3},
	number       = {},
	pages        = {3424--3428 vol.3},
	doi          = {10.1109/CDC.1996.573690}
}
@inproceedings{theisen2021evaluating,
	title        = {Evaluating State-of-the-Art Classification Models Against Bayes Optimality},
	author       = {Theisen, Ryan and Wang, Huan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
	year         = {2021},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {34},
	pages        = {9367--9377},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2021/file/4e0ccd2b894f717df5ebc12f4282ee70-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@book{theodoridis2006pattern,
	title        = {Pattern Recognition, Fourth Edition},
	author       = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
	year         = {2008},
	publisher    = {Academic Press, Inc.},
	address      = {USA},
	isbn         = {1597492728},
	edition      = {4th},
	abstract     = {This book considers classical and current theory and practice, of both supervised and unsupervised pattern recognition, to build a complete background for professionals and students of engineering. The authors, leading experts in the field of pattern recognition, have provided an up-to-date, self-contained volume encapsulating this wide spectrum of information. The very latest methods are incorporated in this edition: semi-supervised learning, combining clustering algorithms, and relevance feedback.This edition includes many more worked examples and diagrams (in two colour) to help give greater understanding of the methods and their application. Computer-based problems will be included with MATLAB code. An accompanying book contains extra worked examples and MATLAB code of all the examples used in this book.Thoroughly developed to include many more worked examples to give greater understanding of this mathematically oriented subjectMany more diagrams included--now in two color--to provide greater insight through visual presentationAn accompanying manual includes Matlab code of the methods and algorithms in the book, together with solved problems and real-life data sets in medical imaging, remote sensing and audio recognition. The Manual is available separately or at a special packaged price (ISBN: 9780123744869).Latest hot topics included to further the reference value of the text including semi-supervised learning, combining clustering algorithms, and relevance feedback.}
}
@article{thornton2021predation,
	title        = {The Predation Game: Does dividing attention affect patterns of human foraging?},
	author       = {Thornton, Ian M. and Tagu, J{\'e}r{\^o}me and Zdravkovi{\'{c}}, Sun{\v{c}}ica and Kristj{\'a}nsson, {\'A}rni},
	year         = {2021},
	month        = {May},
	day          = {06},
	journal      = {Cognitive Research: Principles and Implications},
	volume       = {6},
	number       = {1},
	pages        = {35},
	doi          = {10.1186/s41235-021-00299-w},
	issn         = {2365-7464},
	url          = {https://doi.org/10.1186/s41235-021-00299-w},
	abstract     = {Attention is known to play an important role in shaping the behaviour of both human and animal foragers. Here, in three experiments, we built on previous interactive tasks to create an online foraging game for studying divided attention in human participants exposed to the (simulated) risk of predation. Participants used a ``sheep'' icon to collect items from different target categories randomly distributed across the display. Each trial also contained ``wolf'' objects, whose movement was inspired by classic studies of multiple object tracking. When participants needed to physically avoid the wolves, foraging patterns changed, with an increased tendency to switch between target categories and a decreased ability to prioritise high reward targets, relative to participants who could safely ignore them. However, when the wolves became dangerous by periodically changing form (briefly having big eyes) instead of by approaching the sheep, foraging patterns were unaffected. Spatial disruption caused by the need to rapidly shift position---rather the cost of reallocating attention---therefore appears to influence foraging in this context. These results thus confirm that participants can efficiently alternate between target selection and tracking moving objects, replicating earlier single-target search findings. Future studies may need to increase the perceived risk or potential costs associated with simulated danger, in order to elicit the extended run behaviour predicted by animal models of foraging, but absent in the current data.}
}
@inproceedings{tian2009using,
	title        = {Using Latent Dirichlet Allocation for automatic categorization of software},
	author       = {Tian, Kai and Revelle, Meghan and Poshyvanyk, Denys},
	year         = {2009},
	booktitle    = {2009 6th IEEE International Working Conference on Mining Software Repositories},
	volume       = {},
	number       = {},
	pages        = {163--166},
	doi          = {10.1109/MSR.2009.5069496}
}
@misc{tian2020sticking,
	title        = {Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation},
	author       = {Ran Tian and Shashi Narayan and Thibault Sellam and Ankur P. Parikh},
	year         = {2020},
	url          = {https://openreview.net/forum?id=HkxU2pNYPH}
}
@article{tien2019sentence,
	title        = {Sentence modeling via multiple word embeddings and multi-level comparison for semantic textual similarity},
	author       = {Nguyen Huy Tien and Nguyen Minh Le and Yamasaki Tomohiro and Izuha Tatsuya},
	year         = {2019},
	journal      = {Information Processing  \& Management},
	volume       = {56},
	number       = {6},
	pages        = {102090},
	doi          = {https://doi.org/10.1016/j.ipm.2019.102090},
	issn         = {0306-4573},
	url          = {https://www.sciencedirect.com/science/article/pii/S0306457319301335},
	keywords     = {Multiple word embeddings, Sentence embedding, Semantic, Similarity, Multi-level comparison},
	abstract     = {Recently, using a pretrained word embedding to represent words achieves success in many natural language processing tasks. According to objective functions, different word embedding models capture different aspects of linguistic properties. However, the Semantic Textual Similarity task, which evaluates similarity/relation between two sentences, requires to take into account of these linguistic aspects. Therefore, this research aims to encode various characteristics from multiple sets of word embeddings into one embedding and then learn similarity/relation between sentences via this novel embedding. Representing each word by multiple word embeddings, the proposed MaxLSTM-CNN encoder generates a novel sentence embedding. We then learn the similarity/relation between our sentence embeddings via Multi-level comparison. Our method M-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e., measure textual similarity, identify paraphrase, recognize textual entailment). Our model does not use hand-crafted features (e.g., alignment features, Ngram overlaps, dependency features) as well as does not require pre-trained word embeddings to have the same dimension.}
}
@inproceedings{tillmann1997accelerated,
	title        = {Accelerated {DP} based search for statistical translation},
	author       = {Christoph Tillmann and Stephan Vogel and Hermann Ney and A. Zubiaga and Hassan Sawaf},
	year         = {1997},
	booktitle    = {Fifth European Conference on Speech Communication and Technology, {EUROSPEECH} 1997, Rhodes, Greece, September 22-25, 1997},
	publisher    = {{ISCA}},
	url          = {http://www.isca-speech.org/archive/eurospeech\_1997/e97\_2667.html},
	editor       = {George Kokkinakis and Nikos Fakotakis and Evangelos Dermatas},
	timestamp    = {Tue, 16 Nov 2021 11:37:43 +0100},
	biburl       = {https://dblp.org/rec/conf/interspeech/TillmannVNZS97.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tjeng2017evaluating,
	title        = {Evaluating Robustness of Neural Networks with Mixed Integer Programming},
	author       = {Vincent Tjeng and Kai Yuanqing Xiao and Russ Tedrake},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1711.07356},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HyGIdiRqtm},
	timestamp    = {Tue, 10 Aug 2021 17:46:21 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TjengXT19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{tolga_2020,
	title        = {Feeding flying foxes},
	author       = {Tolga},
	year         = {2020},
	month        = {Aug},
	journal      = {Tolga Bat Hospital},
	url          = {https://tolgabathospital.org/bats/feeding/?portfolioCats=110}
}
@article{touvron2023llama,
	title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	author       = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale andand others},
	year         = {2023},
	journal      = {CoRR},
	volume       = {abs/2307.09288},
	doi          = {10.48550/ARXIV.2307.09288},
	url          = {https://doi.org/10.48550/arXiv.2307.09288},
	eprinttype   = {arXiv},
	eprint       = {2307.09288},
	timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tramer2017ensemble,
	title        = {Ensemble Adversarial Training: Attacks and Defenses},
	author       = {Florian Tram{\`{e}}r and Alexey Kurakin and Nicolas Papernot and Ian J. Goodfellow and Dan Boneh and Patrick D. McDaniel},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rkZvSe-RZ},
	timestamp    = {Sun, 02 Oct 2022 16:05:32 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TramerKPGBM18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tramer2020adaptive,
	title        = {On Adaptive Attacks to Adversarial Example Defenses},
	author       = {Tramer, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
	year         = {2020},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {33},
	pages        = {1633--1645},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/11f38f8ecd71867b42433548d1078e38-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{tran2023fairdp,
	title        = {FairDP: Certified Fairness with Differential Privacy},
	author       = {Tran, Khang and Fioretto, Ferdinando and Khalil, Issa and Thai, My T and Phan, NhatHai},
	year         = {2023},
	journal      = {arXiv preprint arXiv:2305.16474}
}
@article{trigueros2018enhancing,
	title        = {Enhancing convolutional neural networks for face recognition with occlusion maps and batch triplet loss},
	author       = {Daniel {Sáez Trigueros} and Li Meng and Margaret Hartnett},
	year         = {2018},
	journal      = {Image and Vision Computing},
	volume       = {79},
	pages        = {99--108},
	doi          = {https://doi.org/10.1016/j.imavis.2018.09.011},
	issn         = {0262-8856},
	url          = {https://www.sciencedirect.com/science/article/pii/S0262885618301562},
	keywords     = {Face recognition, Convolutional neural networks, Facial occlusions, Distance metric learning},
	abstract     = {Despite the recent success of convolutional neural networks for computer vision applications, unconstrained face recognition remains a challenge. In this work, we make two contributions to the field. Firstly, we consider the problem of face recognition with partial occlusions and show how current approaches might suffer significant performance degradation when dealing with this kind of face images. We propose a simple method to find out which parts of the human face are more important to achieve a high recognition rate, and use that information during training to force a convolutional neural network to learn discriminative features from all the face regions more equally, including those that typical approaches tend to pay less attention to. We test the accuracy of the proposed method when dealing with real-life occlusions using the AR face database. Secondly, we propose a novel loss function called batch triplet loss that improves the performance of the triplet loss by adding an extra term to the loss function to cause minimisation of the standard deviation of both positive and negative scores. We show consistent improvement in the Labeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments to the convolutional neural network training.}
}
@inproceedings{trockman2021orthogonalizing,
	title        = {Orthogonalizing Convolutional Layers with the Cayley Transform},
	author       = {Asher Trockman and J Zico Kolter},
	year         = {2021},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Pbj8H_jEHYv}
}
@inproceedings{tsipras2018robustness,
	title        = {Robustness May Be at Odds with Accuracy},
	author       = {Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=SyxAb30cY7},
	timestamp    = {Thu, 25 Jul 2019 14:26:02 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/TsiprasSETM19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{tsybakov2009nonparametric,
	title        = {Nonparametric estimators},
	author       = {Tsybakov, Alexandre B and Tsybakov, Alexandre B},
	year         = {2009},
	journal      = {Introduction to Nonparametric Estimation},
	publisher    = {Springer},
	pages        = {1--76}
}
@article{tufano2019empirical,
	title        = {An empirical study on learning bug-fixing patches in the wild via neural machine translation},
	author       = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
	year         = {2019},
	journal      = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
	publisher    = {ACM New York, NY, USA},
	volume       = {28},
	number       = {4},
	pages        = {1--29}
}
@article{tumer1996analysis,
	title        = {Analysis of decision boundaries in linearly combined neural classifiers},
	author       = {Tumer, Kagan and Ghosh, Joydeep},
	year         = {1996},
	journal      = {Pattern recognition},
	publisher    = {Elsevier},
	volume       = {29},
	number       = {2},
	pages        = {341--348}
}
@inproceedings{tumer1996estimating,
	title        = {Estimating the Bayes error rate through classifier combining},
	author       = {Tumer, K. and Ghosh, J.},
	year         = {1996},
	booktitle    = {Proceedings of 13th International Conference on Pattern Recognition},
	volume       = {2},
	number       = {},
	pages        = {695--699 vol.2},
	doi          = {10.1109/ICPR.1996.546912}
}
@article{tunstall2023zephyr,
	title        = {Zephyr: Direct Distillation of {LM} Alignment},
	author       = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl{\'{e}}mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
	year         = {2023},
	journal      = {CoRR},
	volume       = {abs/2310.16944},
	doi          = {10.48550/ARXIV.2310.16944},
	url          = {https://doi.org/10.48550/arXiv.2310.16944},
	eprinttype   = {arXiv},
	eprint       = {2310.16944},
	timestamp    = {Thu, 08 Aug 2024 15:05:27 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2310-16944.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{tversky1977features,
	title        = {Features of similarity.},
	author       = {Tversky, Amos},
	year         = {1977},
	journal      = {Psychological review},
	publisher    = {American Psychological Association},
	volume       = {84},
	number       = {4},
	pages        = {327}
}
@inproceedings{udeshi2018automated,
	title        = {Automated directed fairness testing},
	author       = {Udeshi, Sakshi and Arora, Pryanshu and Chattopadhyay, Sudipta},
	year         = {2018},
	booktitle    = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
	location     = {Montpellier, France},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASE '18},
	pages        = {98–108},
	doi          = {10.1145/3238147.3238165},
	isbn         = {9781450359375},
	url          = {https://doi.org/10.1145/3238147.3238165},
	abstract     = {Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof- the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70\% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94\%.},
	numpages     = {11},
	keywords     = {Directed Testing, Machine Learning, Software Fairness}
}
@article{urban2020perfectly,
	title        = {Perfectly parallel fairness certification of neural networks},
	author       = {Urban, Caterina and Christakis, Maria and W\"{u}stholz, Valentin and Zhang, Fuyuan},
	year         = {2020},
	month        = {nov},
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {4},
	number       = {OOPSLA},
	doi          = {10.1145/3428253},
	url          = {https://doi.org/10.1145/3428253},
	issue_date   = {November 2020},
	abstract     = {Recently, there is growing concern that machine-learned software, which currently assists or even automates decision making, reproduces, and in the worst case reinforces, bias present in the training data. The development of tools and techniques for certifying fairness of this software or describing its biases is, therefore, critical. In this paper, we propose a perfectly parallel static analysis for certifying fairness of feed-forward neural networks used for classification of tabular data. When certification succeeds, our approach provides definite guarantees, otherwise, it describes and quantifies the biased input space regions. We design the analysis to be sound, in practice also exact, and configurable in terms of scalability and precision, thereby enabling pay-as-you-go certification. We implement our approach in an open-source tool called Libra and demonstrate its effectiveness on neural networks trained on popular datasets.},
	articleno    = {185},
	numpages     = {30},
	keywords     = {Abstract Interpretation, Fairness, Neural Networks, Static Analysis}
}
@article{usman2022antidotert,
	title        = {AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks},
	author       = {Muhammad Usman and Youcheng Sun and Divya Gopinath and Corina S. Pasareanu},
	year         = {2022},
	journal      = {CoRR},
	volume       = {abs/2202.01179},
	url          = {https://arxiv.org/abs/2202.01179},
	eprinttype   = {arXiv},
	eprint       = {2202.01179},
	timestamp    = {Wed, 09 Feb 2022 15:43:35 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2202-01179.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{utah,
	title        = {E5: 7 domains of food},
	author       = {UTAH, University of Utah Health},
	year         = {2020},
	month        = {Nov},
	journal      = {University of Utah Health},
	url          = {https://healthcare.utah.edu/the-scope/shows.php?shows=1_moomp8q5}
}
@inproceedings{vaishnavi2022accelerating,
	title        = {Accelerating Certified Robustness Training via Knowledge Transfer},
	author       = {Vaishnavi, Pratik and Eykholt, Kevin and Rahmati, Amir},
	year         = {2022},
	journal      = {CoRR},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {35},
	pages        = {5269--5281},
	doi          = {10.48550/arXiv.2210.14283},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2022/file/22bf0634985f4e6dbb1fb40e247d1478-Paper-Conference.pdf},
	editor       = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	eprinttype   = {arXiv},
	eprint       = {2210.14283},
	timestamp    = {Mon, 31 Oct 2022 12:04:42 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2210-14283.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{valencia1991studies,
	title        = {Studies on natural logic and categorial grammar},
	author       = {Valencia, V{\'i}ctor Manuel S{\'a}nchez},
	year         = {1991},
	publisher    = {Universiteit van Amsterdam}
}
@article{van2005management,
	title        = {Management Research as a Design Science: Articulating the Research Products of Mode 2 Knowledge Production in Management},
	author       = {Van Aken, Joan Ernst},
	year         = {2005},
	journal      = {British Journal of Management},
	volume       = {16},
	number       = {1},
	pages        = {19--36},
	doi          = {https://doi.org/10.1111/j.1467-8551.2005.00437.x},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8551.2005.00437.x},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8551.2005.00437.x},
	abstract     = {The relevance problem of academic management research in organization and management is an old and thorny one. Recent discussions on this issue have resulted in proposals to use more Mode 2 knowledge production in our field. These discussions focused mainly on the process of research itself and less on the products produced by this process. Here the focus is on the so-called field-tested and grounded technological rule as a possible product of Mode 2 research with the potential to improve the relevance of academic research in management. Technological rules can be seen as solution-oriented knowledge. Such knowledge may be called Management Theory, while more description-oriented knowledge may be called Organization Theory. In this article the nature of technological rules in management is discussed, as well as their development, their use in actual management practice and the potential for cross-fertilization between Management Theory and Organization Theory.}
}
@article{van2021human,
	title        = {Human evaluation of automatically generated text: Current trends and best practice guidelines},
	author       = {Chris {van der Lee} and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer},
	year         = {2021},
	journal      = {Computer Speech  \& Language},
	volume       = {67},
	pages        = {101151},
	doi          = {https://doi.org/10.1016/j.csl.2020.101151},
	issn         = {0885-2308},
	url          = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
	keywords     = {Natural Language Generation, Human evaluation, Recommendations, Literature review, Open science, Ethics},
	abstract     = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.}
}
@book{vapnik1999nature,
	title        = {The nature of statistical learning theory},
	author       = {Vapnik, Vladimir},
	year         = {1999},
	publisher    = {Springer science  \& business media}
}
@inproceedings{vaswani2017attention,
	title        = {Attention is All you Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	year         = {2017},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {30},
	url          = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{vedantam2015cider,
	title        = {CIDEr: Consensus-Based Image Description Evaluation},
	author       = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
	year         = {2015},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{villegas2018characterizing,
	title        = {Characterizing context-aware recommender systems: A systematic literature review},
	author       = {Villegas, Norha M and S{\'a}nchez, Cristian and D{\'\i}az-Cely, Javier and Tamura, Gabriel},
	year         = {2018},
	journal      = {Knowledge-Based Systems},
	publisher    = {Elsevier},
	volume       = {140},
	pages        = {173--200}
}
@inproceedings{virmaux2018lipschitz,
	title        = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	author       = {Virmaux, Aladin and Scaman, Kevin},
	year         = {2018},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {31},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2018/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@inproceedings{vlm_adv,
	title        = {{VLATTACK:} Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models},
	author       = {Ziyi Yin and Muchao Ye and Tianrong Zhang and Tianyu Du and Jinguo Zhu and Han Liu and Jinghui Chen and Ting Wang and Fenglong Ma},
	year         = {2023},
	booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023},
	url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/a5e3cf29c269b041ccd644b6beaf5c42-Abstract-Conference.html},
	editor       = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
	timestamp    = {Mon, 13 May 2024 13:43:39 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/0003YZDZLCWM23.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wadler1989theorems,
	title        = {Theorems for Free!},
	author       = {Wadler, Philip},
	year         = {1989},
	booktitle    = {Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture},
	location     = {Imperial College, London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FPCA '89},
	pages        = {347--359},
	doi          = {10.1145/99370.99404},
	isbn         = {0897913280},
	url          = {https://doi.org/10.1145/99370.99404},
	numpages     = {13}
}
@article{wald1992sequential,
	title        = {{Sequential Tests of Statistical Hypotheses}},
	author       = {A. Wald},
	year         = {1945},
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = {16},
	number       = {2},
	pages        = {117 -- 186},
	doi          = {10.1214/aoms/1177731118},
	url          = {https://doi.org/10.1214/aoms/1177731118}
}
@inproceedings{wang2006topics,
	title        = {Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends},
	author       = {Wang, Xuerui and McCallum, Andrew},
	year         = {2006},
	booktitle    = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Philadelphia, PA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '06},
	pages        = {424--433},
	doi          = {10.1145/1150402.1150450},
	isbn         = {1595933395},
	url          = {https://doi.org/10.1145/1150402.1150450},
	abstract     = {This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time. Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp. Thus, the meaning of a particular topic can be relied upon as constant, but the topics' occurrence and correlations change significantly over time. We present results on nine months of personal email, 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses, showing improved topics, better timestamp prediction, and interpretable trends.},
	numpages     = {10},
	keywords     = {temporal analysis, graphical models, topic modeling}
}
@inproceedings{wang2012tm,
	title        = {TM-LDA: Efficient Online Modeling of Latent Topic Transitions in Social Media},
	author       = {Wang, Yu and Agichtein, Eugene and Benzi, Michele},
	year         = {2012},
	booktitle    = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '12},
	pages        = {123--131},
	doi          = {10.1145/2339530.2339552},
	isbn         = {9781450314626},
	url          = {https://doi.org/10.1145/2339530.2339552},
	abstract     = {Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.},
	numpages     = {9},
	keywords     = {topic transition modeling, temporal language models, mining social media data}
}
@inproceedings{wang2016theoretical,
	title        = {A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples},
	author       = {Beilun Wang and Ji Gao and Yanjun Qi},
	year         = {2017},
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HkcM7yVKl},
	timestamp    = {Thu, 04 Apr 2019 13:20:10 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/WangGQ17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2017deep,
	title        = {Deep Metric Learning With Angular Loss},
	author       = {Wang, Jian and Zhou, Feng and Wen, Shilei and Liu, Xiao and Lin, Yuanqing},
	year         = {2017},
	month        = {Oct},
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{wang2017residual,
	title        = {Residual Attention Network for Image Classification},
	author       = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year         = {2017},
	month        = {July},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{wang2017unleash,
	title        = {Unleash the Black Magic in Age: A Multi-Task Deep Neural Network Approach for Cross-Age Face Verification},
	author       = {Wang, Xiaolong and Zhou, Yin and Kong, Deguang and Currey, Jon and Li, Dawei and Zhou, Jiayu},
	year         = {2017},
	booktitle    = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
	volume       = {},
	number       = {},
	pages        = {596--603},
	doi          = {10.1109/FG.2017.75}
}
@article{wang2018additive,
	title        = {Additive Margin Softmax for Face Verification},
	author       = {Wang, Feng and Cheng, Jian and Liu, Weiyang and Liu, Haijun},
	year         = {2018},
	journal      = {IEEE Signal Processing Letters},
	volume       = {25},
	number       = {7},
	pages        = {926--930},
	doi          = {10.1109/LSP.2018.2822810}
}
@inproceedings{wang2018attention,
	title        = {Attention-based transactional context embedding for next-item recommendation},
	author       = {Wang, Shoujin and Hu, Liang and Cao, Longbing and Huang, Xiaoshui and Lian, Defu and Liu, Wei},
	year         = {2018},
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = {32},
	number       = {1}
}
@inproceedings{wang2018cosface,
	title        = {CosFace: Large Margin Cosine Loss for Deep Face Recognition},
	author       = {Wang, Hao and Wang, Yitong and Zhou, Zheng and Ji, Xing and Gong, Dihong and Zhou, Jingchao and Li, Zhifeng and Liu, Wei},
	year         = {2018},
	month        = {June},
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{wang2018formal,
	title        = {Formal Security Analysis of Neural Networks using Symbolic Intervals},
	author       = {Shiqi Wang and Kexin Pei and Justin Whitehouse and Junfeng Yang and Suman Jana},
	year         = {2018},
	month        = aug,
	booktitle    = {27th USENIX Security Symposium (USENIX Security 18)},
	publisher    = {USENIX Association},
	address      = {Baltimore, MD},
	pages        = {1599--1614},
	isbn         = {978-1-939133-04-5},
	url          = {https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi}
}
@inproceedings{wang2018happiness,
	title        = {Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation},
	author       = {Wang, Jia and Feng, Yungang and Naghizade, Elham and Rashidi, Lida and Lim, Kwan Hui and Lee, Kate},
	year         = {2018},
	booktitle    = {Companion Proceedings of the The Web Conference 2018},
	location     = {Lyon, France},
	publisher    = {International World Wide Web Conferences Steering Committee},
	address      = {Republic and Canton of Geneva, CHE},
	series       = {WWW '18},
	pages        = {1401--1405},
	doi          = {10.1145/3184558.3191583},
	isbn         = {9781450356404},
	url          = {https://doi.org/10.1145/3184558.3191583},
	abstract     = {Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.},
	numpages     = {5},
	keywords     = {social networks, sentiment analysis, location recommendation}
}
@inproceedings{wang2019improving,
	title        = {Improving adversarial robustness requires revisiting misclassified examples},
	author       = {Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
	year         = {2019},
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{wang2019sparse,
	title        = {On Sparse Linear Regression in the Local Differential Privacy Model},
	author       = {Wang, Di and Xu, Jinhui},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {6628--6637},
	url          = {https://proceedings.mlr.press/v97/wang19m.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/wang19m/wang19m.pdf},
	abstract     = {In this paper, we study the sparse linear regression problem under the Local Differential Privacy (LDP) model. We first show that polynomial dependency on the dimensionality $p$ of the space is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the privacy of the whole dataset needs to be preserved. Similar limitations also exist for other types of error measurements and in the relaxed local models. This indicates that differential privacy in high dimensional space is unlikely achievable for the problem. With the understanding of this limitation, we then present two algorithmic results. The first one is a sequential interactive LDP algorithm for the low dimensional sparse case, called Locally Differentially Private Iterative Hard Thresholding (LDP-IHT), which achieves a near optimal upper bound. This algorithm is actually rather general and can be used to solve quite a few other problems, such as (Local) DP-ERM with sparsity constraints and sparse regression with non-linear measurements. The second one is for the restricted (high dimensional) case where only the privacy of the responses (labels) needs to be preserved. For this case, we show that the optimal rate of the error estimation can be made logarithmically depending on $p$ (i.e., $\log p$) in the local model, where an upper bound is obtained by a label-privacy version of LDP-IHT. Experiments on real world and synthetic datasets confirm our theoretical analysis.}
}
@inproceedings{wang2020improving,
	title        = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},
	author       = {Yisen Wang and Difan Zou and Jinfeng Yi and James Bailey and Xingjun Ma and Quanquan Gu},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rklOg6EFwS},
	timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/0001ZY0MG20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wang2020mis,
	title        = {Mis-Classified Vector Guided Softmax Loss for Face Recognition},
	author       = {Wang, Xiaobo and Zhang, Shifeng and Wang, Shuo and Fu, Tianyu and Shi, Hailin and Mei, Tao},
	year         = {2020},
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {34},
	number       = {07},
	pages        = {12241--12248},
	doi          = {10.1609/aaai.v34i07.6906},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6906},
	abstractnote = {\&lt;p\&gt;Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (\&lt;em\&gt;e.g.\&lt;/em\&gt;, angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative features mining for discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and feature mining into a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives. Our code is available at http://www.cbsr.ia.ac.cn/users/xiaobowang/.\&lt;/p\&gt;}
}
@inproceedings{wang2021beta,
	title        = {Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification},
	author       = {Wang, Shiqi and Zhang, Huan and Xu, Kaidi and Lin, Xue and Jana, Suman and Hsieh, Cho-Jui and Kolter, J. Zico},
	year         = {2021},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {34},
	pages        = {29909--29921},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2021/file/fac7fead96dafceaf80c1daffeae82a4-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@inproceedings{wang2021crafting,
	title        = {Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection},
	author       = {Wang, Chenran and Zhang, Danyi and Huang, Suye and Li, Xiangyang and Ding, Leah},
	year         = {2021},
	booktitle    = {Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems},
	location     = {Virtual Event, Hong Kong},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ASSS '21},
	pages        = {23–28},
	doi          = {10.1145/3457340.3458302},
	isbn         = {9781450384032},
	url          = {https://doi.org/10.1145/3457340.3458302},
	abstract     = {While machine learning based spam detectors have proven useful, spammers are learning to bypass the detectors by modifying their email content. Adversarial attacks on machine learning models have been observed in domains such as image classification. Applying such adversarial attack algorithms to craft spam emails to evade spam email detectors, however, has limitations. Such algorithms generate adversarial perturbations in the feature space. Different from image data, translating the adversarial perturbations from the feature space to text formats, as in emails, changes the effectiveness of the adversarial perturbations. It can reduce the attack success rate in the case of spam email detection. In this paper, we study the feasibility of adversarial attacks on machine learning based spam detectors and propose two novel text crafting methods leveraging adversarial perturbations generated by the adversarial example generation algorithms to improve the attack effectiveness. One method tries to approximate the feature values and the other adds special words to original emails. In experimentation, we use PGD as an example to demonstrate and compare the effectiveness of our attack methods on spam email detectors. We also examine the transferability of the proposed attack methods on different machine learning models.},
	numpages     = {6},
	keywords     = {spam detection, crafting adversarial email, attack transferability, adversarial machine learning}
}
@article{wang2021deep,
	title        = {Deep face recognition: A survey},
	author       = {Mei Wang and Weihong Deng},
	year         = {2021},
	journal      = {Neurocomputing},
	volume       = {429},
	pages        = {215--244},
	doi          = {https://doi.org/10.1016/j.neucom.2020.10.081},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231220316945},
	keywords     = {Deep face recognition, Deep learning, Face processing, Face recognition database, Loss function, Deep network architecture},
	abstract     = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: "one-to-many augmentation" and "many-to-one normalization". Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.}
}
@inproceedings{wang2021enhancing,
	title        = {Enhancing the Transferability of Adversarial Attacks Through Variance Tuning},
	author       = {Wang, Xiaosen and He, Kun},
	year         = {2021},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {1924--1933}
}
@inproceedings{wang2021robot,
	title        = {RobOT: Robustness-Oriented Testing for Deep Learning Systems},
	author       = {Wang, Jingyi and Chen, Jialuo and Sun, Youcheng and Ma, Xingjun and Wang, Dongxia and Sun, Jun and Cheng, Peng},
	year         = {2021},
	booktitle    = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
	volume       = {},
	number       = {},
	pages        = {300--311},
	doi          = {10.1109/ICSE43902.2021.00038}
}
@inproceedings{wang2021understanding,
	title        = {Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning},
	author       = {Wang, Yuyan and Wang, Xuezhi and Beutel, Alex and Prost, Flavien and Chen, Jilin and Chi, Ed H.},
	year         = {2021},
	booktitle    = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
	location     = {Virtual Event, Singapore},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '21},
	pages        = {1748–1757},
	doi          = {10.1145/3447548.3467326},
	isbn         = {9781450383325},
	url          = {https://doi.org/10.1145/3447548.3467326},
	abstract     = {As multi-task models gain popularity in a wider range of machine learning applications, it is becoming increasingly important for practitioners to understand the fairness implications associated with those models. Most existing fairness literature focuses on learning a single task more fairly, while how ML fairness interacts with multiple tasks in the joint learning setting is largely under-explored. In this paper, we are concerned with how group fairness (e.g., equal opportunity, equalized odds) as an ML fairness concept plays out in the multi-task scenario. In multi-task learning, several tasks are learned jointly to exploit task correlations for a more efficient inductive transfer. This presents a multi-dimensional Pareto frontier on (1) the trade-off between group fairness and accuracy with respect to each task, as well as (2) the trade-offs across multiple tasks. We aim to provide a deeper understanding on how group fairness interacts with accuracy in multi-task learning, and we show that traditional approaches that mainly focus on optimizing the Pareto frontier of multi-task accuracy might not perform well on fairness goals. We propose a new set of metrics to better capture the multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely presented in a multi-task learning setting. We further propose a Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task learning. Experiments on several real-world datasets demonstrate the effectiveness of our proposed approach.},
	numpages     = {10},
	keywords     = {pareto frontier, multi-task-aware fairness treatment, multi-task learning, fairness}
}
@inproceedings{wang2022importance,
	title        = {On the importance of asymmetry for siamese representation learning},
	author       = {Wang, Xiao and Fan, Haoqi and Tian, Yuandong and Kihara, Daisuke and Chen, Xinlei},
	year         = {2022},
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {16570--16579}
}
@inproceedings{wang2022mlfw,
	title        = {MLFW: A Database for Face Recognition on Masked Faces},
	author       = {Wang, Chengrui and Fang, Han and Zhong, Yaoyao and Deng, Weihong},
	year         = {2022},
	booktitle    = {Biometric Recognition},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {180--188},
	isbn         = {978-3-031-20233-9},
	editor       = {Deng, Weihong and Feng, Jianjiang and Huang, Di and Kan, Meina and Sun, Zhenan and Zheng, Fang and Wang, Wenfeng and He, Zhaofeng},
	abstract     = {As more and more people begin to wear masks due to current COVID-19 pandemic, existing face recognition systems may encounter severe performance degradation when recognizing masked faces. To figure out the impact of masks on face recognition model, we build a simple but effective tool to generate masked faces from unmasked faces automatically, and construct a new database called Masked LFW (MLFW) based on Cross-Age LFW (CALFW) database. The mask on the masked face generated by our method has good visual consistency with the original face. Moreover, we collect various mask templates, covering most of the common styles appeared in the daily life, to achieve diverse generation effects. Considering realistic scenarios, we design three kinds of combinations of face pairs. The recognition accuracy of SOTA models declines 5{\%}--16{\%} on MLFW database compared with the accuracy on the original images. MLFW database can be viewed and downloaded at http://whdeng.cn/mlfw.}
}
@article{wang2024utilizing,
	title        = {Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges},
	author       = {Wang, Jiajia and Huang, Jimmy Xiangji and Tu, Xinhui and Wang, Junmei and Huang, Angela Jennifer and Laskar, Md Tahmid Rahman and Bhuiyan, Amran},
	year         = {2024},
	month        = {apr},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {56},
	number       = {7},
	doi          = {10.1145/3648471},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3648471},
	issue_date   = {July 2024},
	abstract     = {Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. Additionally, we highlight the advantages of employing encoder-based BERT models in contrast to recent large language models like ChatGPT, which are decoder-based and demand extensive computational resources. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.},
	articleno    = {185},
	numpages     = {33},
	keywords     = {BERT, information retrieval, natural language processing, artificial intelligence}
}
@article{wang2025privacy,
	title        = {Privacy-Preserving Sequential Recommendation with Collaborative Confusion},
	author       = {Wang, Wei and Lin, Yujie and Ren, Pengjie and Chen, Zhumin and Mine, Tsunenori and Zhao, Jianli and Zhao, Qiang and Zhang, Moyan and Ben, Xianye and Li, Yujun},
	year         = {2025},
	journal      = {ACM Transactions on Information Systems},
	publisher    = {ACM New York, NY},
	volume       = {43},
	number       = {2},
	pages        = {1--25}
}
@article{warner1965randomized,
	title        = {Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias},
	author       = {Stanley L. Warner},
	year         = {1965},
	journal      = {Journal of the American Statistical Association},
	publisher    = {ASA Website},
	volume       = {60},
	number       = {309},
	pages        = {63--69},
	doi          = {10.1080/01621459.1965.10480775},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1965.10480775},
	note         = {PMID: 12261830},
	eprint       = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1965.10480775}
}
@article{watson1964smooth,
	title        = {Smooth regression analysis},
	author       = {Watson, Geoffrey S},
	year         = {1964},
	journal      = {Sankhy{\=a}: The Indian Journal of Statistics, Series A},
	publisher    = {JSTOR},
	pages        = {359--372}
}
@article{watson2022systematic,
	title        = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research},
	author       = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
	year         = {2022},
	month        = {mar},
	journal      = {ACM Trans. Softw. Eng. Methodol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {31},
	number       = {2},
	doi          = {10.1145/3485275},
	issn         = {1049-331X},
	url          = {https://doi.org/10.1145/3485275},
	issue_date   = {April 2022},
	abstract     = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE \&amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23\&nbsp;unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.},
	articleno    = {32},
	numpages     = {58},
	keywords     = {Deep learning, neural networks, software engineering, literature review, machine learning}
}
@inproceedings{weinberger2005distance,
	title        = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
	author       = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
	year         = {2005},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	volume       = {18},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
	editor       = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt}
}
@article{wen2020adapting,
	title        = {{Adapting and evaluating a deep learning language model for clinical why-question answering}},
	author       = {Wen, Andrew and Elwazir, Mohamed Y and Moon, Sungrim and Fan, Jungwei},
	year         = {2020},
	month        = {02},
	journal      = {JAMIA Open},
	volume       = {3},
	number       = {1},
	pages        = {16--20},
	doi          = {10.1093/jamiaopen/ooz072},
	issn         = {2574-2531},
	url          = {https://doi.org/10.1093/jamiaopen/ooz072},
	abstract     = {{To adapt and evaluate a deep learning language model for answering why-questions based on patient-specific clinical text.Bidirectional encoder representations from transformers (BERT) models were trained with varying data sources to perform SQuAD 2.0 style why-question answering (why-QA) on clinical notes. The evaluation focused on: (1) comparing the merits from different training data and (2) error analysis.The best model achieved an accuracy of 0.707 (or 0.760 by partial match). Training toward customization for the clinical language helped increase 6\% in accuracy.The error analysis suggested that the model did not really perform deep reasoning and that clinical why-QA might warrant more sophisticated solutions.The BERT model achieved moderate accuracy in clinical why-QA and should benefit from the rapidly evolving technology. Despite the identified limitations, it could serve as a competent proxy for question-driven clinical information extraction.}},
	eprint       = {https://academic.oup.com/jamiaopen/article-pdf/3/1/16/33419137/ooz072.pdf}
}
@inproceedings{wen2020time,
	title        = {Time Series Data Augmentation for Deep Learning: A Survey},
	author       = {Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
	year         = {2021},
	month        = {8},
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {4653--4660},
	doi          = {10.24963/ijcai.2021/631},
	url          = {https://doi.org/10.24963/ijcai.2021/631},
	note         = {Survey Track},
	editor       = {Zhi-Hua Zhou}
}
@inproceedings{wen2023adversarial,
	title        = {Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?},
	author       = {Rui Wen and Zhengyu Zhao and Zhuoran Liu and Michael Backes and Tianhao Wang and Yang Zhang},
	year         = {2023},
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=zKvm1ETDOq}
}
@inproceedings{weng2010twitterrank,
	title        = {TwitterRank: Finding Topic-Sensitive Influential Twitterers},
	author       = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
	year         = {2010},
	booktitle    = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
	location     = {New York, New York, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WSDM '10},
	pages        = {261--270},
	doi          = {10.1145/1718487.1718520},
	isbn         = {9781605588896},
	url          = {https://doi.org/10.1145/1718487.1718520},
	abstract     = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4\% of the users in Twitter follow more than 80\% of their followers, and (2) 80.5\% of the users have 80\% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
	numpages     = {10},
	keywords     = {twitter, pagerank, influential}
}
@inproceedings{weng2018evaluating,
	title        = {Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},
	author       = {Tsui{-}Wei Weng and Huan Zhang and Pin{-}Yu Chen and Jinfeng Yi and Dong Su and Yupeng Gao and Cho{-}Jui Hsieh and Luca Daniel},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BkUHlMZ0b},
	timestamp    = {Sat, 31 Aug 2019 16:23:15 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/WengZCYSGHD18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{weng2018towards,
	title        = {Towards fast computation of certified robustness for relu networks},
	author       = {Weng, Lily and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Daniel, Luca and Boning, Duane and Dhillon, Inderjit},
	year         = {2018},
	booktitle    = {International Conference on Machine Learning},
	pages        = {5276--5285},
	organization = {PMLR}
}
@article{west2021misinformation,
	title        = {Misinformation in and about science},
	author       = {Jevin D. West  and Carl T. Bergstrom},
	year         = {2021},
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = {118},
	number       = {15},
	pages        = {e1912444117},
	doi          = {10.1073/pnas.1912444117},
	url          = {https://www.pnas.org/doi/abs/10.1073/pnas.1912444117},
	eprint       = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912444117},
	abstract     = {Humans learn about the world by collectively acquiring information, filtering it, and sharing what we know. Misinformation undermines this process. The repercussions are extensive. Without reliable and accurate sources of information, we cannot hope to halt climate change, make reasoned democratic decisions, or control a global pandemic. Most analyses of misinformation focus on popular and social media, but the scientific enterprise faces a parallel set of problems—from hype and hyperbole to publication bias and citation misdirection, predatory publishing, and filter bubbles. In this perspective, we highlight these parallels and discuss future research directions and interventions.}
}
@inproceedings{wicker2023certification,
	title        = {Certification of Distributional Individual Fairness},
	author       = {Wicker, Matthew and Piratla, Vihari and Weller, Adrian},
	year         = {2023},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {36},
	pages        = {27670--27681},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/57d8ebf4c2f050a6485f370d47656a9e-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@book{widdows2004geometry,
	title        = {Geometry and Meaning},
	author       = {Dominic Widdows},
	year         = {2004},
	publisher    = {{CSLI} Publications},
	series       = {{CSLI} lecture notes series},
	volume       = {172},
	isbn         = {978-1-57586-448-8},
	timestamp    = {Fri, 08 Nov 2013 14:46:31 +0100},
	biburl       = {https://dblp.org/rec/books/daglib/0031968.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@techreport{wightman_2017,
	title        = {{LSAC National Longitudinal Bar Passage Study}},
	author       = {F. Linda Wightman},
	year         = {2017},
	url          = {{Include URL if available}},
	note         = {Accessed: {DATE}},
	institution  = {Law School Admission Council (LSAC)}
}
@book{williams1959regression,
	title        = {Regression Analysis},
	author       = {Williams, E.J.},
	year         = {1959},
	publisher    = {Wiley},
	series       = {Wiley publication in applied statistics},
	url          = {https://books.google.co.in/books?id=uWkNogEACAAJ},
	lccn         = {59011815}
}
@inproceedings{winkler1990string,
	title        = {String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage},
	author       = {Winkler, William E.},
	year         = {1990},
	booktitle    = {Proceedings of the Section on Survey Research},
	location     = {Wachington, DC},
	publisher    = {ERIC},
	pages        = {354--359},
	url          = {https://eric.ed.gov/?id=ED325505},
	added-at     = {2006-02-14T15:21:42.000+0100},
	interhash    = {15e98ae8cdc43683f96562982e68f1b8},
	intrahash    = {a2dcfaedc01b2ffcab7afaffc6bd03bc},
	keywords     = {purge sunters information felligi integration merge},
	timestamp    = {2006-02-14T15:21:42.000+0100}
}
@article{wise1996yap3,
	title        = {YAP3: Improved Detection of Similarities in Computer Program and Other Texts},
	author       = {Wise, Michael J.},
	year         = {1996},
	month        = {mar},
	journal      = {SIGCSE Bull.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {28},
	number       = {1},
	pages        = {130--134},
	doi          = {10.1145/236462.236525},
	issn         = {0097-8418},
	url          = {https://doi.org/10.1145/236462.236525},
	issue_date   = {March 1996},
	abstract     = {In spite of years of effort, plagiarism in student assignment submissions still causes considerable difficulties for course designers; if students' work is not their own, how can anyone be certain they have learnt anything? YAP is a system for detecting suspected plagiarism in computer programs and other texts submitted by students. The paper reviews YAP3, the third version of YAP, focusing on its novel underlying algorithm - Running-Karp-Rabin Greedy-String-Tiling (or RKS-GST), whose development arose from the observation with YAP and other systems that students shuffle independent code segments. YAP3 is able to detect transposed subsequences, and is less perturbed by spurious additional statements. The paper concludes with a discussion of recent extension of YAP to English texts, further illustrating the flexibility of the YAP approach.},
	numpages     = {5}
}
@article{wiyatno2019adversarial,
	title        = {Adversarial Examples in Modern Machine Learning: {A} Review},
	author       = {Rey Reza Wiyatno and Anqi Xu and Ousmane Dia and Archy de Berker},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1911.05268},
	url          = {http://arxiv.org/abs/1911.05268},
	eprinttype   = {arXiv},
	eprint       = {1911.05268},
	timestamp    = {Mon, 02 Dec 2019 13:44:01 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1911-05268.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wold1987principal,
	title        = {Principal component analysis},
	author       = {Svante Wold and Kim Esbensen and Paul Geladi},
	year         = {1987},
	journal      = {Chemometrics and Intelligent Laboratory Systems},
	volume       = {2},
	number       = {1},
	pages        = {37--52},
	doi          = {https://doi.org/10.1016/0169-7439(87)80084-9},
	issn         = {0169-7439},
	url          = {https://www.sciencedirect.com/science/article/pii/0169743987800849},
	note         = {Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists},
	abstract     = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.}
}
@article{wong1987modeling,
	title        = {On modeling of information retrieval concepts in vector spaces},
	author       = {Wong, S. K. Michael and Ziarko, Wojciech and Raghavan, Vijay V. and Wong, Patrick CN},
	year         = {1987},
	journal      = {ACM Transactions on Database Systems (TODS)},
	publisher    = {ACM New York, NY, USA},
	volume       = {12},
	number       = {2},
	pages        = {299--321}
}
@article{wong2009atec,
	title        = {ATEC: automatic evaluation of machine translation via word choice and word order},
	author       = {Wong, Billy and Kit, Chunyu},
	year         = {2009},
	month        = {Sep},
	day          = {01},
	journal      = {Machine Translation},
	volume       = {23},
	number       = {2},
	pages        = {141--155},
	doi          = {10.1007/s10590-009-9061-x},
	issn         = {1573-0573},
	url          = {https://doi.org/10.1007/s10590-009-9061-x},
	abstract     = {We propose a novel metric ATEC for automatic MT evaluation based on explicit assessment of word choice and word order in an MT output in comparison to its reference translation(s), the two most fundamental factors in the construction of meaning for a sentence. The former is assessed by matching word forms at various linguistic levels, including surface form, stem, sound and sense, and further by weighing the informativeness of each word. The latter is quantified in term of the discordance of word position and word sequence between a translation candidate and its reference. In the evaluations using the MetricsMATR08 data set and the LDC MTC2 and MTC4 corpora, ATEC demonstrates an impressive positive correlation to human judgments at the segment level, highly comparable to the few state-of-the-art evaluation metrics.}
}
@inproceedings{wong2018provable,
	title        = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
	author       = {Wong, Eric and Kolter, Zico},
	year         = {2018},
	booktitle    = {International conference on machine learning},
	pages        = {5286--5295},
	organization = {PMLR}
}
@inproceedings{wong2020fast,
	title        = {Fast is better than free: Revisiting adversarial training},
	author       = {Eric Wong and Leslie Rice and J. Zico Kolter},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=BJx040EFvH},
	timestamp    = {Wed, 16 Dec 2020 15:31:31 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/WongRK20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{wouters1999citation,
	title        = {The citation culture},
	author       = {Wouters, Paulus Franciscus and others},
	year         = {1999},
	school       = {Universiteit van Amsterdam Amsterdam}
}
@article{wu2017lecture,
	title        = {Lecture notes on information-theoretic methods for high-dimensional statistics},
	author       = {Wu, Yihong},
	year         = {2017},
	journal      = {Lecture Notes for ECE598YW (UIUC)},
	volume       = {16},
	pages        = {15}
}
@article{wu2018beyond,
	title        = {Beyond Sparsity: Tree Regularization of Deep Models for Interpretability},
	author       = {Wu, Mike and Hughes, Michael and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	year         = {2018},
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {32},
	number       = {1},
	doi          = {10.1609/aaai.v32i1.11501},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11501},
	abstractnote = {&lt;p&gt; The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power. &lt;/p&gt;}
}
@article{wu2018light,
	title        = {A Light CNN for Deep Face Representation With Noisy Labels},
	author       = {Wu, Xiang and He, Ran and Sun, Zhenan and Tan, Tieniu},
	year         = {2018},
	journal      = {IEEE Transactions on Information Forensics and Security},
	volume       = {13},
	number       = {11},
	pages        = {2884--2896},
	doi          = {10.1109/TIFS.2018.2833032}
}
@inproceedings{wu2023does,
	title        = {Does Label Differential Privacy Prevent Label Inference Attacks?},
	author       = {Wu, Ruihan and Zhou, Jin Peng and Weinberger, Kilian Q. and Guo, Chuan},
	year         = {2023},
	month        = {25--27 Apr},
	booktitle    = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {206},
	pages        = {4336--4347},
	url          = {https://proceedings.mlr.press/v206/wu23a.html},
	editor       = {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
	pdf          = {https://proceedings.mlr.press/v206/wu23a/wu23a.pdf},
	abstract     = {Label differential privacy (label-DP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice label-DP does not preclude label inference attacks (LIAs): Models trained with label-DP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that label-DP is designed to limit the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At label-DP $\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Our bound shows the semantic protection conferred by label-DP and gives guidelines on how to choose $\epsilon$ to limit the threat of LIAs below a certain level. Finally, we empirically demonstrate that our result closely captures the behavior of simulated attacks on both synthetic and real world datasets.}
}
@inproceedings{xia2022vision,
	title        = {Vision transformer with deformable attention},
	author       = {Xia, Zhuofan and Pan, Xuran and Song, Shiji and Li, Li Erran and Huang, Gao},
	year         = {2022},
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {4794--4803}
}
@article{xiao2017fashion,
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = {2017},
	journal      = {CoRR},
	volume       = {abs/1708.07747},
	url          = {http://arxiv.org/abs/1708.07747},
	eprinttype   = {arXiv},
	eprint       = {1708.07747},
	timestamp    = {Mon, 13 Aug 2018 16:47:27 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xie2019improving,
	title        = {Improving Transferability of Adversarial Examples With Input Diversity},
	author       = {Xie, Cihang and Zhang, Zhishuai and Zhou, Yuyin and Bai, Song and Wang, Jianyu and Ren, Zhou and Yuille, Alan L.},
	year         = {2019},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{xie2020adversarial,
	title        = {Adversarial examples improve image recognition},
	author       = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
	year         = {2020},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {819--828}
}
@inproceedings{xu2017feature,
	title        = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks},
	author       = {Weilin Xu and David Evans and Yanjun Qi},
	year         = {2018},
	booktitle    = {25th Annual Network and Distributed System Security Symposium, {NDSS} 2018, San Diego, California, USA, February 18-21, 2018},
	publisher    = {The Internet Society},
	url          = {http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018\_03A-4\_Xu\_paper.pdf},
	timestamp    = {Thu, 17 Jun 2021 16:04:48 +0200},
	biburl       = {https://dblp.org/rec/conf/ndss/Xu0Q18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xu2019achieving,
	title        = {Achieving Differential Privacy and Fairness in Logistic Regression},
	author       = {Xu, Depeng and Yuan, Shuhan and Wu, Xintao},
	year         = {2019},
	booktitle    = {Companion Proceedings of The 2019 World Wide Web Conference},
	location     = {San Francisco, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {WWW '19},
	pages        = {594–599},
	doi          = {10.1145/3308560.3317584},
	isbn         = {9781450366755},
	url          = {https://doi.org/10.1145/3308560.3317584},
	abstract     = {Machine learning algorithms are used to make decisions in various applications. These algorithms rely on large amounts of sensitive individual information to work properly. Hence, there are sociological concerns about machine learning algorithms on matters like privacy and fairness. Currently, many studies focus on only protecting individual privacy or ensuring fairness of algorithms. However, how to meet both privacy and fairness requirements simultaneously in machine learning algorithms is under exploited. In this paper, we focus on one classic machine learning model, logistic regression, and develop differentially private and fair logistic regression models by combining functional mechanism and decision boundary fairness in a joint form. Theoretical analysis and empirical evaluations demonstrate our approaches effectively achieve both differential privacy and fairness while preserving good utility.},
	numpages     = {6},
	keywords     = {Differential Privacy, Fairness-aware Learning, Logistic Regression}
}
@inproceedings{xu2020automatic,
	title        = {Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond},
	author       = {Xu, Kaidi and Shi, Zhouxing and Zhang, Huan and Wang, Yihan and Chang, Kai-Wei and Huang, Minlie and Kailkhura, Bhavya and Lin, Xue and Hsieh, Cho-Jui},
	year         = {2020},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {33},
	pages        = {1129--1141},
	url          = {https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{xu2021robust,
	title        = {To be Robust or to be Fair: Towards Fairness in Adversarial Training},
	author       = {Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
	year         = {2021},
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {139},
	pages        = {11492--11501},
	url          = {https://proceedings.mlr.press/v139/xu21b.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/xu21b/xu21b.pdf},
	abstract     = {Adversarial training algorithms have been proved to be reliable to improve machine learning models’ robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ”automobile” but only 65% and 17% on class ”cat”. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models’ robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.},
	organization = {PMLR}
}
@inproceedings{xu2022gfairhint,
	title        = {{GF}airHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint},
	author       = {Paiheng Xu and Yuhang Zhou and Bang An and Wei Ai and Furong Huang},
	year         = {2022},
	booktitle    = {Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022},
	url          = {https://openreview.net/forum?id=DCQmL-gXGOG}
}
@article{xue2023tale,
	title        = {A Tale of Two Approximations: Tightening Over-Approximation for DNN Robustness Verification via Under-Approximation},
	author       = {Xue, Zhiyi and Liu, Si and Zhang, Zhaodi and Wu, Yiting and Zhang, Min},
	year         = {2023},
	journal      = {arXiv preprint arXiv:2305.16998}
}
@article{yang2012discriminative,
	title        = {Discriminative Feature Selection by Nonparametric Bayes Error Minimization},
	author       = {Yang, Shuang Hong and Hu, Bao-Gang},
	year         = {2012},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {24},
	number       = {8},
	pages        = {1422--1434},
	doi          = {10.1109/TKDE.2011.92}
}
@article{yang2018text,
	title        = {Text Mining of Twitter Data Using a Latent Dirichlet Allocation Topic Model and Sentiment Analysis},
	author       = {Sidi Yang and  Haiyi Zhang},
	year         = {2018},
	journal      = {International Journal of Computer and Information Engineering},
	publisher    = {World Academy of Science, Engineering and Technology},
	volume       = {12},
	number       = {7},
	pages        = {525--529},
	issn         = {eISSN: 1307-6892},
	url          = {https://publications.waset.org/vol/139},
	abstract     = {Twitter is a microblogging platform, where millions of users daily share their attitudes, views, and opinions. Using a probabilistic Latent Dirichlet Allocation (LDA) topic model to discern the most popular topics in the Twitter data is an effective way to analyze a large set of tweets to find a set of topics in a computationally efficient manner. Sentiment analysis provides an effective method to show the emotions and sentiments found in each tweet and an efficient way to summarize the results in a manner that is clearly understood. The primary goal of this paper is to explore text mining, extract and analyze useful information from unstructured text using two approaches: LDA topic modelling and sentiment analysis by examining Twitter plain text data in English. These two methods allow people to dig data more effectively and efficiently. LDA topic model and sentiment analysis can also be applied to provide insight views in business and scientific fields.},
	ee           = {https://publications.waset.org/pdf/10009246},
	bibsource    = {https://publications.waset.org/},
	index        = {Open Science Index 139, 2018}
}
@article{yang2019xlnet,
	title        = {Xlnet: Generalized autoregressive pretraining for language understanding},
	author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	year         = {2019},
	journal      = {Advances in neural information processing systems},
	volume       = {32}
}
@article{yang2020closer,
	title        = {A closer look at accuracy vs. robustness},
	author       = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
	year         = {2020},
	journal      = {Advances in neural information processing systems},
	volume       = {33},
	pages        = {8588--8601}
}
@article{yang2021characterizing,
	title        = {Characterizing the Evasion Attackability of Multi-label Classifiers},
	author       = {Yang, Zhuo and Han, Yufei and Zhang, Xiangliang},
	year         = {2021},
	month        = {May},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {35},
	number       = {12},
	pages        = {10647--10655},
	doi          = {10.1609/aaai.v35i12.17273},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17273},
	abstractnote = {Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator.}
}
@inproceedings{yang2021reachability,
	title        = {Reachability analysis of deep ReLU neural networks using facet-vertex incidence.},
	author       = {Yang, Xiaodong and Johnson, Taylor T and Tran, Hoang-Dung and Yamaguchi, Tomoya and Hoxha, Bardh and Prokhorov, Danil V},
	year         = {2021},
	booktitle    = {HSCC},
	volume       = {21},
	pages        = {19--21}
}
@inproceedings{yao2015incorporating,
	title        = {Incorporating Probabilistic Knowledge into Topic Models},
	author       = {Yao, Liang and Zhang, Yin and Wei, Baogang and Qian, Hongze and Wang, Yibing},
	year         = {2015},
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {586--597},
	isbn         = {978-3-319-18032-8},
	editor       = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
	abstract     = {Probabilistic Topic Models could be used to extract low-dimension aspects from document collections. However, such models without any human knowledge often produce aspects that are not interpretable. In recent years, a number of knowledge-based models have been proposed, which allow the user to input prior knowledge of the domain to produce more coherent and meaningful topics. In this paper, we incorporate human knowledge in the form of probabilistic knowledge base into topic models. By combining latent Dirichlet allocation, a widely used topic model with Probase, a large-scale probabilistic knowledge base, we improve the semantic coherence significantly. Our evaluation results will demonstrate the effectiveness of our method.}
}
@inproceedings{yeom2018privacy,
	title        = {Privacy risk in machine learning: Analyzing the connection to overfitting},
	author       = {Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
	year         = {2018},
	booktitle    = {2018 IEEE 31st computer security foundations symposium (CSF)},
	pages        = {268--282},
	organization = {IEEE}
}
@inproceedings{yeom2020individual,
	title        = {Individual Fairness Revisited: Transferring Techniques from Adversarial Robustness},
	author       = {Yeom, Samuel and Fredrikson, Matt},
	year         = {2020},
	month        = {7},
	booktitle    = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {437--443},
	doi          = {10.24963/ijcai.2020/61},
	url          = {https://doi.org/10.24963/ijcai.2020/61},
	note         = {Main track},
	editor       = {Christian Bessiere}
}
@inproceedings{yeung2017improved,
	title        = {Improved performance of face recognition using CNN with constrained triplet loss layer},
	author       = {Yeung, Henry Wing Fung and Li, Jiaxi and Chung, Yuk Ying},
	year         = {2017},
	booktitle    = {2017 International Joint Conference on Neural Networks (IJCNN)},
	volume       = {},
	number       = {},
	pages        = {1948--1955},
	doi          = {10.1109/IJCNN.2017.7966089}
}
@inproceedings{yin2014two,
	title        = {Two Sides of a Coin: Separating Personal Communication and Public Dissemination Accounts in Twitter},
	author       = {Yin, Peifeng and Ram, Nilam and Lee, Wang-Chien and Tucker, Conrad and Khandelwal, Shashank and Salath{\'e}, Marcel},
	year         = {2014},
	booktitle    = {Advances in Knowledge Discovery and Data Mining},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {163--175},
	isbn         = {978-3-319-06608-0},
	editor       = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	abstract     = {There are millions of accounts in Twitter. In this paper, we categorize twitter accounts into two types, namely Personal Communication Account (PCA) and Public Dissemination Account (PDA). PCAs are accounts operated by individuals and are used to express that individual's thoughts and feelings. PDAs, on the other hand, refer to accounts owned by non-individuals such as companies, governments, etc. Generally, Tweets in PDA (i) disseminate a specific type of information (e.g., job openings, shopping deals, car accidents) rather than sharing an individual's personal life; and (ii) may be produced by non-human entities (e.g., bots). We aim to develop techniques for identifying PDAs so as to (i) facilitate social scientists to reduce ``noise'' in their study of human behaviors, and (ii) to index them for potential recommendation to users looking for specific types of information. Through analysis, we find these two types of accounts follow different temporal, spatial and textual patterns. Accordingly we develop probabilistic models based on these features to identify PDAs. We also conduct a series of experiments to evaluate those algorithms for cleaning the Twitter data stream.}
}
@inproceedings{yin2015answering,
	title        = {Answering questions with complex semantic constraints on open knowledge bases},
	author       = {Yin, Pengcheng and Duan, Nan and Kao, Ben and Bao, Junwei and Zhou, Ming},
	year         = {2015},
	booktitle    = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
	pages        = {1301--1310}
}
@inproceedings{yu2016dynamic,
	title        = {A dynamic recurrent model for next basket recommendation},
	author       = {Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
	year         = {2016},
	booktitle    = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},
	pages        = {729--732}
}
@incollection{yu2020deep,
	title        = {Deep learning for real-time social media text classification for situation awareness--using Hurricanes Sandy, Harvey, and Irma as case studies},
	author       = {Yu, Manzhu and Huang, Qunying and Qin, Han and Scheele, Chris and Yang, Chaowei},
	year         = {2020},
	booktitle    = {Social Sensing and Big Data Computing for Disaster Management},
	publisher    = {Routledge},
	pages        = {33--50},
	doi          = {10.4324/9781003106494},
	editor       = {Li, Z. and Huang, Q. and Emrich, C. T.}
}
@misc{yu2022survey,
	title        = {Survey of Query-based Text Summarization},
	author       = {Hang Yu},
	year         = {2022},
	eprint       = {2211.11548},
	archiveprefix = {arXiv},
	primaryclass = {cs.IR}
}
@article{yuan2012improving,
	title        = {Improving software diagnosability via log enhancement},
	author       = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
	year         = {2012},
	journal      = {ACM Transactions on Computer Systems (TOCS)},
	publisher    = {ACM New York, NY, USA},
	volume       = {30},
	number       = {1},
	pages        = {1--28}
}
@inproceedings{yurochkin2020sensei,
	title        = {SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness},
	author       = {Mikhail Yurochkin and Yuekai Sun},
	year         = {2021},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=DktZb97_Fx}
}
@inproceedings{yurochkin2020training,
	title        = {Training individually fair ML models with sensitive subspace robustness},
	author       = {Mikhail Yurochkin and Amanda Bower and Yuekai Sun},
	year         = {2020},
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=B1gdkxHFDH}
}
@inproceedings{zagoruyko2016wide,
	title        = {Wide Residual Networks},
	author       = {Sergey Zagoruyko and Nikos Komodakis},
	year         = {2016},
	month        = {September},
	booktitle    = {Proceedings of the British Machine Vision Conference (BMVC)},
	publisher    = {BMVA Press},
	pages        = {87.1--87.12},
	doi          = {10.5244/C.30.87},
	isbn         = {1-901725-59-6},
	url          = {https://dx.doi.org/10.5244/C.30.87},
	articleno    = {87},
	numpages     = {12},
	editor       = {Richard C. Wilson, Edwin R. Hancock and William A. P. Smith}
}
@article{zeiler2012adadelta,
	title        = {{ADADELTA:} An Adaptive Learning Rate Method},
	author       = {Matthew D. Zeiler},
	year         = {2012},
	journal      = {CoRR},
	volume       = {abs/1212.5701},
	url          = {http://arxiv.org/abs/1212.5701},
	eprinttype   = {arXiv},
	eprint       = {1212.5701},
	timestamp    = {Mon, 13 Aug 2018 16:45:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1212-5701.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zemel2013learning,
	title        = {Learning Fair Representations},
	author       = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
	year         = {2013},
	month        = {17--19 Jun},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	volume       = {28},
	number       = {3},
	pages        = {325--333},
	url          = {https://proceedings.mlr.press/v28/zemel13.html},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/zemel13.pdf},
	abstract     = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}
@article{zhang2004statistical,
	title        = {{Statistical behavior and consistency of classification methods based on convex risk minimization}},
	author       = {Tong Zhang},
	year         = {2004},
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = {32},
	number       = {1},
	pages        = {56 -- 85},
	doi          = {10.1214/aos/1079120130},
	url          = {https://doi.org/10.1214/aos/1079120130},
	keywords     = {boosting, classification, consistency, kernel methods, large margin methods}
}
@article{zhang2007gene,
	title        = {Gene selection for classification of microarray data based on the Bayes error},
	author       = {Zhang, Ji-Gang and Deng, Hong-Wen},
	year         = {2007},
	month        = {Oct},
	day          = {03},
	journal      = {BMC Bioinformatics},
	volume       = {8},
	number       = {1},
	pages        = {370},
	doi          = {10.1186/1471-2105-8-370},
	issn         = {1471-2105},
	url          = {https://doi.org/10.1186/1471-2105-8-370},
	abstract     = {With DNA microarray data, selecting a compact subset of discriminative genes from thousands of genes is a critical step for accurate classification of phenotypes for, e.g., disease diagnosis. Several widely used gene selection methods often select top-ranked genes according to their individual discriminative power in classifying samples into distinct categories, without considering correlations among genes. A limitation of these gene selection methods is that they may result in gene sets with some redundancy and yield an unnecessary large number of candidate genes for classification analyses. Some latest studies show that incorporating gene to gene correlations into gene selection can remove redundant genes and improve classification accuracy.}
}
@inproceedings{zhang2015character,
	title        = {Character-level Convolutional Networks for Text Classification},
	author       = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
	year         = {2015},
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = {28},
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
	editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@inproceedings{zhang2017mixup,
	title        = {mixup: Beyond Empirical Risk Minimization},
	author       = {Hongyi Zhang and Moustapha Ciss{\'{e}} and Yann N. Dauphin and David Lopez{-}Paz},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=r1Ddp1-Rb},
	timestamp    = {Thu, 25 Jul 2019 14:25:50 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCDL18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2018efficient,
	title        = {Efficient Neural Network Robustness Certification with General Activation Functions},
	author       = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
	year         = {2018},
	journal      = {Advances in neural information processing systems},
	booktitle    = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	location     = {Montr\'{e}al, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'18},
	volume       = {31},
	pages        = {4944–4953},
	abstract     = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
	numpages     = {10}
}
@inproceedings{zhang2019adacos,
	title        = {AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations},
	author       = {Zhang, Xiao and Zhao, Rui and Qiao, Yu and Wang, Xiaogang and Li, Hongsheng},
	year         = {2019},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {10823--10832}
}
@article{zhang2019bertscore,
	title        = {BERTScore: Evaluating Text Generation with {BERT}},
	author       = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
	year         = {2019},
	journal      = {CoRR},
	volume       = {abs/1904.09675},
	url          = {http://arxiv.org/abs/1904.09675},
	eprinttype   = {arXiv},
	eprint       = {1904.09675},
	timestamp    = {Wed, 03 Jun 2020 10:08:39 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09675.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2019limitations,
	title        = {The Limitations of Adversarial Training and the Blind-Spot Attack},
	author       = {Huan Zhang and Hongge Chen and Zhao Song and Duane S. Boning and Inderjit S. Dhillon and Cho{-}Jui Hsieh},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HylTBhA5tQ},
	timestamp    = {Wed, 02 Dec 2020 16:43:27 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCSBDH19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2019measuring,
	title        = {Measuring Similarity between Brands Using Followers' Post in Social Media},
	author       = {Zhang, Yiwei and Wang, Xueting and Sakai, Yoshiaki and Yamasaki, Toshihiko},
	year         = {2020},
	booktitle    = {Proceedings of the ACM Multimedia Asia},
	location     = {Beijing, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MMAsia '19},
	doi          = {10.1145/3338533.3366600},
	isbn         = {9781450368414},
	url          = {https://doi.org/10.1145/3338533.3366600},
	abstract     = {In this paper, we propose a new measure to estimate the similarity between brands via posts of brands' followers on social network services (SNS). Our method was developed with the intention of exploring the brands that customers are likely to jointly purchase. Nowadays, brands use social media for targeted advertising because influencing users' preferences can greatly affect the trends in sales. We assume that data on SNS allows us to make quantitative comparisons between brands. Our proposed algorithm analyzes the daily photos and hashtags posted by each brand's followers. By clustering them and converting them to histograms, we can calculate the similarity between brands. We evaluated our proposed algorithm with purchase logs, credit card information, and answers to the questionnaires. The experimental results show that the purchase data maintained by a mall or a credit card company can predict the co-purchase very well, but not the customer's willingness to buy products of new brands. On the other hand, our method can predict the users' interest on brands with a correlation value over 0.53, which is pretty high considering that such interest to brands are high subjective and individual dependent.},
	articleno    = {6},
	numpages     = {6},
	keywords     = {Brands, Computational Marketing, Hashtag, Social Media}
}
@inproceedings{zhang2019mixup,
	title        = {mixup: Beyond empirical risk minimization},
	author       = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
	year         = {2019},
	booktitle    = {7th International Conference on Learning Representations (ICLR)}
}
@inproceedings{zhang2019theoretically,
	title        = {Theoretically Principled Trade-off between Robustness and Accuracy},
	author       = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	year         = {2019},
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {97},
	pages        = {7472--7482},
	url          = {https://proceedings.mlr.press/v97/zhang19p.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf},
	abstract     = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of \&nbsp;2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L_2 perturbation distance.}
}
@finproceedings{zhang2019towards,
	title        = {Towards Stable and Efficient Training of Verifiably Robust Neural Networks},
	author       = {Huan Zhang and Hongge Chen and Chaowei Xiao and Sven Gowal and Robert Stanforth and Bo Li and Duane S. Boning and Cho{-}Jui Hsieh},
	year         = {2020},
	booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Skxuk1rFwB},
	timestamp    = {Fri, 04 Dec 2020 15:21:06 +0100},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhangCXGSLBH20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2020black,
	title        = {Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework},
	author       = {Zhang, Dinghuai and Ye, Mao and Gong, Chengyue and Zhu, Zhanxing and Liu, Qiang},
	year         = {2020},
	booktitle    = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, BC, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'20},
	isbn         = {9781713829546},
	abstract     = {Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for ℓ2 perturbation. We propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distributions and leverage it to design new families of non-Gaussian smoothing distributions that work more efficiently for different ℓp settings, including ℓ1, ℓ2 and ℓ∞ attacks. Our proposed methods achieve better certification results than previous works and provide a new perspective on randomized smoothing certification.},
	articleno    = {195},
	numpages     = {11}
}
@inproceedings{zhang2020pegasus,
	title        = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive Summarization},
	author       = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
	year         = {2020},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = {119},
	pages        = {11328--11339},
	url          = {http://proceedings.mlr.press/v119/zhang20ae.html},
	timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
	biburl       = {https://dblp.org/rec/conf/icml/ZhangZSL20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2020towards,
	title        = {Towards characterizing adversarial defects of deep learning software from the lens of uncertainty},
	author       = {Zhang, Xiyue and Xie, Xiaofei and Ma, Lei and Du, Xiaoning and Hu, Qiang and Liu, Yang and Zhao, Jianjun and Sun, Meng},
	year         = {2020},
	booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
	location     = {Seoul, South Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '20},
	pages        = {739–751},
	doi          = {10.1145/3377811.3380368},
	isbn         = {9781450371216},
	url          = {https://doi.org/10.1145/3377811.3380368},
	abstract     = {Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty.In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35\%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.},
	numpages     = {13},
	keywords     = {adversarial attack, deep learning, software testing, uncertainty}
}
@inproceedings{zhang2020white,
	title        = {White-box fairness testing through adversarial sampling},
	author       = {Zhang, Peixin and Wang, Jingyi and Sun, Jun and Dong, Guoliang and Wang, Xinyu and Wang, Xingen and Dong, Jin Song and Dai, Ting},
	year         = {2020},
	booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
	location     = {Seoul, South Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '20},
	pages        = {949–960},
	doi          = {10.1145/3377811.3380331},
	isbn         = {9781450371216},
	url          = {https://doi.org/10.1145/3377811.3380331},
	abstract     = {Although deep neural networks (DNNs) have demonstrated astonishing performance in many applications, there are still concerns on their dependability. One desirable property of DNN for applications with societal impact is fairness (i.e., non-discrimination). In this work, we propose a scalable approach for searching individual discriminatory instances of DNN. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which makes it significantly more scalable than existing methods. Experimental results show that our approach explores the search space more effectively (9 times) and generates much more individual discriminatory instances (25 times) using much less time (half to 1/7).},
	numpages     = {12}
}
@article{zhang2021automatic,
	title        = {Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling},
	author       = {Zhang, Peixin and Wang, Jingyi and Sun, Jun and Wang, Xinyu and Dong, Guoliang and Wang, Xingen and Dai, Ting and Dong, Jin Song},
	year         = {2022},
	journal      = {IEEE Transactions on Software Engineering},
	volume       = {48},
	number       = {9},
	pages        = {3593--3612},
	doi          = {10.1109/TSE.2021.3101478},
	keywords     = {Data models;Deep learning;Systematics;Recurrent neural networks;Neurons;Training data;Task analysis;Deep learning;fairness testing;individual discrimination;gradient}
}
@article{zhang2021understanding,
	title        = {Understanding deep learning (still) requires rethinking generalization},
	author       = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year         = {2021},
	month        = {feb},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	address      = {New York, NY, USA},
	volume       = {64},
	number       = {3},
	pages        = {107--115},
	doi          = {10.1145/3446776},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/3446776},
	issue_date   = {March 2021},
	abstract     = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.We interpret our experimental findings by comparison with traditional models.We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
	numpages     = {9}
}
@inproceedings{zhang2023coophance,
	title        = {CoopHance: Cooperative Enhancement for Robustness of Deep Learning Systems},
	author       = {Zhang, Quan and Tian, Yongqiang and Ding, Yifeng and Li, Shanshan and Sun, Chengnian and Jiang, Yu and Sun, Jiaguang},
	year         = {2023},
	booktitle    = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
	location     = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ISSTA 2023},
	pages        = {753–765},
	doi          = {10.1145/3597926.3598093},
	isbn         = {9798400702211},
	url          = {https://doi.org/10.1145/3597926.3598093},
	abstract     = {
		Adversarial attacks have been a threat to Deep Learning (DL) systems to be reckoned with. By adding human-imperceptible perturbation to benign inputs, adversarial attacks can cause the incorrect behavior of DL systems. Considering the popularity of DL systems in the industry, it is critical and urgent for developers to enhance the robustness of DL systems against adversarial attacks.

		In this study, we propose a novel enhancement technique for DL systems, namely CoopHance. CoopHance leverages two specifically customized components, Regulator and Inspector, to cooperatively enhance the DL systems' robustness against adversarial examples with different distortions. Regulator can purify adversarial examples with low or moderate distortions, while Inspector is responsible for detecting these adversarial examples with high distortion by capturing the abnormal status of DL systems. Our evaluation using various attacks shows that, on average, CoopHance can successfully resist 90.62\% and 96.56\% of the adversarial examples that are generated for the unprotected systems on CIFAR-10 and SVHN datasets separately, which is 188.14\% more effective than five state-of-the-art enhancement techniques, including Feature Squeeze, LID, SOAP, Adversarial Training, and MagNet. Meanwhile, when attackers generate new adversarial examples on the enhanced systems, CoopHance can reject 78.06\% of attacks, which outperforms the best of five enhancement techniques by 82.71\% on average.
	},
	numpages     = {13},
	keywords     = {Robustness Enhancement, Deep Learning System}
}
@inproceedings{zhang2023proa,
	title        = {PRoA: A Probabilistic Robustness Assessment Against Functional Perturbations},
	author       = {Zhang, Tianle and Ruan, Wenjie and Fieldsend, Jonathan E.},
	year         = {2023},
	booktitle    = {Machine Learning and Knowledge Discovery in Databases},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {154--170},
	isbn         = {978-3-031-26409-2},
	editor       = {Amini, Massih-Reza and Canu, St{\'e}phane and Fischer, Asja and Guns, Tias and Kralj Novak, Petra and Tsoumakas, Grigorios},
	abstract     = {In safety-critical deep learning applications robustness measurement is a vital pre-deployment phase. However, existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand, these methods attempt to claim that no perturbations can ``fool'' deep neural networks (DNNs), which may be too stringent in practice. On the other hand, existing works rigorously consider {\$}{\$}L{\_}p{\$}{\$}Lpbounded additive perturbations on the pixel space, although perturbations, such as colour shifting and geometric transformations, are more practically and frequently occurring in the real world. Thus, from the practical standpoint, we present a novel and general probabilistic robustness assessment method (PRoA) based on the adaptive concentration, and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model, i.e., the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations, and PRoA can scale well to various large-scale deep neural networks compared to existing state-of-the-art baselines. For the purpose of reproducibility, we release our tool on GitHub: https://github.com/TrustAI/PRoA.}
}
@article{zhang2023towards,
	title        = {Towards Certified Probabilistic Robustness with High Accuracy},
	author       = {Zhang, Ruihan and Zhang, Peixin and Sun, Jun},
	year         = {2023}
}
@inproceedings{zhang2024certified,
	title        = {Certified Robust Accuracy of Neural Networks Are Bounded Due to Bayes Errors},
	author       = {Zhang, Ruihan and Sun, Jun},
	year         = {2024},
	booktitle    = {Computer Aided Verification},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {352--376},
	isbn         = {978-3-031-65630-9},
	editor       = {Gurfinkel, Arie and Ganesh, Vijay},
	abstract     = {Adversarial examples pose a security threat to many critical systems built on neural networks. While certified training improves robustness, it also decreases accuracy noticeably. Despite various proposals for addressing this issue, the significant accuracy drop remains. More importantly, it is not clear whether there is a certain fundamental limit on achieving robustness whilst maintaining accuracy. In this work, we offer a novel perspective based on Bayes errors. By adopting Bayes error to robustness analysis, we investigate the limit of certified robust accuracy, taking into account data distribution uncertainties. We first show that the accuracy inevitably decreases in the pursuit of robustness due to changed Bayes error in the altered data distribution. Subsequently, we establish an upper bound for certified robust accuracy, considering the distribution of individual classes and their boundaries. Our theoretical results are empirically evaluated on real-world datasets and are shown to be consistent with the limited success of existing certified training results, e.g., for CIFAR10, our analysis results in an upper bound (of certified robust accuracy) of 67.49{\%}, meanwhile existing approaches are only able to increase it from 53.89{\%} in 2017 to 62.84{\%} in 2023.}
}
@article{zhang2024does,
	title        = {How Does Bayes Error Limit Probabilistic Robust Accuracy},
	author       = {Zhang, Ruihan and Sun, Jun},
	year         = {2024},
	journal      = {arXiv preprint arXiv:2405.14923}
}
@inproceedings{zhang2024feddcsr,
	title        = {FedDCSR: Federated cross-domain sequential recommendation via disentangled representation learning},
	author       = {Zhang, Hongyu and Zheng, Dongyi and Yang, Xu and Feng, Jiyuan and Liao, Qing},
	year         = {2024},
	booktitle    = {Proceedings of the 2024 SIAM International Conference on Data Mining (SDM)},
	pages        = {535--543},
	organization = {SIAM}
}
@misc{zhang2025are,
	title        = {Are Probabilistic Robust Accuracy Bounded},
	author       = {Ruihan Zhang and Jun Sun},
	year         = {2025},
	url          = {https://openreview.net/forum?id=fVgUXaesSS}
}
@article{zhang2025correct,
	title        = {Correct-By-Construction: Certified Individual Fairness through Neural Network Training},
	author       = {Ruihan Zhang and Jun Sun},
	year         = {2025},
	month        = oct,
	journal      = {Proc. ACM Program. Lang.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {1},
	number       = {OOPSLA},
	issue_date   = {October 2025},
	articleno    = {80},
	numpages     = {30},
	keywords     = {Algorithmic Fairness, Probabilistic Inference, Probabilistic Programming}
}
@article{zhang2025robface,
	title        = {RobFace: A Test Suite for Efficient Robustness Evaluation of Face Recognition Systems},
	author       = {Zhang, Ruihan and Sun, Jun},
	year         = {2025},
	journal      = {IEEE Transactions on Reliability},
	volume       = {},
	number       = {},
	pages        = {1--14},
	doi          = {10.1109/TR.2025.3554575},
	keywords     = {Face recognition;Robustness;Perturbation methods;Testing;Vectors;Estimation;Accuracy;Optimization;Sun;Neural networks;Face recognition;robustness}
}
@misc{zhang2025towards,
	title        = {Towards Provably Unlearnable Examples via Bayes Error Optimization},
	author       = {Ruihan Zhang and Peixin Zhang and Ee-Peng Lim and Jun Sun},
	year         = {2025}
}
@inproceedings{zhao2011comparing,
	title        = {Comparing Twitter and Traditional Media Using Topic Models},
	author       = {Zhao, Wayne Xin and Jiang, Jing and Weng, Jianshu and He, Jing and Lim, Ee-Peng and Yan, Hongfei and Li, Xiaoming},
	year         = {2011},
	booktitle    = {Advances in Information Retrieval},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {338--349},
	isbn         = {978-3-642-20161-5},
	editor       = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
	abstract     = {Twitter as a new form of social media can potentially contain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter. We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into consideration topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for downstream IR or DM applications.}
}
@article{zhao2013beyond,
	title        = {Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications},
	author       = {Ming-Jie Zhao and Narayanan Edakunni and Adam Pocock and Gavin Brown},
	year         = {2013},
	journal      = {Journal of Machine Learning Research},
	volume       = {14},
	number       = {32},
	pages        = {1033--1090},
	url          = {http://jmlr.org/papers/v14/zhao13a.html}
}
@inproceedings{zhao2017gated,
	title        = {Gated Neural Network for Sentence Compression Using Linguistic Knowledge},
	author       = {Yang Zhao and Hajime Senuma and Xiaoyu Shen and Akiko Aizawa},
	year         = {2017},
	booktitle    = {Natural Language Processing and Information Systems - 22nd International Conference on Applications of Natural Language to Information Systems, {NLDB} 2017, Li{\`{e}}ge, Belgium, June 21-23, 2017, Proceedings},
	publisher    = {Springer},
	series       = {Lecture Notes in Computer Science},
	volume       = {10260},
	pages        = {480--491},
	doi          = {10.1007/978-3-319-59569-6\_56},
	url          = {https://doi.org/10.1007/978-3-319-59569-6\_56},
	editor       = {Flavius Frasincar and Ashwin Ittoo and Le Minh Nguyen and Elisabeth M{\'{e}}tais},
	timestamp    = {Tue, 24 Sep 2019 17:17:46 +0200},
	biburl       = {https://dblp.org/rec/conf/nldb/ZhaoSSA17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhao2020rdcface,
	title        = {RDCFace: Radial Distortion Correction for Face Recognition},
	author       = {Zhao, He and Ying, Xianghua and Shi, Yongjie and Tong, Xin and Wen, Jingsi and Zha, Hongbin},
	year         = {2020},
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{zheng2015towards,
	title        = {Towards Lifestyle Understanding: Predicting Home and Vacation Locations from User's Online Photo Collections},
	author       = {Zheng, Danning and Hu, Tianran and You, Quanzeng and Kautz, Henry and Luo, Jiebo},
	year         = {2021},
	month        = {Aug.},
	journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
	volume       = {9},
	number       = {1},
	pages        = {553--560},
	url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/14591},
	abstractnote = {\&lt;p\&gt; Semantic place labeling has been actively studied in the past few years due to its importance in understanding human mobility and lifestyle patterns. In the last decade, the rapid growth of geotagged multimedia data from online social networks provides a valuable opportunity to predict people's POI locations from temporal, spatial and visual cues. Among the massive amount of social media data, one important type of data is the geotagged web images from image-sharing websites. In this paper, we develop a reliable photo classifier based on the Convolutional Neutral Networks to classify the photo-taking scene of real-life photos. We then present a novel approach to home location and vacation locations prediction by fusing together the visual content of photos and the spatiotemporal features of people's mobility patterns. Using a well-trained classifier, we showed that the robust fusion of visual and spatiotemporal features achieves significant accuracy improvement over each of the features alone for both home and vacation detection. \&lt;/p\&gt;}
}
@article{zheng2018cross,
	title        = {Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments},
	author       = {Zheng, Tianyue and Deng, Weihong},
	year         = {2018},
	journal      = {Beijing University of Posts and Telecommunications, Tech. Rep},
	volume       = {5},
	pages        = {7}
}
@article{zheng2018survey,
	title        = {A Survey of Location Prediction on Twitter},
	author       = {Zheng, Xin and Han, Jialong and Sun, Aixin},
	year         = {2018},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {30},
	number       = {9},
	pages        = {1652--1671},
	doi          = {10.1109/TKDE.2018.2807840}
}
@inproceedings{zheng2022neuronfair,
	title        = {NeuronFair: interpretable white-box fairness testing through biased neuron identification},
	author       = {Zheng, Haibin and Chen, Zhiqing and Du, Tianyu and Zhang, Xuhong and Cheng, Yao and Ji, Shouling and Wang, Jingyi and Yu, Yue and Chen, Jinyin},
	year         = {2022},
	booktitle    = {Proceedings of the 44th International Conference on Software Engineering},
	location     = {Pittsburgh, Pennsylvania},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICSE '22},
	pages        = {1519–1531},
	doi          = {10.1145/3510003.3510123},
	isbn         = {9781450392211},
	url          = {https://doi.org/10.1145/3510003.3510123},
	abstract     = {Deep neural networks (DNNs) have demonstrated their outperformance in various domains. However, it raises a social concern whether DNNs can produce reliable and fair decisions especially when they are applied to sensitive domains involving valuable resource allocation, such as education, loan, and employment. It is crucial to conduct fairness testing before DNNs are reliably deployed to such sensitive domains, i.e., generating as many instances as possible to uncover fairness violations. However, the existing testing methods are still limited from three aspects: interpretability, performance, and generalizability. To overcome the challenges, we propose NeuronFair, a new DNN fairness testing framework that differs from previous work in several key aspects: (1) interpretable - it quantitatively interprets DNNs' fairness violations for the biased decision; (2) effective - it uses the interpretation results to guide the generation of more diverse instances in less time; (3) generic - it can handle both structured and unstructured data. Extensive evaluations across 7 datasets and the corresponding DNNs demonstrate NeuronFair's superior performance. For instance, on structured datasets, it generates much more instances (~ \texttimes{}5.84) and saves more time (with an average speedup of 534.56\%) compared with the state-of-the-art methods. Besides, the instances of NeuronFair can also be leveraged to improve the fairness of the biased DNNs, which helps build more fair and trustworthy deep learning systems. The code of NeuronFair is open-sourced at https://github.com/haibinzheng/NeuronFair.},
	numpages     = {13},
	keywords     = {biased neuron, deep learning, discriminatory instance, fairness testing, interpretability}
}
@article{zhi2023seed,
	title        = {Seed Selection for Testing Deep Neural Networks},
	author       = {Zhi, Yuhan and Xie, Xiaofei and Shen, Chao and Sun, Jun and Zhang, Xiaoyu and Guan, Xiaohong},
	year         = {2023},
	month        = {nov},
	journal      = {ACM Trans. Softw. Eng. Methodol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {33},
	number       = {1},
	doi          = {10.1145/3607190},
	issn         = {1049-331X},
	url          = {https://doi.org/10.1145/3607190},
	issue_date   = {January 2024},
	abstract     = {Deep learning (DL) has been applied in many applications. Meanwhile, the quality of DL systems is becoming a big concern. To evaluate the quality of DL systems, a number of DL testing techniques have been proposed. To generate test cases, a set of initial seed inputs are required. Existing testing techniques usually construct seed corpus by randomly selecting inputs from training or test dataset. Till now, there is no study on how initial seed inputs affect the performance of DL testing and how to construct an optimal one. To fill this gap, we conduct the first systematic study to evaluate the impact of seed selection strategies on DL testing. Specifically, considering three popular goals of DL testing (i.e., coverage, failure detection, and robustness), we develop five seed selection strategies, including three based on single-objective optimization (SOO) and two based on multi-objective optimization (MOO). We evaluate these strategies on seven testing tools. Our results demonstrate that the selection of initial seed inputs greatly affects the testing performance. SOO-based selection can construct the best seed corpus that can boost DL testing with respect to the specific testing goal. MOO-based selection strategies can construct seed corpus that achieve balanced improvement on multiple objectives.},
	articleno    = {23},
	numpages     = {33},
	keywords     = {robustness, coverage, seed selection, Deep learning testing}
}
@article{zhou2020semi,
	title        = {Semi-Supervised Trajectory Understanding with POI Attention for End-to-End Trip Recommendation},
	author       = {Zhou, Fan and Wu, Hantao and Trajcevski, Goce and Khokhar, Ashfaq and Zhang, Kunpeng},
	year         = {2020},
	month        = {feb},
	journal      = {ACM Trans. Spatial Algorithms Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = {6},
	number       = {2},
	doi          = {10.1145/3378890},
	issn         = {2374-0353},
	url          = {https://doi.org/10.1145/3378890},
	issue_date   = {June 2020},
	abstract     = {Trip planning/recommendation is an important task for a plethora of applications in urban settings (e.g., tourism, transportation, social outings), relying on services provided by Location-Based Social Networks (LBSN). To provide greater context-awareness in trajectory planning, LBSNs combine historical trajectories of users for generating various hand-crafted features—e.g., geo-tags of photos taken by tourists and textual characteristics derived from reviews. Those features are used to learn tourists' preferences, which are then used to generate a travel plan recommendation. However, many such features are extracted based on prior knowledge or empirical analysis specific to particular datasets, rendering the corresponding solutions not to be generalizable to diverse data sources. Thus, one important question for managing mobility is how to learn an accurate tour planning model based solely on POI visits or user check-ins and without the efforts of hand-crafted feature engineering. Inspired by recent successes of deep learning in sequence learning, we develop a solution to the tour planning problem based on the semi-supervised learning paradigm. An important aspect of our solution is that it does not involve any feature engineering. Specifically, we propose the Trip Recommendation method via trajectory Encoder and Decoder—a novel end-to-end approach encoding historical trajectories into vectors, while capturing both the intrinsic characteristics of individual POIs and the transition patterns among POIs. We also incorporate historical attention mechanism in our sequence-to-sequence trip recommendation task to improve the effectiveness. Experiments conducted on multiple publicly available LBSN datasets demonstrate significantly superior performance of our method.},
	articleno    = {13},
	numpages     = {25},
	keywords     = {attention mechanism, Trip recommendation, semi-supervised learning, recurrent neural networks, encoder-decoder}
}
@article{zhou2023synthetic,
	title        = {Synthetic data generation method for data-free knowledge distillation in regression neural networks},
	author       = {Tianxun Zhou and Keng-Hwee Chiam},
	year         = {2023},
	journal      = {Expert Systems with Applications},
	volume       = {227},
	pages        = {120327},
	doi          = {https://doi.org/10.1016/j.eswa.2023.120327},
	issn         = {0957-4174},
	url          = {https://www.sciencedirect.com/science/article/pii/S0957417423008291},
	keywords     = {Data-free knowledge distillation, Knowledge distillation, Neural network, Regression, Machine learning},
	abstract     = {Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks in the absence of original training data, the existing method uses a generator model trained adversarially against the student model to generate synthetic data to train the student model. In this study, we propose a new synthetic data generation strategy that directly optimizes for a large but bounded difference between the student and teacher model. Our results on benchmark experiments demonstrate that the proposed strategy allows the student model to learn better and emulate the performance of the teacher model more closely.}
}
@article{zhu2016computing,
	title        = {Computing Semantic Similarity of Concepts in Knowledge Graphs},
	author       = {Zhu, Ganggao and Iglesias, Carlos A.},
	year         = {2017},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {29},
	number       = {1},
	pages        = {72--85},
	doi          = {10.1109/TKDE.2016.2610428}
}
@article{zhu2017differentially,
	title        = {Differentially private data publishing and analysis: A survey},
	author       = {Zhu, Tianqing and Li, Gang and Zhou, Wanlei and Yu, Philip S},
	year         = {2017},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE},
	volume       = {29},
	number       = {8},
	pages        = {1619--1638}
}
@inproceedings{zhu2017prune,
	title        = {To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
	author       = {Michael Zhu and Suyog Gupta},
	year         = {2018},
	booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=Sy1iIDkPM},
	timestamp    = {Thu, 04 Apr 2019 13:20:09 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/ZhuG18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhu2024detection,
	title        = {Detection and defense of unlearnable examples},
	author       = {Zhu, Yifan and Yu, Lijia and Gao, Xiao-Shan},
	year         = {2024},
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = {38},
	number       = {15},
	pages        = {17211--17219}
}
@inproceedings{ziegler2019fine,
	title        = {Fine-Grained Access Control in Industrial Internet of Things - Evaluating Outsourced Attribute-Based Encryption},
	author       = {Dominik Ziegler and Josef Sabongui and Gerald Palfinger},
	year         = {2019},
	booktitle    = {{ICT} Systems Security and Privacy Protection - 34th {IFIP} {TC} 11 International Conference, {SEC} 2019, Lisbon, Portugal, June 25-27, 2019, Proceedings},
	publisher    = {Springer},
	series       = {{IFIP} Advances in Information and Communication Technology},
	volume       = {562},
	pages        = {91--104},
	doi          = {10.1007/978-3-030-22312-0\_7},
	url          = {https://doi.org/10.1007/978-3-030-22312-0\_7},
	editor       = {Gurpreet Dhillon and Fredrik Karlsson and Karin Hedstr{\"{o}}m and Andr{\'{e}} Z{\'{u}}quete},
	timestamp    = {Wed, 17 Feb 2021 10:36:26 +0100},
	biburl       = {https://dblp.org/rec/conf/sec/ZieglerSP19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zou2007illumination,
	title        = {Illumination Invariant Face Recognition: A Survey},
	author       = {Zou, Xuan and Kittler, Josef and Messer, Kieron},
	year         = {2007},
	booktitle    = {2007 First IEEE International Conference on Biometrics: Theory, Applications, and Systems},
	volume       = {},
	number       = {},
	pages        = {1--8},
	doi          = {10.1109/BTAS.2007.4401921}
}
@article{zubiaga2016analysing,
	title        = {Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads},
	author       = {Zubiaga, Arkaitz AND Liakata, Maria AND Procter, Rob AND Wong Sak Hoi, Geraldine AND Tolmie, Peter},
	year         = {2016},
	month        = {03},
	journal      = {PLOS ONE},
	publisher    = {Public Library of Science},
	volume       = {11},
	number       = {3},
	pages        = {1--29},
	doi          = {10.1371/journal.pone.0150989},
	url          = {https://doi.org/10.1371/journal.pone.0150989},
	abstract     = {As breaking news unfolds people increasingly rely on social media to stay abreast of the latest updates. The use of social media in such situations comes with the caveat that new information being released piecemeal may encourage rumours, many of which remain unverified long after their point of release. Little is known, however, about the dynamics of the life cycle of a social media rumour. In this paper we present a methodology that has enabled us to collect, identify and annotate a dataset of 330 rumour threads (4,842 tweets) associated with 9 newsworthy events. We analyse this dataset to understand how users spread, support, or deny rumours that are later proven true or false, by distinguishing two levels of status in a rumour life cycle i.e., before and after its veracity status is resolved. The identification of rumours associated with each event, as well as the tweet that resolved each rumour as true or false, was performed by journalist members of the research team who tracked the events in real time. Our study shows that rumours that are ultimately proven true tend to be resolved faster than those that turn out to be false. Whilst one can readily see users denying rumours once they have been debunked, users appear to be less capable of distinguishing true from false rumours when their veracity remains in question. In fact, we show that the prevalent tendency for users is to support every unverified rumour. We also analyse the role of different types of users, finding that highly reputable users such as news organisations endeavour to post well-grounded statements, which appear to be certain and accompanied by evidence. Nevertheless, these often prove to be unverified pieces of information that give rise to false rumours. Our study reinforces the need for developing robust machine learning techniques that can provide assistance in real time for assessing the veracity of rumours. The findings of our study provide useful insights for achieving this aim.}
}
@book{zwillinger1999crc,
	title        = {CRC standard probability and statistics tables and formulae},
	author       = {Zwillinger, Daniel and Kokoska, Stephen},
	year         = {1999},
	publisher    = {Crc Press}
}
